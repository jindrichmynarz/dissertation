<!DOCTYPE html>
<html  lang="en"
  prefix="cc:      http://creativecommons.org/ns#
          dcterms: http://purl.org/dc/terms/
          ldp:     http://www.w3.org/ns/ldp#
          oa:      http://www.w3.org/ns/oa#
          schema:  http://schema.org/
          xsd:     http://www.w3.org/2001/XMLSchema#">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Jindřich Mynarz">
            <meta name="date" content="2017-01-01">
            <meta name="keywords" content="matchmaking, recommender systems, case-based reasoning, linked data, open data, public procurement">
        <title>Matchmaking of bidders and public contracts using linked open data</title>
                <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" type="text/css">
        <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:700" type="text/css">
        <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" type="text/css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" type="text/css">
        <link rel="stylesheet" href="https://dokie.li/media/css/do.css" type="text/css">
        <link rel="stylesheet" href="resources/css/bootstrap.css" type="text/css">
              <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
            <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-39372032-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments)};
      gtag('js', new Date());

      gtag('config', 'UA-39372032-1');
    </script>
  </head>
  <body about="" typeof="schema:ScholarlyArticle">
    <main class="container">
      <div>
        <h1 class="title" property="dcterms:title">Matchmaking of bidders and public contracts using linked open data</h1>

        
        <div id="authors">
                    <p class="author">
            <a rel="dcterms:creator" href="http://mynarz.net/#jindrich">Jindřich Mynarz</a>
          </p>
                  </div>

        <p>Department of Information and Knowledge Engineering, <a href="https://www.vse.cz/english">University of Economics, Prague</a></p>

        <p rel="dcterms:creator">
          <a about="http://mynarz.net/#jindrich" href="mailto:jindrich.mynarz@vse.cz">
            <code lang="" property="schema:email">jindrich.mynarz@vse.cz</code>
          </a>
        </p>

                <h3 class="date" property="dcterms:created" datatype="xsd:gYear">2017</h3>
        
        <dl>
          <dt>Supervisor</dt>
          <dd><a href="http://nb.vse.cz/~svatek/welcom_e.htm">doc. Ing. Vojtěch Svátek, Dr.</a></dd>
          <dt>Annotation service</dt>
          <dd><a href="https://linkedresearch.org/annotation/mynarz.net/dissertation/" rel="oa:annotationService">https://linkedresearch.org/annotation/mynarz.net/dissertation/</a></dd>
          <dt>Notifications inbox</dt>
          <dd><a href="https://linkedresearch.org/inbox/mynarz.net/dissertation/"
                 rel="ldp:inbox">https://linkedresearch.org/inbox/mynarz.net/dissertation/</a></dd>
          <dt>Licence</dt>
          <dd>
            <a rel="dcterms:license" href="https://creativecommons.org/licenses/by-sa/3.0/cz">
              <img alt="Creative Commons License"
                   src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" />
              Creative Commons Attribution-ShareAlike 3.0 Czech Republic License
            </a>
            <span property="cc:attributionName" content="Jindřich Mynarz" lang=""></span>
            <span rel="cc:attributionURL" resource="http://mynarz.net/dissertation"></span>
          </dd>
        </dl>
      </div>

            <h2>Table of contents</h2>
      <div id="TOC">
      <ul>
      <li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
      <li><a href="#research-topic"><span class="toc-section-number">1.1</span> Research topic</a></li>
      <li><a href="#goals-and-methods"><span class="toc-section-number">1.2</span> Goals and methods</a></li>
      <li><a href="#outline"><span class="toc-section-number">1.3</span> Outline</a></li>
      <li><a href="#linked-open-data"><span class="toc-section-number">1.4</span> Linked open data</a><ul>
      <li><a href="#sec:open-data"><span class="toc-section-number">1.4.1</span> Open data</a></li>
      <li><a href="#sec:linked-data"><span class="toc-section-number">1.4.2</span> Linked data</a></li>
      </ul></li>
      <li><a href="#sec:public-procurement"><span class="toc-section-number">1.5</span> Public procurement domain</a><ul>
      <li><a href="#sec:legal-context"><span class="toc-section-number">1.5.1</span> Legal context</a></li>
      <li><a href="#economic-context"><span class="toc-section-number">1.5.2</span> Economic context</a></li>
      <li><a href="#use-cases-for-matchmaking"><span class="toc-section-number">1.5.3</span> Use cases for matchmaking</a></li>
      </ul></li>
      <li><a href="#matchmaking"><span class="toc-section-number">1.6</span> Matchmaking</a><ul>
      <li><a href="#sec:cbr"><span class="toc-section-number">1.6.1</span> Case-based reasoning</a></li>
      <li><a href="#sec:srl"><span class="toc-section-number">1.6.2</span> Statistical relational learning</a></li>
      </ul></li>
      <li><a href="#sec:related-work"><span class="toc-section-number">1.7</span> Related work</a><ul>
      <li><a href="#applications"><span class="toc-section-number">1.7.1</span> Applications</a></li>
      <li><a href="#vocabularies"><span class="toc-section-number">1.7.2</span> Vocabularies</a></li>
      <li><a href="#related-technologies"><span class="toc-section-number">1.7.3</span> Related technologies</a></li>
      </ul></li>
      </ul></li>
      <li><a href="#sec:data-preparation"><span class="toc-section-number">2</span> Data preparation</a><ul>
      <li><a href="#sec:modelling"><span class="toc-section-number">2.1</span> Modelling</a><ul>
      <li><a href="#sec:pco"><span class="toc-section-number">2.1.1</span> Public Contracts Ontology</a></li>
      <li><a href="#sec:concrete-data-model"><span class="toc-section-number">2.1.2</span> Concrete data model</a></li>
      </ul></li>
      <li><a href="#extraction"><span class="toc-section-number">2.2</span> Extraction</a></li>
      <li><a href="#sec:transformation"><span class="toc-section-number">2.3</span> Transformation</a><ul>
      <li><a href="#challenges"><span class="toc-section-number">2.3.1</span> Challenges</a></li>
      <li><a href="#transformations"><span class="toc-section-number">2.3.2</span> Transformations</a></li>
      </ul></li>
      <li><a href="#sec:linking"><span class="toc-section-number">2.4</span> Linking</a><ul>
      <li><a href="#content-based-addressing"><span class="toc-section-number">2.4.1</span> Content-based addressing</a></li>
      <li><a href="#technologies"><span class="toc-section-number">2.4.2</span> Technologies</a></li>
      <li><a href="#tasks"><span class="toc-section-number">2.4.3</span> Tasks</a></li>
      <li><a href="#evaluation"><span class="toc-section-number">2.4.4</span> Evaluation</a></li>
      <li><a href="#sec:geocoding"><span class="toc-section-number">2.4.5</span> Geocoding</a></li>
      <li><a href="#linked-datasets"><span class="toc-section-number">2.4.6</span> Linked datasets</a></li>
      </ul></li>
      <li><a href="#sec:fusion"><span class="toc-section-number">2.5</span> Fusion</a><ul>
      <li><a href="#conflict-resolution-strategies"><span class="toc-section-number">2.5.1</span> Conflict resolution strategies</a></li>
      <li><a href="#evaluation-2"><span class="toc-section-number">2.5.2</span> Evaluation</a></li>
      </ul></li>
      <li><a href="#loading"><span class="toc-section-number">2.6</span> Loading</a><ul>
      <li><a href="#sparql-based-matchmakers"><span class="toc-section-number">2.6.1</span> SPARQL-based matchmakers</a></li>
      <li><a href="#sec:loading-rescal"><span class="toc-section-number">2.6.2</span> RESCAL-based matchmakers</a></li>
      </ul></li>
      <li><a href="#sec:data-summary"><span class="toc-section-number">2.7</span> Summary</a></li>
      </ul></li>
      <li><a href="#sec:methods"><span class="toc-section-number">3</span> Matchmaking methods</a><ul>
      <li><a href="#sec:ground-truth"><span class="toc-section-number">3.1</span> Ground truth</a></li>
      <li><a href="#sec:method-sparql"><span class="toc-section-number">3.2</span> SPARQL</a><ul>
      <li><a href="#benefits-and-drawbacks"><span class="toc-section-number">3.2.1</span> Benefits and drawbacks</a></li>
      <li><a href="#ranking"><span class="toc-section-number">3.2.2</span> Ranking</a></li>
      <li><a href="#sec:blind-matchmakers"><span class="toc-section-number">3.2.3</span> Blind matchmakers</a></li>
      <li><a href="#implementation"><span class="toc-section-number">3.2.4</span> Implementation</a></li>
      </ul></li>
      <li><a href="#sec:method-rescal"><span class="toc-section-number">3.3</span> Tensor factorization</a><ul>
      <li><a href="#rescal"><span class="toc-section-number">3.3.1</span> RESCAL</a></li>
      <li><a href="#ranking-1"><span class="toc-section-number">3.3.2</span> Ranking</a></li>
      <li><a href="#implementation-1"><span class="toc-section-number">3.3.3</span> Implementation</a></li>
      </ul></li>
      </ul></li>
      <li><a href="#sec:evaluation"><span class="toc-section-number">4</span> Evaluation</a><ul>
      <li><a href="#ground-truth"><span class="toc-section-number">4.1</span> Ground truth</a></li>
      <li><a href="#sec:evaluation-protocol"><span class="toc-section-number">4.2</span> Evaluation protocol</a></li>
      <li><a href="#sec:evaluated-metrics"><span class="toc-section-number">4.3</span> Evaluated metrics</a></li>
      <li><a href="#results-of-sparql-based-matchmakers"><span class="toc-section-number">4.4</span> Results of SPARQL-based matchmakers</a><ul>
      <li><a href="#blind-matchmakers"><span class="toc-section-number">4.4.1</span> Blind matchmakers</a></li>
      <li><a href="#aggregation-functions"><span class="toc-section-number">4.4.2</span> Aggregation functions</a></li>
      <li><a href="#individual-features"><span class="toc-section-number">4.4.3</span> Individual features</a></li>
      <li><a href="#combined-features"><span class="toc-section-number">4.4.4</span> Combined features</a></li>
      <li><a href="#query-expansion"><span class="toc-section-number">4.4.5</span> Query expansion</a></li>
      <li><a href="#data-reduction"><span class="toc-section-number">4.4.6</span> Data reduction</a></li>
      <li><a href="#data-refinement"><span class="toc-section-number">4.4.7</span> Data refinement</a></li>
      <li><a href="#counter-measures-to-limits-of-ground-truth"><span class="toc-section-number">4.4.8</span> Counter-measures to limits of ground truth</a></li>
      </ul></li>
      <li><a href="#results-of-rescal-based-matchmakers"><span class="toc-section-number">4.5</span> Results of RESCAL-based matchmakers</a><ul>
      <li><a href="#hyper-parameters"><span class="toc-section-number">4.5.1</span> Hyper-parameters</a></li>
      <li><a href="#feature-selection"><span class="toc-section-number">4.5.2</span> Feature selection</a></li>
      <li><a href="#ageing"><span class="toc-section-number">4.5.3</span> Ageing</a></li>
      <li><a href="#literals"><span class="toc-section-number">4.5.4</span> Literals</a></li>
      </ul></li>
      <li><a href="#comparison-of-the-results"><span class="toc-section-number">4.6</span> Comparison of the results</a></li>
      </ul></li>
      <li><a href="#sec:conclusions"><span class="toc-section-number">5</span> Conclusions</a></li>
      <li><a href="#references">References</a></li>
      <li><a href="#sec:software"><span class="toc-section-number">6</span> Software</a><ul>
      <li><a href="#reused-software"><span class="toc-section-number">6.1</span> Reused software</a><ul>
      <li><a href="#elasticsearch"><span class="toc-section-number">6.1.1</span> Elasticsearch</a></li>
      <li><a href="#geotools"><span class="toc-section-number">6.1.2</span> GeoTools</a></li>
      <li><a href="#linkedpipes-etl"><span class="toc-section-number">6.1.3</span> LinkedPipes-ETL</a></li>
      <li><a href="#openlink-virtuoso"><span class="toc-section-number">6.1.4</span> OpenLink Virtuoso</a></li>
      <li><a href="#sec:rescal-software"><span class="toc-section-number">6.1.5</span> RESCAL</a></li>
      <li><a href="#saxon-xslt-and-xquery-processor"><span class="toc-section-number">6.1.6</span> Saxon XSLT and XQuery Processor</a></li>
      <li><a href="#silk-link-discovery-framework"><span class="toc-section-number">6.1.7</span> Silk Link Discovery Framework</a></li>
      <li><a href="#tarql"><span class="toc-section-number">6.1.8</span> Tarql</a></li>
      <li><a href="#unifiedviews"><span class="toc-section-number">6.1.9</span> UnifiedViews</a></li>
      </ul></li>
      <li><a href="#developed-software"><span class="toc-section-number">6.2</span> Developed software</a><ul>
      <li><a href="#sec:discretize-sparql"><span class="toc-section-number">6.2.1</span> discretize-sparql</a></li>
      <li><a href="#elasticsearch-geocoding"><span class="toc-section-number">6.2.2</span> elasticsearch-geocoding</a></li>
      <li><a href="#jsonld-to-elasticsearch"><span class="toc-section-number">6.2.3</span> jsonld-to-elasticsearch</a></li>
      <li><a href="#sec:matchmaker-sparql"><span class="toc-section-number">6.2.4</span> matchmaker-sparql</a></li>
      <li><a href="#sec:matchmaker-rescal"><span class="toc-section-number">6.2.5</span> matchmaker-rescal</a></li>
      <li><a href="#sparql-to-csv"><span class="toc-section-number">6.2.6</span> sparql-to-csv</a></li>
      <li><a href="#sparql-to-graphviz"><span class="toc-section-number">6.2.7</span> sparql-to-graphviz</a></li>
      <li><a href="#sparql-to-jsonld"><span class="toc-section-number">6.2.8</span> sparql-to-jsonld</a></li>
      <li><a href="#sec:sparql-to-tensor"><span class="toc-section-number">6.2.9</span> sparql-to-tensor</a></li>
      <li><a href="#sparql-unlimited"><span class="toc-section-number">6.2.10</span> sparql-unlimited</a></li>
      <li><a href="#vocab-to-graphviz"><span class="toc-section-number">6.2.11</span> vocab-to-graphviz</a></li>
      </ul></li>
      </ul></li>
      <li><a href="#sec:abbreviations">Abbreviations</a></li>
      </ul>
      </div>
            <article class="row text-justify">
        <div class="col-md-8 offset-md-2 dont-break-out">
          <section id="introduction" class="level1">
          <h1><span class="header-section-number">1</span> Introduction</h1>
          <!-- What is the problem? -->
          <p>In order for demand and supply to meet, they must learn about each other. Data on demands and supplies thus needs to be accessible, discoverable, and usable. As data grows to larger volumes, its machine-readability becomes paramount, so that machines can make it usable for people, for whom dealing with large data is impractical. Moreover, relevant data may be fragmented across diverse data sources, which need to be integrated to enable their effective use. Nevertheless, when data collection and integration is done manually, it takes a lot of effort.</p>
          <p>Some manual effort involved in gathering and evaluation of data about demands and supplies can be automated by matchmaking. Simply put, matchmakers are tools that retrieve data matching a query. In the matchmaking setting, either demands or supplies are cast as queries while the other side is treated as the queried data. The queries produce matches from the queried data ranked by their degree to which they satisfy a given query.</p>
          <!-- Motivation: efficiency -->
          <p>Our work concerns matchmaking of bidders and public contracts. The primary motivation for our research is to improve the efficiency of resource allocation in public procurement by providing better information to the participants of the public procurement market. We employ matchmaking as a way to find information that is useful for the market participants. In the context of public procurement, matchmaking can suggest relevant business opportunities to bidders or recommend to contracting authorities which bidders are suitable to be approached for a given public contract.</p>
          <!--
          For the first time in history we have open data on past experiences in making public contracts.
          Using this data we can learn how to make better contracts.
          Contracting authorities and bidders can learn from the history of public procurement to be able to agree on better deals.
          Matchmaking is one way to learn from these experiences.
          
          Public contracts exist in a network of relationships between organizations.
          This network manifests in the data about public contracts.
          The relationships in the data mirror the relationships in the real world.
          This is why it is important to combine data from multiple domains to learn about the context in which public contracts are made.
          Here, semantics is a way to agree on meaning of things in the data.
          Semantic web is a way to agree on what things are, so that we can combine the data about the things.
          
          ## Matchmaking vs. advertising
          
          Matchmaking substitutes advertising.
          Business models based on advertising distort the design of web services.
          => Motivation
          Personalization makes matchmaking approach many-to-1 advertising.
          -->
          <section id="research-topic" class="level2">
          <h2><span class="header-section-number">1.1</span> Research topic</h2>
          <p>Our approach to matchmaking is based on two components: good data and good technologies. We employ linked open data as a method to defragment and integrate public procurement data and enable to combine it with other data. A key challenge in using linked open data is to reuse or develop appropriate techniques for data preparation.</p>
          <p>We demonstrate how two generic approaches can be applied to the matchmaking task, namely case-based reasoning and statistical relational learning. In the context of case-based reasoning, we treated matchmaking as top-<span class="math inline">\(k\)</span> recommendation. We used the SPARQL <span class="citation" data-cites="Harris2013">(Harris and Seaborne <a href="#ref-Harris2013">2013</a>)</span> query language to implement this task. In the case of statistical relational learning, we approached matchmaking as link prediction. We used tensor factorization with RESCAL <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span> for this task. The key challenges of matchmaking by these technologies involve feature selection or feature construction, ranking by feature weights, and combination functions for aggregating similarity scores of matches. Our work discusses these challenges and proposes novel ways of addressing them.</p>
          <p>In order to explore the outlined approaches we prepared a Czech public procurement dataset that links several related open government data sources together, such as the Czech business register or the postal addresses from the Czech Republic. Our work can be therefore considered a concrete use case in the Czech public procurement. Viewed as a use case, our task is to select, combine, and apply the state-of-the-art techniques to a real-world scenario. Our key stakeholders in this use case are the participants in the public procurement market: contracting authorities, who represent the side of demand, and bidders, who represent the side of supply. The stakeholder groups are driven by different interests; contracting authorities represent the public sector while bidders represent the private sector, which gives rise to a sophisticated interplay of the legal framework of public procurement and the commercial interests.</p>
          <!-- Problem statement
          
          Specific problems:
          
          * Data integration
          * Feature construction for matchmaking
          * How to engineer matchmaking methods to achieve high accuracy and diversity of recommendations?
          -->
          <!-- Problem context 
          
          * Selection of a matching object; specifically for tenders
          * Social context: Better matchmaking helps avoid passive waste with public resources.
          -->
          <!-- Research domain
          
          The domain of this research is an intersection of matchmaking and semantic web knowledge engineering.
          -->
          </section>
          <section id="goals-and-methods" class="level2">
          <h2><span class="header-section-number">1.2</span> Goals and methods</h2>
          <!-- Research goals -->
          <p>Our research goal is to explore matchmaking of public contracts to bidders operating on linked open data. In order to pursue this goal we prepare public procurement linked open data and develop software for matchmaking. Our secondary target implied by our research direction is to test the available implementations of the semantic web technologies for handling linked open data and, if these tools are found lacking, to develop auxiliary tools to support data preparation and matchmaking. These secondary goals were not formulated upfront; we only specified them explicitly as we progressed the pursuit of our primary goal.</p>
          <!-- Prerequisites -->
          <p>In order to be able to deliver on the stated goals, their prerequisites must be fulfilled. Applied research depends on the availability of its building blocks. Our research is built on open data and open-source software. We need public procurement data to be available as open data, as described in the sec. <a href="#sec:open-data">1.4.1</a>. The data must be structured in a way from which a semantic description of the data can be created, implying that the data is machine-readable and consistent. Consistency of the data arises from standardization, including the adherence to fixed schemas and code lists. We conceive matchmaking as a high-level task based on many layers of technology. Both our data preparation tools and matchmakers build on open-source components. In the pursuit of our goals we reused and orchestrated a large set of existing open-source software.</p>
          <!-- Scientific methods -->
          <p>We adopt the methods of the design science <span class="citation" data-cites="Hevner2004">(Hevner et al. <a href="#ref-Hevner2004">2004</a>)</span> in our research. We design artefacts, including the Czech public procurement dataset and the matchmakers, and experiment with them to tell which of their variants perform better. Viewed this way, our task is to explore what kinds of artefacts for matchmaking of public contracts to bidders makes linked open data feasible to build.</p>
          <!-- Evaluation -->
          <p>The key question to evaluate is whether we can develop a matchmaker that can produce results deemed useful by domain experts representing the stakeholders. We evaluate the developed matchmakers via offline experiments on retrospective data. <!-- and via interviews with representatives of contracting authorities. --> In terms of our target metrics, we aim to recommend matches exhibiting both high accuracy and diversity. In order to discover the key factors that improve matchmaking we compare the evaluation results produced by the developed matchmakers in their different configurations.</p>
          <!--
          Designing an artefact is a way of "constructive proof".
          Designing the matchmakers can be considered a constructive proof to answer our question.
          -->
          <!-- Contributions -->
          <p>The principal contributions of our work are the implemented matchmaking methods, the reusable datasets for testing these methods, and generic software for processing linked open data. By using experimental evaluation of these methods we derive general findings about the factors that have the largest impact on the quality of matchmaking of bidders to public contracts.</p>
          <!-- Limitations -->
          <p>We need to acknowledge the limitations of our contributions. Our work covers only a narrow fraction of matchmaking that is feasible. The two methods we applied to matchmaking are evaluated on a single dataset. Narrowing down the data we experimented with to one dataset implies a limited generalization ability of our findings. Consequently, we cannot guarantee that the findings would translate to other public procurement datasets. We used only quantitative evaluation with retrospective data, which gives us a limited insight into the evaluated matchmaking methods. A richer understanding of the methods could have been obtained via qualitative evaluation or online evaluation involving users of the matchmakers.</p>
          </section>
          <section id="outline" class="level2">
          <h2><span class="header-section-number">1.3</span> Outline</h2>
          <p>We follow a simple structure in this dissertation. This chapter introduces our research and explains both the preliminaries and context in which our work is built as well as surveying the related research (sec. <a href="#sec:related-work">1.7</a>) to position our contributions. The dissertation continues with a substantial chapter on data preparation (sec. <a href="#sec:data-preparation">2</a>) that describes the extensive effort we invested in pre-processing data for the purposes of matchmaking. In line with the characteristics of linked open data, the key parts of this chapter deal with linking (sec. <a href="#sec:linking">2.4</a>) and data fusion (sec. <a href="#sec:fusion">2.5</a>). We follow up with a principal chapter that describes the matchmaking methods we designed and implemented (sec. <a href="#sec:methods">3</a>), which includes matchmaking based on SPARQL (sec. <a href="#sec:method-sparql">3.2</a>) and on tensor factorization by RESCAL (sec. <a href="#sec:method-rescal">3.3</a>). The subsequent chapter discusses the evaluation (sec. <a href="#sec:evaluation">4</a>) of the devised matchmaking methods by using the datasets we prepared. We experimented with many configurations of the matchmaking methods in the evaluation. In this chapter, we present the results of selected quantitative evaluation metrics and provide interpretations of the obtained results. Finally, the concluding chapter (sec. <a href="#sec:conclusions">5</a>) closes the dissertation, summarizing its principal contributions as well as remarking on its limitations that may be addresses in future research.</p>
          <p>The contributions presented in this dissertation including the methods and software were authored or co-authored by the dissertation’s author, unless stated otherwise. Both the reused and the developed software is listed in the appendix <a href="#sec:software">6</a>. The abbreviations used throughout the text are collected at the end of the dissertation. All vocabulary prefixes used in the text can be resolved to their corresponding namespace IRIs via <a href="http://prefix.cc" class="uri">http://prefix.cc</a>.</p>
          <!-- Out-takes -->
          <!--
          ## Core hypotheses
          
          * Additional features obtained from linked open data can improve matchmaking.
          * Matchmaking methods that are able to leverage textual data effectively surpass the methods that cannot.
            * Feedback: Too vague.
          * Improving data quality has an larger impact on matchmaking than the sophistication of matchmaking algorithms.
          * Combination of semantic and statistical features of data has a synergic effect that can produce better matchmaking results than when only semantic or statistical features are used.
          -->
          <!-- Le old
          
          A key obstacle to achieving the stated goal is fragmentation of data on the Web.
          Data about demands and offers is dispersed across a multitude of web sites, including electronic marketplaces or public sector registries. 
          To get a broader picture about the market one must scan through the relevant yet heterogeneous sources of data, each of which may expose a different access interface, most of which will be suitable only for humans to process.
          Therefore, *"search and matchmaking between two business parties over the current Web are still very time-consuming if [...] information from multiple sources needs to be combined to assess the relevance or execute the query"* [@Radinger2013].
          To improve this situation, the broad goal of this dissertation is to contribute to defragmentation of online markets by linking the data communicated between them.
          Having links in between datasets on the Web that are traversable by machines may enable to query the distributed markets as a single virtual market.
          Such virtual market may facilitate supply and demand to meet in a distributed linked open data infrastructure.
          
          A fundamental prerequisite to making this happen is to have the data in question openly available in a machine readable format.
          Open access to the data is needed to remove the information asymmetries between the actors in online marketplaces, which introduce unnecessary friction to the process of supply meeting demand.
          Nonetheless, data describing demands or offers on the current Web is exposed predominantly in documents, such as public procurement notices or calls for papers.
          Such documents are typically produced not with machine readability in mind, and thus their automated processing is difficult.
          Therefore, it is necessary to make this implicit data explicit by expressing it in structured way.
          Structuring data about both demands and offers in a granular fashion is especially crucial for complex multidimensional descriptions that cannot be simply reduced, such as to a single comparable number (e.g., a price tag).
          Having access to structured data enables automated processing and granular descriptions provide a basis for novel functionalities.
          
          In the public procurement domain, better information can improve the quality of government's decision making and thus make the allocation of public resources more efficient.
          Matchmaking can help public bodies to find a more suitable supplier, while companies can benefit from finding business opportunities in relevant calls for tenders.
          In effect, the ambition of the developed system is mainly to reduce passive waste with public funds [@Bandiera2009], which, unlike active waste, does not benefit the decision-making civil servant, but instead is caused rather by lack of information, skills, and motivation.
          -->
          <!--
          Side goals:
          
          * Transparency through data integration: Prior to data integration the meaning of data is lost in the noise of user-generated data. Data integration cleans the view and provides greater overview over public procurement.
          * Democratization of public procurement data analysis: Enables the rise of the armchair auditors.
          
          The proposed solution to address the dissertation's goal uses linked open data as a method for integrating data on the Web.
          The means towards approaching the depicted vision consist of application of semantic web technologies for matching data about offers and demands on the Web.
          To achieve the goal of machine readable data the semantic web stack offers a strong basis.
          Resource Description Framework (RDF) may be used as a common formalism, while RDF vocabularies and ontologies may serve as shared conceptualizations for modelling data.
          Data integration with linked data is based on explicit typed hyperlinks between datasets.
          
          Matchmaking linked open data can use both statistical and semantic inference.
          It can leverage both machine learning due to the volume of the data and semantic reasoning thanks to the formal representation of the data.
          
          A key part of this research are the similarity metrics for multidimensional and heterogeneous data described using RDF.
          -->
          <!--
          ## Design science framing
          
          ### Design problems
          
          Develop matchmakers
          
          ### Knowledge questions
          
          Do the developed matchmakers provide value to bidders and contracting authorities?
          (Are the matchmakers "good enough" (= accurate enough and diverse enough)?)
          (Would users continue to use the matchmakers if they are provided with a demo?)
          -->
          </section>
          <section id="linked-open-data" class="level2">
          <h2><span class="header-section-number">1.4</span> Linked open data</h2>
          <p>Linked open data (LOD) is the intersection of open and linked data. It combines proactive disclosure of open data, which is unencumbered by restrictions to access and use, with linked data, which provides a model for publishing semantic structured data on the Web. LOD serves as a fundamental component of our work that enables matchmaking to be executed.</p>
          <section id="sec:open-data" class="level3">
          <h3><span class="header-section-number">1.4.1</span> Open data</h3>
          <p>Open data is data that can be accessed equally by people and machines. Its definition is grounded in principles that assert what conditions data must meet to achieve legal and technical openness. Principles of open data are perhaps best embodied in the Open Definition <span class="citation" data-cites="OpenDefinition2015">(Open Knowledge <a href="#ref-OpenDefinition2015">2015</a>)</span> and the Eight principles of open government data <span class="citation" data-cites="EightPrinciples2007">(<a href="#ref-EightPrinciples2007">2007</a>)</span>. According to the Open Definition’s summary, <em>“open data and content can be freely used, modified, and shared by anyone for any purpose.”</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <!-- Legal conditions of open data are usually established via a licence or waiver, such as the Open Data Commons Public Domain Dedication and Licence (ODC PDDL).^[<https://opendatacommons.org/licenses/pddl/1.0>] --> The Eight principles of open government data draw similar requirements as the Open Definition and add demands for completeness, primacy, and timeliness.</p>
          <p>Open data is particularly prominent in the public sector, since public sector data is subject to disclosure mandated by law. Open data can be a result of either reactive disclosure, such as upon Freedom of Information requests, or proactive disclosure, such as by publishing open data. In case of the EU, disclosure of public sector data is regulated by the directive 2013/37/EU on the re-use of public sector information <span class="citation" data-cites="EU2013">(EU <a href="#ref-EU2013">2013</a>)</span>.</p>
          <p>While releasing open data is frequently framed as a means to improve transparency of the public sector, it can also have a positive effect on its efficiency <span class="citation" data-cites="AccessInfoEurope2011">(Access Info Europe and Open Knowledge Foundation <a href="#ref-AccessInfoEurope2011">2011</a>, p. 69)</span>, since the public sector itself is often the primary user of open data. Using open data can help streamline public sector processes <span class="citation" data-cites="Parycek2014">(Parycek et al. <a href="#ref-Parycek2014">2014</a>, p. 90)</span> and curb unnecessary expenditures <span class="citation" data-cites="Presern2014">(Prešern and Žejn <a href="#ref-Presern2014">2014</a>, p. 4)</span>. The publication of public procurement data is claimed to improve <em>“the quality of government investment decision-making”</em> <span class="citation" data-cites="Kenny2012">(Kenny and Karver <a href="#ref-Kenny2012">2012</a>, p. 2)</span>, as supervision enabled by access to data puts a pressure on contracting authorities to follow fair and budget-wise contracting procedures. Matchmaking public contracts to relevant suppliers can be considered an application of open data that can contribute to better-informed decisions leading to more economically advantageous contracts.</p>
          <p>Open data can help balance information asymmetries between participants of public procurement markets. The asymmetries may be caused by clientelism, siloing data in applications with restricted access, or fragmentation of data across multiple sources. Open access to public procurement data can increase the number of participants in procurement, since more bidders can learn about relevant opportunities if they are advertised openly. Even distribution of open data may eventually lead to better decisions of the market participants, thereby increasing the efficiency of resource allocation in public procurement.</p>
          <p>Open data addresses two fundamental problems of recommender systems, which apply to matchmaking as well. These problems comprise the cold start problem and data sparseness, which can be jointly described as the data acquisition problem <span class="citation" data-cites="Heitmann2010">(Heitmann and Hayes <a href="#ref-Heitmann2010">2010</a>)</span>. Cold start problem concerns the lack of data needed to make recommendations. It appears in new recommender systems that have yet to acquire users to amass enough data to make accurate recommendations. Open data ameliorates this problem by allowing to bootstrap a system from openly available datasets. In our case, we use open data from business registers to obtain descriptions of business entities that have not been awarded a contract yet, in order to make them discoverable for matchmaking. Data sparseness refers to the share of missing values in a dataset. If a large share of the matched entities is lacking values of the key properties leveraged by matchmaking, the quality of matchmaking results deteriorates. Complementary open datasets can help fill in the blank values or add extra features <span class="citation" data-cites="DiNoia2015">(Di Noia and Ostuni <a href="#ref-DiNoia2015">2015</a>, p. 102)</span> that improve the quality of matchmaking.</p>
          <p>The hereby presented work was done within the broader context of the OpenData.cz<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> initiative. OpenData.cz is an initiative for a transparent data infrastructure of the Czech public sector. It advocates adopting open data and linked data principles for publishing data on the Web. Our contributions described in the sec. <a href="#sec:data-preparation">2</a> enhance this infrastructure by supplying it with more open datasets and improving the existing ones.</p>
          </section>
          <section id="sec:linked-data" class="level3">
          <h3><span class="header-section-number">1.4.2</span> Linked data</h3>
          <p>Linked data is a set of practices for publishing structured data on the Web. It is a way of structuring data that identifies entities with Internationalized Resource Identifiers (IRIs) and materializes their relationships as a network of machine-processable data <span class="citation" data-cites="Ayers2007">(Ayers <a href="#ref-Ayers2007">2007</a>, p. 94)</span>. IRIs are universal, so that any entity can be identified with a IRI, and have global scope, therefore an IRI can only identify one entity <span class="citation" data-cites="BernersLee1996">(Berners-Lee <a href="#ref-BernersLee1996">1996</a>)</span>. A major manifestation of linked data is the Linking Open Data Cloud <span class="citation" data-cites="Abele2017">(Abele et al. <a href="#ref-Abele2017">2017</a>)</span>, which maps the web of semantically structured data that the spans hundreds of datasets from diverse domains, such as health care or linguistics. In this section we provide a basic introduction to the key aspects of linked data that we built on in this dissertation. A more detailed introduction to linked data in available in Heath and Bizer <span class="citation" data-cites="Heath2011">(<a href="#ref-Heath2011">2011</a>)</span>.</p>
          <p>Linked data may be seen as a pragmatic implementation of the so-called semantic web vision. It is based on semantic web technologies. This technology stack is largely built upon W3C standards.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> The fundamental standards of the semantic web technology stack, which are used throughout our work, are the Resource Description Framework (RDF), RDF Schema (RDFS), and SPARQL.</p>
          <!-- RDF -->
          <p>RDF <span class="citation" data-cites="Cyganiak2014b">(Cyganiak et al. <a href="#ref-Cyganiak2014b">2014</a>)</span> is a graph data format for exchanging data on the Web. The formal data model of RDF is a directed labelled multi-graph. Nodes and edges in RDF graphs are called resources. Resources can be either IRIs, blank nodes, or literals. IRIs from the set <span class="math inline">\(I\)</span> refer to resources, blank nodes from the set <span class="math inline">\(B\)</span> reference resources without intrinsic names, and literals from the set <span class="math inline">\(L\)</span> represent textual values. <span class="math inline">\(I\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(L\)</span> are pairwise disjoint sets. An RDF graph can be decomposed into a set of statements called RDF triples. An RDF triple can be defined as <span class="math inline">\((s, p, o) \in (I \cup B) \times I \times (I \cup B \cup L)\)</span>. In such triple, <span class="math inline">\(s\)</span> is called <em>subject</em>, <span class="math inline">\(p\)</span> is <em>predicate</em>, and <span class="math inline">\(o\)</span> is <em>object</em>. As the definition indicates, subjects can be either IRIs or blank nodes, predicates can be only IRIs, and objects can be IRIs, blank nodes, or literals. Predicates are also often referred to as properties. RDF graphs can be grouped into RDF datasets. Each graph in an RDF dataset can be associated with a name <span class="math inline">\(g \in (I \cup B)\)</span>. RDF datasets can be thus decomposed into quads <span class="math inline">\((s, p, o, g)\)</span>, where <span class="math inline">\(g\)</span> is called <em>named graph</em>.</p>
          <p>What we described above is the abstract syntax of RDF. In order to be able to exchange RDF graphs and datasets, a serialization is needed. RDF can be serialized into several concrete syntaxes, including Turtle <span class="citation" data-cites="Beckett2014">(Beckett et al. <a href="#ref-Beckett2014">2014</a>)</span>, JSON-LD <span class="citation" data-cites="Sporny2014">(Sporny et al. <a href="#ref-Sporny2014">2014</a>)</span>, or N-Quads <span class="citation" data-cites="Carothers2014">(Carothers <a href="#ref-Carothers2014">2014</a>)</span>. An example of data describing a public contract serialized in the Turtle syntax is shown in lst. <a href="#lst:example-turtle">1</a>.</p>
          <div id="lst:example-turtle" class="listing">
          <p>Listing 1: Example data in Turtle</p>
          <pre><code>@prefix contract: &lt;http://linked.opendata.cz/resource/isvz.cz/contract&gt; .
          @prefix dcterms:  &lt;http://purl.org/dc/terms/&gt; .
          @prefix pc:       &lt;http://purl.org/procurement/public-contracts#&gt; .
          
          contract:60019151 a pc:Contract ;
            dcterms:title &quot;Poskytnutí finančního úvěru&quot;@cs ;
            pc:contractingAuthority business-entity:CZ00275492 .</code></pre>
          </div>
          <!-- RDF Schema -->
          <p>RDFS <span class="citation" data-cites="Brickley2014">(Brickley and Guha <a href="#ref-Brickley2014">2014</a>)</span> is an ontological language for describing semantics of data. It provides a way to group resources as instances of classes and describe relationships among the resources. RDFS terms are endowed with inference rules that can be used to materialize data implied by the rules. Relationships between RDF resources are represented as properties. Properties are defined in RDFS in terms of their domain and range. For each RDF triple with a given property, its subject may be inferred to be an instance of the property’s domain, while its object is treated as an instance of the property’s range. Moreover, RDFS can express subsumption hierarchies between classes or properties. If more sophisticated ontological constraints are required, they can be defined by the Web Ontology Language (OWL) <span class="citation" data-cites="W3C2012">(W3C OWL Working Group <a href="#ref-W3C2012">2012</a>)</span>. RDFS and OWL can be used in tandem to create vocabularies that provide classes and properties to describe data. Vocabularies enable tools to operate on datasets sharing the same vocabulary without dataset-specific adaptations. The explicit semantics provided by RDF vocabularies makes datasets described by such vocabularies machine-understandable to a limited extent. For example, we use the Public Contracts Ontology, described in sec. <a href="#sec:pco">2.1.1</a>, for this purpose in our work.</p>
          <!-- SPARQL -->
          <p>SPARQL <span class="citation" data-cites="Harris2013">(Harris and Seaborne <a href="#ref-Harris2013">2013</a>)</span> is a query language for RDF data. The syntax of SPARQL was inspired by SQL. The <code>WHERE</code> clauses in SPARQL specify graph patterns to match in the queried data. The syntax of graph patterns extends the Turtle RDF serialization with variables, which are names prefixed either by <code>?</code> or <code>$</code>. Solutions to SPARQL queries are subgraphs that match the specified graph patterns. The solutions are subsequently processed by modifiers, such as by deduplication or ordering. Solutions are output based on the query type. ASK queries output boolean values, SELECT queries output tabular data, and CONSTRUCT or DESCRIBE queries output RDF graphs. An example SPARQL query that retrieves all classes instantiated in a dataset is shown in the lst. <a href="#lst:sparql-example">2</a>.</p>
          <div id="lst:sparql-example" class="listing">
          <p>Listing 2: Example SPARQL query</p>
          <pre><code>SELECT DISTINCT ?class
          WHERE {
            [] a ?class .
          }
          ORDER BY ?class</code></pre>
          </div>
          <!-- Linked data principles -->
          <p>Use of the above-mentioned semantic web technologies for publishing linked data is guided by four principles <span class="citation" data-cites="BernersLee2009">(Berners-Lee <a href="#ref-BernersLee2009">2009</a>)</span>:</p>
          <ol type="1">
          <li>Use IRIs as names for things.</li>
          <li>Use HTTP IRIs so that people can look up those names.</li>
          <li>When someone looks up a IRI, provide useful information, using the standards (RDF, SPARQL).</li>
          <li>Include links to other IRIs, so that they can discover more things.</li>
          </ol>
          <p>Besides prescribing the way to identify resources, the principles describe how to navigate linked data. The principles invoke the mechanism of dereferencing, by which an HTTP request to a resource’s IRI should return the resource’s description in RDF.</p>
          <p>Linked data invokes several assumptions that have implications for its users. <em>Non-unique name assumption</em> (non-UNA) posits that two names (identifiers) may refer to the same entity unless explicitly stated otherwise. This assumption implies that deduplication may be needed if identifiers are required to be unique. <em>Open world assumption</em> (OWA) supposes that <em>“the truth of a statement is independent of whether it is known. In other words, not knowing whether a statement is explicitly true does not imply that the statement is false”</em> <span class="citation" data-cites="Hebeler2009">(Hebeler et al. <a href="#ref-Hebeler2009">2009</a>, p. 103)</span>. Due to OWA we cannot infer that missing statements are false. However, it allows us to model incomplete data. This is useful in matchmaking, where <em>“the absence of a characteristic in the description of a supply or demand should not be interpreted as a constraint”</em> <span class="citation" data-cites="DiNoia2007">(Di Noia et al. <a href="#ref-DiNoia2007">2007</a>, p. 279)</span>. Nonetheless, OWA poses a potential problem for classification tasks in machine learning, because linked data rarely contains explicit negative examples <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 272)</span>. The principle of <em>Anyone can say anything about anything</em> (AAA) assumes that the open world of linked data provides no guarantees that the assertions published as linked data are consistent or uncontradictory. Given this assumption, quality assessment followed by data pre-processing is typically required when using linked data.</p>
          <!-- Benefits of linked data -->
          <p>Having considered the characteristics of linked data we may highlight its advantages. Many of these advantages are related to data preparation, which we point out in sec. <a href="#sec:data-preparation">2</a>, however, linked data can also benefit matchmaking in several ways. This overview draws upon the benefits of linked data for recommender systems identified in related research <span class="citation" data-cites="DiNoia2014 DiNoia2016">(Di Noia et al. <a href="#ref-DiNoia2014">2014</a>, <a href="#ref-DiNoia2016">2016</a>)</span>, since these benefits apply to matchmaking too.</p>
          <p>Unlike textual content, linked data is structured, so there is less need for structuring it via content analysis. RDF gives linked data not only its structure but also a flexibility to model diverse kinds of data. Both content and preferences in recommender systems or matchmaking, such as contract awards in our case, can be expressed in RDF in an uniform way in the same feature space, which simplifies operations on the data. Moreover, the common data model enables combining linked data with external linked datasets that can provide valuable additional features. The mechanism of tagging literal values with language identifiers also makes for an easy representation of multilingual data, such as in the case of cross-country procurement in the EU.</p>
          <p>The features in RDF are endowed with semantics originating in RDF vocabularies. The explicit semantics makes the features more telling, as opposed to features produced by shallow content analysis <span class="citation" data-cites="Jannach2010">(Jannach et al. <a href="#ref-Jannach2010">2010</a>, p. 75)</span>, such as keywords. While traditional recommender systems are mostly unaware of the semantics of the features they use, linked data features do not have to be treated like black boxes, since their expected interpretations can be looked up in the corresponding RDF vocabularies that define the features.</p>
          <p>If the values of features are resources compliant with the linked data principles, their IRIs can be dereferenced to obtain more features from the descriptions of the resources. In this way, linked data allows to automate the retrieval of additional features. IRIs of linked resources can be automatically crawled to harvest contextual data. Furthermore, crawlers may recursively follow links in the obtained data. The links between datasets can be used to provide cross-domain recommendations. In such scenario, preferences from one domain can be used to predict preferences in another domain. For example, if in our case we combine data from business and public procurement registers, we may leverage the links between business entities described with concepts from an economic classification to predict their associations to concepts from a procurement classification. If there is no overlap between the resources from the combined datasets, there may be at least an overlap in the RDF vocabularies describing the resources <span class="citation" data-cites="Heitmann2016">(Heitmann and Hayes <a href="#ref-Heitmann2016">2016</a>)</span>, which provide broader conceptual associations.</p>
          <!--
          - Results contrary to the dissertation that more features lead to better recommendations: *"We point out that even if very few ratings are available, simple rating-based predictors outperform purely metadata-based ones"* [@Pilaszy2009, p. 93].
          
          Semantic features ~ smart descriptors
          - Low-level descriptors require complex distance metrics.
          - High-level, smart descriptors can work with simpler distance metrics.
          -->
          </section>
          </section>
          <section id="sec:public-procurement" class="level2">
          <h2><span class="header-section-number">1.5</span> Public procurement domain</h2>
          <p>Our work targets the domain of public procurement. In particular, we apply the developed matchmaking methods to data describing the Czech public procurement. Public procurement is the process by which public bodies purchase products or services from companies. Public bodies make such purchases in public interest in order to pursue their mission. For example, public procurement can cater for road repairs or arrange supplies of electricity. Bodies issuing public contracts, such as ministries or municipalities, are referred to as contracting authorities. Companies competing for contract awards are called bidders. Since public procurement is a legal domain, public contracts are legally enforceable agreements on purchases financed from public funds. Public contracts are publicized and monitored by contract notices. Contract notices announce competitive bidding for the award of public contracts <span class="citation" data-cites="Distinto2016">(Distinto et al. <a href="#ref-Distinto2016">2016</a>, p. 14)</span> and update the progress of public contracts as they go through their life cycle, ending either in completion or cancellation. In our case we deal with public contracts that can be described more precisely as proposed contracts <span class="citation" data-cites="Distinto2016">(Distinto et al. <a href="#ref-Distinto2016">2016</a>, p. 14)</span> until they are awarded and agreements with suppliers are signed. We use the term “public contract” as a conceptual shortcut to denote the initial phase of contract life-cycle.</p>
          <p>Public procurement is an uncommon domain for recommender and matchmaking systems. Recommender systems are conventionally used in domains of leisure, such as books, movies, or music. In fact, the <em>“experiment designs that evaluate different algorithm variants on historical user ratings derived from the movie domain form by far the most popular evaluation design and state of practice”</em> <span class="citation" data-cites="Jannach2010">(Jannach et al. <a href="#ref-Jannach2010">2010</a>, p. 175)</span> in recommender systems. Our use case thereby constitutes a rather novel application of these technologies.</p>
          <p>Matchmaking in public procurement can be framed in its legal and economic context.</p>
          <section id="sec:legal-context" class="level3">
          <h3><span class="header-section-number">1.5.1</span> Legal context</h3>
          <p>Public procurement is a domain governed by law. We are focused on the Czech public procurement, for which there are two primary sources of relevant law, including the national law and the EU law. Public procurement in the Czech Republic is governed by the act no. 2016/134 <span class="citation" data-cites="CzechRepublic2016">(Czech Republic <a href="#ref-CzechRepublic2016">2016</a>)</span>. Czech Republic, as a member state of the European Union, harmonises its law with EU directives, in particular the directives 2014/24/EU <span class="citation" data-cites="EU2014a">(EU <a href="#ref-EU2014a">2014</a><a href="#ref-EU2014a">a</a>)</span> and 2014/25/EU <span class="citation" data-cites="EU2014b">(EU <a href="#ref-EU2014b">2014</a><a href="#ref-EU2014b">b</a>)</span> in case of public procurement. The first directive regulates public procurement of works, supplies, or services, while the latter one regulates public procurement of utilities, including water, energy, transport, and postal services. The act no. 2016/134 transposes these directives into the Czech legislation. Besides legal terms and conditions to harmonize public procurement in the EU member states, these directives also define standard forms for EU public procurement notices,<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> which constitute a common schema of public notices. The directives design Tenders Electronic Daily (“Supplement to the Official Journal”)<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> to serve as the central repository of public notices conforming to the standard forms.</p>
          <p>In an even broader context, the EU member states adhere to the Agreement on Government Procurement (GPA)<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> set up by the World Trade Organization. GPA mandates the involved parties to observe rules of fair, transparent, and non-discriminatory public procurement. In this way, the agreement sets basic expectations facilitating international public procurement.</p>
          <p>Legal regulation of public procurement has important implications for matchmaking, including explicit formulation of demands, their proactive disclosure, desire for conformity, and standardization. Public procurement law requires explicit formulation of demands in contract notices to ensure a basic level of transparency. In most markets only supply is described explicitly, such as through advertising, while demand is left implicit. Since matchmaking requires demands to be specified, public procurement makes for a suitable market to apply the matchmaking methods.</p>
          <!-- Complex representation -->
          <p>Since public procurement often pursues multiple objectives, public contracts are demands with variable degrees of complexity and completeness. Their explicit formulation thus requires sufficiently expressive modelling, making it a fitting use case for the semantic web technologies, including RDF and RDF Schema. Public contracts may stipulate non-negotiable qualification criteria as well as setting desired, but negotiable qualities sought in bidders. The objects of public contracts are often heterogeneous products or services, that cannot be described only in terms of price. Apart from their complex representation, public contracts have many features unavailable as structured data. These features comprise unstructured documentation or undisclosed terms and conditions. Consequently, matchmaking has to operate on simplified models of public contracts. The data model for public contracts we designed is described in the sec. <a href="#sec:modelling">2.1</a>.</p>
          <!-- Proactive disclosure -->
          <p>There is a legal mandate for proactive disclosure of contract notices. Public contracts that meet the prescribed minimum conditions, including thresholds for the amounts of money spent, must be advertised publicly <span class="citation" data-cites="Graux2012">(Graux and Tom <a href="#ref-Graux2012">2012</a>, p. 7)</span>. Moreover, since public contracts in the EU are classified as public sector information, they fall within the regime of mandatory public disclosure under the terms of the Directive on the re-use of public sector information <span class="citation" data-cites="EU2013">(EU <a href="#ref-EU2013">2013</a>)</span>. In theory, this provides equal access to contract notices for all members of the public without the need to make requests for the notices, which in turn helps to enable fair competition in the public procurement market. In practice, the disclosure of public procurement data is often lagging behind the stipulations of law.</p>
          <!-- Desire for conformity -->
          <p>Overall, public procurement is subject to stringent and complex legal regulations. Civil servants responsible for public procurement therefore put a strong emphasis on legal conformance. Moreover, contracting authorities strive at length to make evaluation of contract award criteria incontestable in order to avoid protracted appeals of unsuccessful bidders that delay realization of contracts. Consequently, representatives of contracting authorities may exhibit high risk aversion and desire for conformity at the cost of compromising economical advantageousness. For example, the award criterion of lowest price may be overused because it decreases the probability of an audit three times, even though it often leads to inefficient contracts, as observed for the Czech public procurement by <span class="citation" data-cites="Nedved2017">Nedvěd et al. (<a href="#ref-Nedved2017">2017</a>)</span>. Desire for conformity can explain why not deviating from defaults or awarding popular bidders may be perceived as a safe choice. In effect, this may imply there is less propensity for diversity in recommendations produced via matchmaking. On the one hand, matchmaking may address this by trading in improved accuracy for decreased diversity in matchmaking results. On the other hand, it may intentionally emphasize diversity to offset the desire for conformity. <!--
          Qualitative evaluation via interviews with civil servants may potentially indicate which direction is preferable.
          --></p>
          <!-- Standardization -->
          <p>Finally, legal regulations standardize the communication in public procurement. Besides prescribing procedures that standardize how participants in public procurement communicate, it standardizes the messages exchanged between the participants. Contracting authorities have to disclose public procurement data following the structure of standard forms for contract notices. The way in which public contracts are described in these forms is standardized to some degree via shared vocabularies and code lists, such as the Common Procurement Vocabulary (CPV) or the Nomenclature of Territorial Units for Statistics (NUTS). Standardization is especially relevant in the public sector, since it is characterized by <em>“a variety of information, of variable granularity and quality created by different institutions and represented in heterogeneous formats”</em> <span class="citation" data-cites="Euzenat2013">(Euzenat and Shvaiko <a href="#ref-Euzenat2013">2013</a>, p. 12)</span>.</p>
          <p>Standardization of data contributes to defragmentation of the public procurement market. Defragmentation of the EU member states’ markets is the prime goal of the EU’s common regulatory framework. It aims to create a single public procurement market that enables cross-country procurement among the member states. Standardization simplifies the reuse of public procurement data by third parties, such as businesses or supervisory public bodies. Better reuse of data balances the information asymmetries that fragment the public procurement market.</p>
          <!-- Imperfect standardization -->
          <p>Nevertheless, public procurement data is subject to imperfect standardization, which introduces variety in it. The imperfect standardization is caused by divergent transpositions of EU directives into the legal regimes of EU member states, lack of adherence to standards, underspecified standards leaving open space for inconsistencies, or meagre incentives and sanctions for abiding by the standards and the prescribed practices. Violations of the prescribed schema, lacking data validation, and absent enforcement of the mandated practices of public disclosure require a large effort from those wanting to make effective use of the data. For example, tasks such as search in aggregated data or establishing the identities of economic operators suffer from data inconsistency. Moreover, public procurement data can be distributed across disparate sources providing varying level of detail and completeness, such as in public profiles of contracting authorities and central registers. Fragmentation of public procurement data thus requires further data integration in order for the consumer to come to a unified view of the procurement domain that is necessary for conducting fruitful data analyses. In fact, one of the reasons why the public procurement market is dominated by large companies may be that they, unlike small and medium-sized enterprises, can afford the friction involved in processing the data.</p>
          <p>According to our approach to data preparation, linked data provides a way to compensate the impact of imperfect standardization. While a standard can be defined as <em>“coordination mechanism around non-proprietary knowledge that organizes and directs technological change”</em> <span class="citation" data-cites="Gosain2003">(Gosain <a href="#ref-Gosain2003">2003</a>, p. 18)</span>, linked data enables to cope with insufficient standardization by allowing for <em>“cooperation without coordination”</em> <span class="citation" data-cites="Wood2011">(Wood <a href="#ref-Wood2011">2011</a>, p. 5)</span> or without centralization. Instead, linked data bridges local heterogeneities via the flexible data model of RDF and explicit links between the decentralized data sources. We describe our use of linked data in detail in the sec. <a href="#sec:data-preparation">2</a>.</p>
          </section>
          <section id="economic-context" class="level3">
          <h3><span class="header-section-number">1.5.2</span> Economic context</h3>
          <p>Public procurement constitutes a large share of the volume of transactions in the economy. The share of expenditures in the EU member states’ public procurement on works, goods, and services (excluding utilities) was estimated to be <em>“13.1 % of the EU GDP in 2015, the highest value for the last 4 years”</em> <span class="citation" data-cites="EuropeanCommission2016">(European Commission <a href="#ref-EuropeanCommission2016">2016</a>)</span>. This estimate amounted to 24.2 billion EUR in 2015 in the Czech Republic, which translated to 14.5 % of the country’s GDP <span class="citation" data-cites="EuropeanCommission2016">(European Commission <a href="#ref-EuropeanCommission2016">2016</a>)</span>. Compared with the EU, the Czech Republic exhibits consistent above-average values of this indicator, as can be seen in the fig. <a href="#fig:gdp">1</a>.</p>
          <figure>
          <img src="resources/img/share_of_gdp_v2.png" alt="Figure 1: Percentage of public procurement’s share of GDP. Source: Public Procurement Indicators 2012-2015 (European Commission 2016)" id="fig:gdp" /><figcaption>Figure 1: Percentage of public procurement’s share of GDP. Source: Public Procurement Indicators 2012-2015 <span class="citation" data-cites="EuropeanCommission2016">(European Commission <a href="#ref-EuropeanCommission2016">2016</a>)</span></figcaption>
          </figure>
          <!-- Economy of scale -->
          <p>The large volume of transactions in public procurement gives rise to economies of scale, so that even minor improvements can accrue substantial economic impact, since the scale of operations in this domain provides ample opportunity for cost savings. Publishing open data on public procurement as well as using matchmaking methods can be considered among the examples of such improvements, which can potentially increase the efficiency of resource allocation in the public sector, as mentioned in the sec. <a href="#sec:open-data">1.4.1</a>.</p>
          <!-- Passive waste -->
          <p>Due to the volume of the public funds involved in public procurement, it is prone to waste and political graft. Wasteful spending in public procurement can be classified either as active waste, which entails benefits to the public decision maker, or as passive waste, which does not benefit the decision maker. Whereas active waste may result from corruption or clientelism, passive waste proceeds from inefficiencies caused by the lack of skills or incentives. Although active waste is widely perceived to be the main problem of public procurement, a study of the Italian public sector <span class="citation" data-cites="Bandiera2009">(Bandiera et al. <a href="#ref-Bandiera2009">2009</a>, p. 1282)</span> observed that 83 % of uneconomic spending in public procurement can be attributed to passive waste. We therefore decided to focus on optimizing public procurement where most impact can be expected. We argue that matchmaking can help improve the public procurement processes cut down passive waste. It can assist civil servants by providing relevant information, thus reducing the decision-making effort related to public procurement processes. <!--
          Moreover, communication protocols can reduce corruption potential.
          In particular, protocols that automate interactions between contracting authorities and bidders leave less leeway.
          For instance, *"automatic matching services can find suitable pairs for a resource transfer.
          The transfer itself can be mediated by a protocol, reducing human interaction to a necessary minimum."*^[<https://sat.researchstudio.at/en/web-of-needs>]
          On the other hand, automation may be corrupted too.
          --> We identified several use cases in public procurement where matchmaking can help.</p>
          </section>
          <section id="use-cases-for-matchmaking" class="level3">
          <h3><span class="header-section-number">1.5.3</span> Use cases for matchmaking</h3>
          <p>Matchmaking covers the information phase of market transaction <span class="citation" data-cites="Schmid1998">(Schmid and Lindemann <a href="#ref-Schmid1998">1998</a>, p. 194)</span> that corresponds to the preparation and tendering stages in public procurement life-cycle <span class="citation" data-cites="Necasky2014">(Nečaský et al. <a href="#ref-Necasky2014">2014</a>, p. 865)</span>. During this phase <em>“participants to the market seek potential partners”</em> <span class="citation" data-cites="DiNoia2004">(Di Noia et al. <a href="#ref-DiNoia2004">2004</a>)</span>, so that public bodies learn about relevant bidders and companies learn about relevant open calls. In this sense, demands for products and services correspond to information needs and the aim of matchmaking is to retrieve the information that will satisfy them. Several use cases for matchmaking follow from the public procurement legislation according to the procedure types chosen for public contracts.</p>
          <p>Public procurement law defines types of procedures that govern how contracting authorities communicate with bidders. In particular, procedure types determine what data on public contracts is published, along with specifying who has access to it and when it needs to be made available. The procedure types can be classified either as open or as restricted. Open procedures mandate contracting authorities to disclose data on contracts publicly, so that any bidders can respond with offers. In this case, contracting authorities do not negotiate with bidders and contracts are awarded solely based on the received bids. Restricted procedures differ by including an extra screening step. As in open procedures, contracting authorities announce contracts publicly, but bidders respond with expression of interest instead of bids. Contracting authorities then screen the interested bidders and send invitations to tender to the selected bidders.</p>
          <p>The chosen procedure type determines for which users is matchmaking relevant. Bidders can use matchmaking both in case of open and restricted procedures to be alerted about the current business opportunities in public procurement that are relevant to them. Contracting authorities can use matchmaking in restricted procedures to get recommendations of suitable bidders. <!-- A potential issue: A restricted procedure is probably chosen when the contracting authority already knows which bidders to invite. --> Moreover, in case of the simplified under limit procedure, which is allowed in the Czech Republic for public contracts below a specified financial threshold, contracting authority can approach bidders directly. In such case, at least five bidders must be approached according to the act no. 2016/134 <span class="citation" data-cites="CzechRepublic2016">(Czech Republic <a href="#ref-CzechRepublic2016">2016</a>)</span>. <!-- <https://www.zakonyprolidi.cz/cs/2016-134#p53> --> In that scenario, matchmaking can help recommend appropriate bidders to interest in the public contract. There are also other procedure types, such as innovation partnership, in which matchmaking is applicable to a lesser extent.</p>
          <p>An additional use case for similarity-based retrieval employed by matchmaking may occur during contract specification. The Czech act no. 2016/134 <span class="citation" data-cites="CzechRepublic2016">(Czech Republic <a href="#ref-CzechRepublic2016">2016</a>)</span> suggests contracting authorities to estimate contract price based on similar contracts. <!-- <https://www.zakonyprolidi.cz/cs/2016-134#f5805154> --> In order to address this use case, based on incomplete descriptions of contracts matchmaking can recommend similar contracts, the actual prices of which can help estimate the price of the formulated contract.</p>
          <!--
          A particular feature of public procurement is that it is a domain where interests of public and private sector meet.
          It is a domain situated at the intersection of the public and private sector where different motivations overlap.
          Public contracts constitute a specific case of demand that represents public interest, while the side of supplies is driven by private interests.
          
          Problems of transparency:
          
          - Matchmaking will be used only by willing civil servants.
          - Corruption scandals are ignored and eventually forgotten by the public.
          
          B2B context: although contracting authorities are not necessarily typical "businesses"
          
          Although the share of cross-country procurement is minimal. See <http://www.govtransparency.eu/wp-content/uploads/2016/03/Fazekas-Skuhrovec_OECD-Integrity-Forum_draft_160321_towebsite.pdf>
          
          * Single market for cross-country public procurement: However, the public procurements markets in the EU member states are fragmented.
          * Clientelism, collusion, bid rigging: contributes to fragmentation of the public procurement market
          
          Civil servants are not motivated, so they behave like satisficers, searching for "good enough" solutions.
          -->
          </section>
          </section>
          <section id="matchmaking" class="level2">
          <h2><span class="header-section-number">1.6</span> Matchmaking</h2>
          <p>Matchmaking is an information retrieval task that ranks pairs of demands and offers according to the degree to which the offer satisfies the demand. It is a <em>“process of searching the space of possible matches between demand and supplies”</em> <span class="citation" data-cites="DiNoia2004">(Di Noia et al. <a href="#ref-DiNoia2004">2004</a>, p. 9)</span>. For example, matchmaking can pair job seekers with job postings, discover suitable opponents for doctoral theses, or match romantic partners.</p>
          <!-- Dual perspective: data vs. query -->
          <p>Matchmaking recasts either demands or offers as queries, while the rest is treated as data to query. In this setting, <em>“the choice of which is the data, and which is the query depends just on the point of view”</em> <span class="citation" data-cites="DiNoia2004">(Di Noia et al. <a href="#ref-DiNoia2004">2004</a>)</span>. Both data describing offers and data about demands can be turned either into queries or into queried data. For example, in our case we may treat public contracts as queries for suitable bidders, or, vice versa, bidder profiles may be recast as preferences for public contracts. Matchmakers are given a query and produce <span class="math inline">\(k\)</span> results best-fulfilling the query <span class="citation" data-cites="DiNoia2007">(Di Noia et al. <a href="#ref-DiNoia2007">2007</a>, p. 278)</span>. Viewed from this perspective, matchmaking can be considered a case of top-<span class="math inline">\(k\)</span> retrieval.</p>
          <!-- Complex data -->
          <p>Matchmaking typically operates on complex data structures. Both demands and supplies may combine non-negotiable restrictions with more flexible requirements or vague semi-structured descriptions. Descriptions of demands and offers thus cannot be reduced to a single dimension, such as a price tag. Matchmakers operating on such complex data often suffer from the curse of dimensionality. It implies that linear increase in dimensionality may cause an exponential growth of negative effects. Complex descriptions make demands and offers difficult to compare. Since demands and supplies are usually complex, <em>“most real-world problems require multidimensional matchmaking”</em> <span class="citation" data-cites="Veit2001">(Veit et al. <a href="#ref-Veit2001">2001</a>)</span>. For example, matchmaking may involve similarity functions that aggregate similarities of individual dimensions.</p>
          <!-- Semantic matchmaking -->
          <p>Our work focuses on semantic matchmaking that requires a semantic level of agreement between offers and demands. In order to be able to compare descriptions of offers or demands, they need to share the same semantics <span class="citation" data-cites="GonzalezCastillo2001">(González-Castillo et al. <a href="#ref-GonzalezCastillo2001">2001</a>)</span>. Semantic matchmaking thus describes both queries and data <em>“with reference to a shared specification of a conceptualization for the knowledge domain at hand, i.e., an ontology”</em> <span class="citation" data-cites="DiNoia2007">(Di Noia et al. <a href="#ref-DiNoia2007">2007</a>, p. 270)</span>. Ontologies give the descriptions of entities involved in matchmaking comparable schemata. Data pre-processing may reformulate demands and offers to be comparable, e.g., by aligning their schemata. In order to be able to leverage the semantic features of data, our approach can be thus regarded as schema-aware, as opposed to schema-agnostic matchmaking</p>
          <!-- Distinction between matchmaking and recommender systems -->
          <p>Matchmaking overlaps with recommender systems in many respects. Both employ similar methods to achieve their task. However, <em>“every recommender system must develop and maintain a user model or user profile that, for example, contains the user’s preferences”</em> <span class="citation" data-cites="Jannach2010">(Jannach et al. <a href="#ref-Jannach2010">2010</a>, p. 1)</span>. Instead of using user profiles, matchmaking uses queries. Although this is a simplifying description and the distinction between matchmaking and recommender systems is in fact blurry, designating our work as matchmaking is more telling.</p>
          <!-- Ambiguity of matchmaking -->
          <p>Besides the similarities with recommender systems, matchmaking may invoke different connotations, as the term is used in other disciplines that imbue it with different meanings. For instance, it appears in graph theory naming the task of producing subsets of edges without common vertices. To avoid this ambiguity, in this text we will use the term “matchmaking” only in the way described here.</p>
          <!--
          Calls for tenders are queries.
          Matchmaking mediates between offers and demands.
          Demand and supply meet
          
          Public contracts ~ history of queries ~ query log
          
          Should we distinguish two phases of matchmaking?
          1. Filtering: satisfaction of non-negotiable constraints
          2. Ranking: ordering results based on the degree they satisfy the query
          -->
          <!--
          *"Matchmaking is generally defined as the ranking of a set of offers according to a request"* [@Agarwal2005].
          *"Matchmaking is an information retrieval task whereby queries (a.k.a. demands) and resources (a.k.a. supplies) are expressed using semi-structured data in the form of advertisements, and task results are ordered (ranked) lists of those resources best fulfilling the query"* [@DiNoia2007, p. 278].
          *"the objective of a matchmaking process is to discover best available offers to a given request"* [@DiNoia2007, p. 269]
          -->
          <p>We adapted two general approaches for matchmaking: case-based reasoning and statistical relational learning. Both have many things in common and employ similar techniques to achieve their goal. Both learn from past data to produce predictions that are not guaranteed to be correct. A more detailed comparison of case-based reasoning with machine learning is in Richter and Weber <span class="citation" data-cites="Richter2013">(<a href="#ref-Richter2013">2013</a>, p. 531)</span>.</p>
          <section id="sec:cbr" class="level3">
          <h3><span class="header-section-number">1.6.1</span> Case-based reasoning</h3>
          <!-- Definitions -->
          <p>Case-base reasoning (CBR) is a problem solving methodology that learns from experiences of previously solved problems, which are called cases <span class="citation" data-cites="Richter2013">(Richter and Weber <a href="#ref-Richter2013">2013</a>, p. 17)</span>. A case consists of a problem specification and a problem solution. Experiences described in cases can be either positive or negative. Positive experiences propose solutions to be reused, whereas the negative ones indicate solutions to avoid. For example, experiences may concern diagnosing a patient and evaluating the outcome of the diagnosis, which may be either successful or unsuccessful. Cases are stored and organized in a case base, such as a database. Case base serves as a memory that retains the experiences to learn from.</p>
          <p>The workings of CBR systems can be described in terms of the CBR cycle <span class="citation" data-cites="Kolodner1992">(Kolodner <a href="#ref-Kolodner1992">1992</a>)</span>. The cycle consists of four principal steps a CBR system may iterate through:</p>
          <ol type="1">
          <li>Retrieve</li>
          <li>Reuse</li>
          <li>Revise</li>
          <li>Retain</li>
          </ol>
          <p>In the <em>Retrieve</em> step a CBR system gets cases similar to the query problem. Case bases are thus usually built for efficient similarity-based retrieval. Since descriptions of cases are often complex, computing their similarity may involve determining and weighting similarities of their individual features. For each use case and each feature a different similarity measure may be adopted, which allows to use pairwise similarity metrics tailored for particular kinds of data. This also enables assigning each feature a different weight, so that more relevant features may be emphasized. The employed metrics may be either symmetric or asymmetric. For example, we can use an asymmetric metric to favour lower prices over higher prices, even though their distance to the price in the query is the same. Since the similarity metrics allow fuzzy matches, reasoning in CBR systems is approximate. As Richter and Weber argue, <em>“the most important characteristic that distinguishes case-based reasoning from other kinds of reasoning is that it does not lead from true assumptions to true conclusions”</em> <span class="citation" data-cites="Richter2013">(<a href="#ref-Richter2013">2013</a>, p. 18)</span>.</p>
          <p>A key feature of CBR is that similarity computation typically requires background knowledge. While similarity of ordinal features can be determined without it, categorical features call for additional knowledge to assess their degree of similarity. For instance, a taxonomy may be used to compute similarity as the inverse of taxonomic distance between the values of the compared feature. Since hand-coding background knowledge is expensive, and typically requires assistance of domain experts, CBR research considered alternatives for knowledge acquisition, such as using external semantics from linked open data or discovering latent semantics via machine learning.</p>
          <p>The retrieved nearest neighbour cases serve as potential sources of a solution to the query problem. Solutions of these cases are copied and adapted in the <em>Reuse</em> step to formulate a solution answering the query. If a solved case matches the problem at hand exactly, we may directly reuse its solution. However, exact matches are rare, so the solutions to matching cases often need to be adapted. For example, solutions may be reused at different levels. We may either reuse the process that generated the solution, reuse the solution itself, or do something in between.</p>
          <p>The reused solution is evaluated in the <em>Revise</em> step to assess whether it is applicable to the query problem. Without this step a CBR system cannot learn from its mistakes. It is the step in which CBR may add user feedback.</p>
          <p>Finally, in the <em>Retain</em> step, the query problem and its revised and adopted solution may be incorporated in the case base as a new case for future learning. Alternatively, the generated case may be discarded if the CBR system stores only the actual cases.</p>
          <p>The CBR cycle may be preceded by preparatory steps described by Richter and Weber <span class="citation" data-cites="Richter2013">(<a href="#ref-Richter2013">2013</a>)</span>. A CBR system can be initialized by the <em>Knowledge representation</em> step, which structures the knowledge contained in cases the system learns from. Cases are explicitly formulated and described in a structured way, so that their similarity can be determined effectively. The simplest representation of a case is a set of feature-value pairs. However, using more sophisticated data structures is common. In order to compute similarity of cases, they must be described using comparable features, or, put differently, the descriptions of cases must adhere to the same schema.</p>
          <p><em>Problem formulation</em> is a preliminary step in which a query problem is formulated. A query can be considered a partially specified case. It may be either underspecified, such that it matches several existing cases, or overspecified, if it has no matches due to being too specific. Underspecified queries may require solutions from the matching cases to be combined, while overspecified queries may need to be relaxed or provided with partial matches.</p>
          <p>Overall, the CBR cycle resembles human reasoning, such as problem solving by finding analogies. In fact, the CBR research is rooted in psychology and cognitive science. It is also similar to case law, which reasons from precedents to produce new interpretations of law. Thanks to these similarities, CBR is perceived as natural by its users <span class="citation" data-cites="Kolodner1992">(Kolodner <a href="#ref-Kolodner1992">1992</a>)</span>, which makes its function usually easy to explain.</p>
          <!-- Case-based recommenders -->
          <p>CBR is commonly employed in recommender systems. Case-based recommenders are classified as a subset of knowledge-based recommenders <span class="citation" data-cites="Jannach2010">(Jannach et al. <a href="#ref-Jannach2010">2010</a>)</span>. Similarly to collaborative recommendation approaches, case-based recommenders exploit data about past behaviour. However, unlike collaborative recommenders, <em>“the case-based approach enjoys a level of transparency and flexibility that is not always possible with other forms of recommendation”</em> <span class="citation" data-cites="Smyth2007">(Smyth <a href="#ref-Smyth2007">2007</a>, p. 370)</span>, since it is based on reasoning with explicit knowledge. Our adaptation of CBR for matchmaking can be thus considered a case-based recommender.</p>
          <!--
          CBR is based on extensional semantics.
          Instead of defining a solution to a case as a class of instances, CBR defines it simply by the set of cases, without synthesizing general class characteristics.
          
          Important factors for case bases:
          1. Representational fidelity (knowledge representation of cases)
          2. Size (more cases to learn from)
          -->
          </section>
          <section id="sec:srl" class="level3">
          <h3><span class="header-section-number">1.6.2</span> Statistical relational learning</h3>
          <p>Statistical relational learning (SRL) is a subfield of machine learning that is concerned with learning from relational data. SRL learns models that <em>“describe probability distributions <span class="math inline">\(P(\{X\})\)</span> over the random variables in a relational domain”</em> <span class="citation" data-cites="Tresp2014">(Tresp and Nickel <a href="#ref-Tresp2014">2014</a>, p. 1554)</span>. Here, <span class="math inline">\(X\)</span> denotes a random variable and <span class="math inline">\(\{X\}\)</span> refers to a set of such variables in a relational domain. The learned model reflects the characteristic patterns and global dependencies in relational data. Unlike inference rules, these statistical patterns may not be universally true, but have useful predictive power nonetheless. An example of such pattern is homophily <span class="citation" data-cites="McPherson2001">(McPherson et al. <a href="#ref-McPherson2001">2001</a>)</span>, which describes the tendency of similar entities to be related. A model created by SRL is used to predict probabilities of unknown relations in data. In other words, in SRL <em>“the underlying idea is that reasoning can often be reduced to the task of classifying the truth value of potential statements”</em> <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 271)</span>.</p>
          <!-- #### Collective learning with latent feature models -->
          <p>There are two basic kinds of SRL models: models with observable features and models with latent features. Our work focuses on the latent feature models. Unlike observable features, latent features are not directly observed in data. Instead, they are assumed to be the hidden causes for the observed variables. Consequently, results from machine learning based on latent features are usually difficult to interpret. Latent feature models are used to derive relationships between entities from interactions of their latent features <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 17)</span>. Since latent features correspond to global patterns in data, they can be considered products of collective learning.</p>
          <!--
          Latent feature models (tensor factorization) assume that features are conditionally independent.
          Latent features model global patterns. On the other hand, graph feature models are suitable for local patterns.
          Creating latent features is also called predicate invention.
          -->
          <p>Collective learning <em>“refers to the effect that an entity’s relationships, attributes, or class membership can be predicted not only from the entity’s attributes but also from information distributed in the network environment of the entity”</em> <span class="citation" data-cites="Tresp2014">(Tresp and Nickel <a href="#ref-Tresp2014">2014</a>, p. 1550)</span>. It involves <em>“automatic exploitation of attribute and relationship correlations across multiple interconnections of entities and relations”</em> <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 272)</span>. The exploited contextual information propagates through relations in data, so that the inferred dependencies may span across entity boundaries and involve entities that are more distant in a relational graph. Among other things, this feature of collective learning can help cope with modelling artefacts in RDF, such as intermediate resources that decompose n-ary relations into binary predicates.</p>
          <!--
          *"In relational data, the similarity of entities is determined by the similarity of their relationships"* [@Nickel2013c, p. 620]
          *"Some Relational Machine Learning approaches can exploit contextual information that might be more distant in the relational graph, a capability often referred to as collective learning."* [@Nickel2012, p. 271]
          Information propagates through the relations in data.
          *"Collective learning is a form of relational learning where information distant in the graph can be made useful."*
          <http://www.eswc2012.org/sites/default/files/Tutorial4-Material.pdf>
          dependencies can be derived across entity boundaries, such as homophily (entities tend to be associated with similar entities)
          Importance for modelling artefacts, such as intermediate resources that decompose complex relations into binary predicates: *"Since attributes and complex relations are often connected by intermediary nodes such as blank nodes or abstract entities when modeled according to the RDF formalism, this collective learning ability of RESCAL is a very important feature for learning on the Semantic Web."* [@Nickel2012, p. 272]
          Transitivity of relations, discovery of indirect relations
          -->
          <p>Collective learning is a distinctive feature of SRL and is particularly manifest in <em>“domains where entities are interconnected by multiple relations”</em> <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span>. Conversely, traditional machine learning expects data from a single relation, usually provided in a single propositional table. It considers only attributes of the involved entities, which are assumed to be mutually independent. This is one of the reasons that can explain why SRL was demonstrated to be able to produce superior results for relational data when compared to learning methods that do not take relations into account <span class="citation" data-cites="Tresp2014">(Tresp and Nickel <a href="#ref-Tresp2014">2014</a>, p. 1551)</span>. These results mark the importance of being able to leverage the relations in data effectively.</p>
          <p>Nowadays the relevance of SRL grows as relational data becomes still more prevalent. In fact, many datasets have relational nature. For instance, vast amounts of relational data are produced by social networking sites. Relational data appears in many contexts, including relational databases, ground predicates in first order logic, or RDF.</p>
          <p>Using relational datasets is nevertheless challenging, since many of them are incomplete or noisy and contain uncertain or false statements. Fortunately, SRL is relatively robust to inconsistencies, noise, or adversarial input, since it utilizes non-deterministic dependencies in data. Yet it is worth noting that even though SRL usually copes well with faulty data, systemic biases in the data will manifest in biased results produced by this method.</p>
          <!-- #### Statistical relational learning for linked open data -->
          <p>LOD is a prime example of a large-scale source of relational data afflicted with the above-mentioned ills. The open nature of LOD has direct consequences for data inconsistency and noisiness. These consequences make LOD challenging for reasoning and querying. While SRL can overcome these challenges to some extent, they pose a massive hurdle for traditional reasoning using inference based on description logic. Logical inference imposes strict constraints on its input, which are often violated in real-world data <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 28)</span>:</p>
          <blockquote>
          <p><em>“Concerning requirements on the input data, it is quite unrealistic to expect that data from the open Semantic Web will ever be clean enough such that classical reasoning systems will be able to draw useful inferences from them. This would require Semantic Web data to be engineered strongly according to shared principles, which not only contrasts with the bottom-up nature of the Web, but is also unrealistic in terms of conceptual realizability: many statements are not true or false, they rather depend on the perspective taken.”</em> <span class="citation" data-cites="Hitzler2010">(Hitzler and Harmelen <a href="#ref-Hitzler2010">2010</a>, p. 42)</span></p>
          </blockquote>
          <p>To compound matters further, reasoning with ontologies is computationally demanding, which makes it difficult to scale to the larger datasets in LOD. While we cannot guarantee most LOD datasets to be sound enough for reasoning based on logical inference, <em>“it is reasonable to assume that there exist many dependencies in the LOD cloud which are rather statistical in nature than deterministic”</em> <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 271)</span>. Approximate reasoning by SRL is well-suited to exploit these dependencies and to address the challenges inherent to LOD. This setup enables logical inference to complement SRL where appropriate. For example, results produced by logical inference can serve as gold standard for evaluation of SRL, such as in case of <span class="citation" data-cites="Nickel2012">Nickel et al. (<a href="#ref-Nickel2012">2012</a>)</span>, who used <code>rdfs:subClassOf</code> inferences to evaluate a classification task.</p>
          <!--
          Relational features can be enriched with aggregations.
          
          Relational learning typically involves unirelational graphs.
          We focus on multi-relational data in RDF.
          -->
          <!-- #### Link prediction -->
          <p>We conceived matchmaking via SRL as a link prediction task. Link prediction is <em>“concerned with predicting the existence (or probability of correctness) of (typed) edges in the graph”</em> <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 14)</span>. In the context of knowledge graphs, such as LOD, link prediction is also known as knowledge graph completion <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 14)</span>. An example application of link prediction is discovery of protein interactions in bioinformatics. Typical cases of link prediction operate on multi-relational and noisy data, which makes the task suitable for SRL. In our case, we predict the link between a public contract and its awarded bidder.</p>
          <!-- Out-takes
          
          SRL exploits correlations in the target relation.
          Knowledge graphs encode the existence of facts. [@Nickel2016, p. 25]
          There is an inherent disproportion between existing and potential links in link prediction.
          -->
          </section>
          </section>
          <section id="sec:related-work" class="level2">
          <h2><span class="header-section-number">1.7</span> Related work</h2>
          <p>Before we present our approaches to matchmaking we survey the research related to our work. This section summarizes the background to our research and helps to discern the progress beyond the state of the art in our contributions. This overview of the related work is divided into matchmaking applications, vocabularies for matchmaking, and technologies related to matchmaking.</p>
          <section id="applications" class="level3">
          <h3><span class="header-section-number">1.7.1</span> Applications</h3>
          <!-- Description logics -->
          <p>Early matchmaking dates back to the 1990s. Matchmakers proposed during that era often adopted reasoning with description logics (DL) and communication between software agents. An example of such approach is the work of Kuokka and Harada <span class="citation" data-cites="Kuokka1995">(<a href="#ref-Kuokka1995">1995</a>)</span>, who used Knowledge Query and Manipulation Language (KQML) to describe messages exchanged between agents participating in matchmaking. However, without a common vocabulary the semantics of the messages had to be hardwired in application code.</p>
          <!-- Semantic web -->
          <p>A new wave of matchmaking based on DL arose with the semantic web initiative in the 2000s. These efforts employed then created ontological languages, such as the DARPA Agent Markup Language plus the Ontology Inference Layer (DAML+OIL) <span class="citation" data-cites="GonzalezCastillo2001">(González-Castillo et al. <a href="#ref-GonzalezCastillo2001">2001</a>)</span>, or the Web Ontology Language (OWL) <span class="citation" data-cites="DiNoia2004 DiNoia2007">(Di Noia et al. <a href="#ref-DiNoia2004">2004</a>, <a href="#ref-DiNoia2007">2007</a>)</span>, and approached matchmaking as a task for DL reasoning. Viewed in this way, matchmaking queries can be formulated as classes of matches and matches may thus be tested via subsumption or satisfiability of the class constraints. Such inferences can be produced by standard reasoners, such as RACER <span class="citation" data-cites="Haarslev2001">(Haarslev and Möller <a href="#ref-Haarslev2001">2001</a>)</span>. During this time, typical application domains for matchmaking included web service discovery <span class="citation" data-cites="Trastour2001 Ankolekar2002">(Trastour et al. <a href="#ref-Trastour2001">2011</a>; Ankolekar et al. <a href="#ref-Ankolekar2002">2002</a>)</span> or e-commerce <span class="citation" data-cites="Li2004">(Li and Horrocks <a href="#ref-Li2004">2004</a>)</span>.</p>
          <!-- SPARQL -->
          <p>Using reasoners for matchmaking turned out to be problematic as their performance did not scale well for larger data. In time with the initial release of SPARQL in 2008 <span class="citation" data-cites="Prudhommeaux2008">(Prud’hommeaux and Seaborne <a href="#ref-Prudhommeaux2008">2008</a>)</span> several efforts appeared that approached matchmaking via production rules implemented as database queries in SPARQL. The turn to SPARQL provided matchmaking with better performance and expressivity. An example of this approach was used in the Market Blended Insight project <span class="citation" data-cites="Salvadores2008">(Salvadores et al. <a href="#ref-Salvadores2008">2008</a>)</span>. While this project was concerned mostly with data preparation and feature extraction, basic matchmaking was included as its part, using SPARQL to discover the matches satisfying <code>owl:onProperty</code> constraints. Matchmaking was used as means of micro-segmentation to target specific agents exhibiting the propensity to buy. An RDF version of the Standard Industry Classification 1992 was used to determine similarity of the matched entities. A similar technique that combined SPARQL with RDFS entailment was explored in BauDataWeb <span class="citation" data-cites="Radinger2013">(Radinger et al. <a href="#ref-Radinger2013">2013</a>)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> BauDataWeb applied matchmaking to the European building and construction materials market. Similarity of the matched entities was determined via the FreeClassOWL taxonomy.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
          <!-- 10ders Information Services -->
          <p>Perhaps the first application of matchmaking in public procurement was conceived in the Spanish research project 10ders Information Services.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> Overall, this project aimed to design an interoperable architecture of a pan-European platform for aggregating and mediating public procurement notices in the EU. A part of the project that explored semantic web technologies in public procurement was called Methods on Linked Data for E-procurement Applying Semantics (MOLDEAS) <span class="citation" data-cites="AlvarezRodriguez2012">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2012">2012</a>)</span>. MOLDEAS covered algorithms for enriching data about public procurement notices <span class="citation" data-cites="AlvarezRodriguez2011b">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2011b">2011</a><a href="#ref-AlvarezRodriguez2011b">c</a>)</span>, integration of diverse data sources via linked data <span class="citation" data-cites="AlvarezRodriguez2011a">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2011a">2011</a><a href="#ref-AlvarezRodriguez2011a">a</a>)</span>, and matchmaking via SPARQL enhanced with query expansion <span class="citation" data-cites="AlvarezRodriguez2011c">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2011c">2011</a><a href="#ref-AlvarezRodriguez2011c">b</a>)</span> or spreading activation <span class="citation" data-cites="AlvarezRodriguez2013">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2013">2013</a>, p. 118)</span>. <!-- Unfortunately, it is difficult to compare its results with our approach, because neither implementation details nor evaluation were revealed in the papers describing this work. --> The project emphasized product classification schemes and devoted extensive efforts to converting such classifications to RDF and linking them. Product Types Ontology (PTO),<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> a product ontology derived from Wikipedia, was selected as a linking hub to tie these classifications together.</p>
          <!-- MOLDEAS
          
          Methods on Linked Data for E-procurement Applying Semantics (MOLDEAS): matchmaking via spreading activation [@AlvarezRodriguez2011a], [@AlvarezRodriguez2012], [@AlvarezRodriguez2013]
          Used SPARQL for matchmaking organizations and public procurement notices along with several methods of query expansion [@AlvarezRodriguez2013, p. 118]
          
          MOLDEAS explores a variety of semantic methods for processing public contracts data to offer rich services, such as advanced search.
          Its overall architecture [@AlvarezRodriguez2011a] takes into account diverse sources, such as TED, BOE – the official bulletin of Spanish government, or BOPA – the official bulletin of the local government of the Spanish region of Asturias.
          The data coming from disparate sources are combined using linked data technologies to allow for application of se- mantics methods, such as SPARQL query expansion and query performance optimization [@AlvarezRodriguez2011c] or spreading activation techniques used for information retrieval in graph data [@AlvarezRodriguez2011a]
          Another research thread explored in MOLDEAS is constituted by the conversions of thesauri and product classifications to RDF [@Paredes2008]
          Controlled vocabularies, such as public sector code lists or product classifications (for example, the Common Procurement Vocabulary or EUROVOC), are transformed to RDF, interlinked with external datasets, and used for indexing of public contracts data to enable intelligent services, such as complex subject-oriented queries.
          Public Procurement Notices ontology was developed for the project, however it was not published.
          -->
          <p>10ders Information Services also involved Euroalert.net <span class="citation" data-cites="Marin2013">(Marín et al. <a href="#ref-Marin2013">2013</a>)</span>,<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> a commercial undertaking that alerts small and medium enterprises about relevant public sector information, including current public contracts. Euroalert.net generates alerts by matching the profiles of its subscribers to an incoming stream of published calls for tenders from Tenders Electronic Daily (TED).<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> TED is an EU-wide register of public procurement that aggregates data about public contracts from the EU member states. According to the public description of Euroalert.net, its matchmaking is based on comparison of keywords and code lists and does not exploit semantics or linked open data.</p>
          <!--
          a commercial undertaking that aims to alert small and medium enterprises about relevant public tenders based on subscriber profile model
          enhances the consumed open data to create information products and services with added value, such as customizable widgets.
          Its approach is to use structured open data instead of relying on brittle techniques of screen-scraping.
          Such decision results in a lower amount of data at first, however the service is made to be more reliable and sustainable in this way.
          However, Euroalert.net currently does not expose any linked open data.
          -->
          <p>While SPARQL improves on reasoning-based matchmaking in terms of better expressivity and performance, queries need to be restricted to exact matches for the most part in order to maintain a good runtime. A fundamental feature of matchmaking is ranking matches by the degree to which they satisfy a query. Exact matching, to the contrary, produces only matches and non-matches, without any way to rank the matches. Moreover, exact matches in SPARQL are optimized for structured data, so that performance degrades if SPARQL queries analyse semi-structured or unstructured data, such as literals, which may nevertheless supply valuable data to matchmaking. Concerns such as these led to the development of approaches to matchmaking that involved full-text search or machine learning.</p>
          <p>A forerunner of this research direction was iSPARQL <span class="citation" data-cites="Kiefer2007">(Kiefer et al. <a href="#ref-Kiefer2007">2007</a>)</span>. iSPARQL extends SPARQL with similarity measures implemented using Apache Jena<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> with custom property functions. In this way, it allows to combine graph pattern matching with similarity-based retrieval within a single query. While conceived as a general approach, it was also applied to matchmaking of web services <span class="citation" data-cites="Kiefer2008">(Kiefer and Bernstein <a href="#ref-Kiefer2008">2008</a>)</span>. This application coupled iSPARQL with machine learning in order to improve the detection of approximate matches. This use case demonstrated that the hybrid <em>“combination of logical deduction and statistical induction produces superior performance over logical inference only”</em> <span class="citation" data-cites="Kiefer2008">(Kiefer and Bernstein <a href="#ref-Kiefer2008">2008</a>, p. 473)</span>. Moreover, the similarity-based queries <em>“that exploit textual information of the services turned out to be very effective”</em> <span class="citation" data-cites="Kiefer2008">(Kiefer and Bernstein <a href="#ref-Kiefer2008">2008</a>, p. 475)</span>. Both these findings greatly influenced our approaches to matchmaking. For example, we leverage machine learning in the RESCAL-based matchmaking.</p>
          <p>Another attempt to go beyond SPARQL was the initial matchmaker developed for the PC Filing App <span class="citation" data-cites="Snoha2013">(Snoha et al. <a href="#ref-Snoha2013">2013</a>)</span> in the context of the LOD2 project.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> PC Filing App was a content management system for administering public contracts by contracting authorities. The matchmaker integrated in this application combined SPARQL, which retrieved the matches satisfying the declared hard constraints, with a custom Java implementation of similarity measures between the pre-filtered matches. While its second step enabled the matchmaker to leverage literals more effectively, the footprint of its in-memory implementation based on Java objects led to its poor performance. Our SPARQL-based matchmaker later replaced this matchmaker in the LOD2 project.</p>
          <p>A related research effort that closely matches the objectives pursued by our work is the Web of Needs project <span class="citation" data-cites="Kleedorfer2014">(Kleedorfer et al. <a href="#ref-Kleedorfer2014">2014</a>)</span>. Its <em>“overall goal is to create a decentralized infrastructure that allows people to publish documents on the Web which make it possible to contact each other”</em> <span class="citation" data-cites="Kleedorfer2013">(Kleedorfer and Busch <a href="#ref-Kleedorfer2013">2013</a>)</span>. Web of Needs thus covers the entire distributed infrastructure for marketplaces on the Web with matchmaking being just one of its components. The infrastructure supports three principal tasks: describing supply or demand, identifying trading partners, and conducting a transaction <span class="citation" data-cites="Kleedorfer2014">(Kleedorfer et al. <a href="#ref-Kleedorfer2014">2014</a>)</span>. The proposed process overview involving these tasks, described in detail in <span class="citation" data-cites="Kleedorfer2016">Kleedorfer et al. (<a href="#ref-Kleedorfer2016">2016</a>)</span>, includes online and offline matchmaking. The online matchmaking, which is capable of serving queries in real time, is implemented via full-text search in semi-structured data using Apache Solr.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> The offline matchmaking, which delivers results periodically as it processes queries in batches, is implemented using machine learning via RESCAL <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span>. It thus closely resembles our matchmaker based on the same technology. Detailed evaluation of the offline matchmaker is available in <span class="citation" data-cites="Friedrich2015">Friedrich (<a href="#ref-Friedrich2015">2015</a>)</span>. While our approaches to matchmaking mirror the ones in the Web of Needs to a large extent, their fundamental difference is the application to the public procurement domain. Matchmakers in the Web of Needs are generic, since they are not tuned for any specific use case. Instead, they support common matchmaking scenarios shared in many domains, so they are based on a common denominator of data about demands and offers, including features such as title, description, tags, or price <span class="citation" data-cites="Friedrich2016">(Friedrich et al. <a href="#ref-Friedrich2016">2016</a>)</span>. However, the architecture of the Web of Needs allows extensions to particular vertical marketplaces, such as the public procurement, so that more powerful domain-specific features can be leveraged in matchmaking.</p>
          <p>An alternative tensor-based approach to matchmaking semantic web services is described in <span class="citation" data-cites="Szwabe2015">(Szwabe et al. <a href="#ref-Szwabe2015">2015</a>)</span>. This proposal combines tuple-based probabilistic tensor modeling with covariance-based multilinear filtering. Extensive evaluation shows the presented approach as superior to other matchmaking methods for the evaluated task.</p>
          <!--
          (Actual matchmaking done (some of it in context of public procurement))
          
          - IntelligentMatch
            - <http://www.intelligent-match.de>
            - German national project *IntelligentMatch* (2010 — 2012)^[<http://www.intelligent-match.de>] set as its goal to build up technological infrastructure for intelligent matchmaking.
            - Among the results of this project is *i-text*, a prototype of matchmaking engine targeted at the textile industry.
          - Public procurement in Chile [@AravenaDiaz2016], matches potential bidders, uses natural language processing and latent semantic analysis, early stage
          -->
          </section>
          <section id="vocabularies" class="level3">
          <h3><span class="header-section-number">1.7.2</span> Vocabularies</h3>
          <p>Semantic matchmaking operates on data described by vocabularies and ontologies. Vocabularies enable to bestow data with semantic features that matchmaking can leverage. Support for matchmaking was one of the design goals of the Public Contracts Ontology (PCO), described in sec. <a href="#sec:pco">2.1.1</a>, which we developed to represent public procurement data. Here we present a review of related vocabularies that too can provide support for matchmaking.</p>
          <p>Call for Anything (C4N)<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> is a simple vocabulary for describing demands, such as calls for tenders or calls for papers. C4N can be regarded as one of the first to aim for explicit formulation of demands on the Web. However, the vocabulary features only rudimentary means to express what is sought by demands, as it relies on unstructured literals to specify the objects in demand.</p>
          <!-- Call for Anything
          
          a simple RDF vocabulary by the DERI's Linked Data Research Centre that is intended for expressing demand, such as calls for tenders or calls for papers
          Call for Anything vocabulary can be considered as one of the first attempts towards a greater coverage of the side of demand, for which it provides means of expression.
          However, the vocabulary does not provide guidance on the potentially complex modelling of call conditions and it features only a rudimentary functionality for expressing object of the call, relying on literals to convey the semantics of the demanded object.
          -->
          <p>GoodRelations <span class="citation" data-cites="Hepp2008">(Hepp <a href="#ref-Hepp2008">2008</a>)</span> is an ontology for e-commerce on the Web. It focuses on describing offers, which it views as promises, emphasizing the importance of good and explicitly captured relationships between entities in the e-commerce domain. While the ontology is oriented towards supplies, its cookbook remarks that it is possible to <em>“use the very same GoodRelations vocabulary for the buy and the sell side of commerce.”</em><a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> In order to do that, the ontology proposes a conceptual symmetry between demand and supply. It suggests to model demands as ideal offers (i.e. instances of <code>gr:Offering</code>) satisfying what that entities seek (i.e. link via the <code>gr:seeks</code> property). In this way, GoodRelations can take advantage of its comprehensive vocabulary for offers to describe demands, including specifications of the demanded products and services or the payment conditions.</p>
          <!-- GoodRelations
          
          GoodRelations is an ontology for the domain of e-commerce mainly focused at describing offerings on the Web.
          GoodRelations, developed by Martin Hepp, takes a view of offerings as promises, emphasizing the importance of explicit capture of relationships (as hinted by the ontology’s name) between entities from the e-commerce domain, such as businesses, products, or services.
          It provides a detailed conceptualization of the common resource types encountered within the e-commerce domain, such as business entities or terms and conditions under which products or services are offered, and the complex network of relationships linking these resource types together.
          
          The difference in describing the sought product or service instead of the demanded one lies in linking the involved entity (`gr:BusinessEntity`) with the product or service (`gr:Offering`) via the `gr:seeks` property instead of `gr:offers`, which would be used for attaching offerings.
          The offering (`gr:Offering`) is then described as the "ideal" offering satisfying the demand.
          Leaving this difference aside, the way demands can be expressed follows the same guidelines as expression of offerings does.
          -->
          <!-- LOTED2 -->
          <p>LOTED2 <span class="citation" data-cites="Distinto2016">(Distinto et al. <a href="#ref-Distinto2016">2016</a>)</span> is a legal ontology for public procurement notices. As a legal ontology, it closely follows the EU directives governing public procurement, which we described in the sec. <a href="#sec:legal-context">1.5.1</a>. As such, the ontology enables to describe the tendering process for public contracts in legal terms. It pays a special attention to qualification criteria, which matchmaking may interpret as hard constraints for filtering bidders who are allowed to compete for public contracts. As the name indicates, LOTED2 evolved from Linked Open Tenders Electronic Daily (LOTED) <span class="citation" data-cites="Valle2010">(Valle et al. <a href="#ref-Valle2010">2010</a>)</span>, an effort to convert TED to RDF using a simple vocabulary that mirrored the structure of the source data. The account on LOTED2 <span class="citation" data-cites="Distinto2016">(Distinto et al. <a href="#ref-Distinto2016">2016</a>, p. 21)</span> proposes matchmaking as future work and suggests matching TED to OpenCorporates,<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a> an open database of companies, using reasoning and matching classifications.</p>
          <!-- LOTED -->
          <!--
          Tenders Electronic Daily (TED)^[<http://ted.europa.eu>]
          
          primarily for data analytics applications (does not mention matchmaking)
          delved into triplification of tenders in European Union coming from the Tenders Electronic Daily (TED) portal.
          The project addressed conversion to RDF and interlinking of data from RSS feeds providing a timely access to the public contracts data from the EU member countries that are aggregated by TED.
          By lifting the data by converting it to RDF and enriching it with well-defined structure and semantics, LOTED was able to apply novel data analysis methods in this field and showcase interactive visualizations.
          The outcomes of the work dedicated to LOTED are available for reuse, since it is an open source project with its source code available online.
          
          Source code: <https://sourceforge.net/projects/loted>
          
          LOTED Ontology^[<http://loted.eu/ontology>]
          closely matches the schema of the source data from TED
          does not reuse other RDF vocabularies
          
          LOTED data is linked to geographical entities in DBpedia and Geonames
          -->
          <p>Public Procurement Ontology (PPROC) <span class="citation" data-cites="MunozSoro2016">(Muñoz-Soro et al. <a href="#ref-MunozSoro2016">2016</a>)</span> is an ontology that covers the complete life-cycle of public contracts, ranging from their issue to termination. As such, it supports both publication of public contracts as open data and management of public procurement processes in a transparent and accountable way. Its stated underlying goal is to enable open access to procurement data to the public <span class="citation" data-cites="MunozSoro2015">(Muñoz-Soro and Esteban <a href="#ref-MunozSoro2015">2015</a>)</span>. Although the publications about the ontology are agnostic of its intended use in applications, the ontology was already used in practice for integration of public procurement data from Spanish administrative bodies. It was adopted for public contracts of several authorities from the autonomous community of Aragón.</p>
          </section>
          <section id="related-technologies" class="level3">
          <h3><span class="header-section-number">1.7.3</span> Related technologies</h3>
          <p>We conclude this section with a brief overview of related technologies. To the best of our knowledge, these technologies have not yet found use in matchmaking, although they were adopted for related tasks, such as in recommender systems.</p>
          <p>LOD-enabled recommender systems <span class="citation" data-cites="DiNoia2012a DiNoia2012b DiNoia2016 Thalhammer2012">(Di Noia et al. <a href="#ref-DiNoia2012a">2012</a><a href="#ref-DiNoia2012a">b</a>, <a href="#ref-DiNoia2012b">2012</a><a href="#ref-DiNoia2012b">a</a>, <a href="#ref-DiNoia2016">2016</a>; Thalhammer <a href="#ref-Thalhammer2012">2012</a>)</span> constitute a source from which many technologies applicable to semantic matchmaking can be drawn. These systems typically employ established techniques for producing recommendations, such as matrix factorization <span class="citation" data-cites="Koren2009">(Koren et al. <a href="#ref-Koren2009">2009</a>)</span>, but enhance them with semantic features extracted from LOD. Since the graph data model of LOD is conducive to the use of graph algorithms, some of the LOD-enabled recommender systems found uses for such algorithms or proposed novel algorithms operating on graph data. Examples of this sort include personalized PageRank <span class="citation" data-cites="Nguyen2015">(Nguyen et al. <a href="#ref-Nguyen2015">2015</a>)</span>, spreading activation <span class="citation" data-cites="Heitmann2014 Heitmann2016">(Heitmann and Hayes <a href="#ref-Heitmann2014">2014</a>, <a href="#ref-Heitmann2016">2016</a>)</span>, or WeightedNIPaths <span class="citation" data-cites="Ristoski2015">(Ristoski et al. <a href="#ref-Ristoski2015">2015</a>)</span>.</p>
          <p>Matchmaking can also derive inspiration from technologies in two broader research areas. Instance matching <span class="citation" data-cites="Christen2012 Bryl2014">(Christen <a href="#ref-Christen2012">2012</a>; Bryl et al. <a href="#ref-Bryl2014">2014</a>)</span> is usually limited to discovering equivalence relationships, although its similarity measures and combination functions to aggregate similarity scores are also applicable to discovering matches between demands and supplies. Semantic search <span class="citation" data-cites="Davies2009">(Davies et al. <a href="#ref-Davies2009">2009</a>)</span> can be considered a research area to which semantic matchmaking belongs. Matchmaking can borrow many techniques from this parent, such as query expansion or retrieval from semi-structured data. A notable example of a semantic search engine for RDF is SIREn <span class="citation" data-cites="Delbru2012">(Delbru et al. <a href="#ref-Delbru2012">2012</a>)</span>, which extends Apache Lucene<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> with capabilities to search deeply nested data without a fixed schema.</p>
          </section>
          </section>
          </section>
          <section id="sec:data-preparation" class="level1">
          <h1><span class="header-section-number">2</span> Data preparation</h1>
          <p>A fundamental part of the hereby presented work is preparation of the Czech public procurement dataset enriched with linked data. The prepared dataset was used to evaluate the matchmakers we built as our main contribution. It served as a use case for applied research in the public procurement domain to explore whether the proposed matchmakers provide useful recommendations in a real-world setting.</p>
          <!-- Characteristics of ETL -->
          <p>In this chapter we will describe the data preparation using the framework of Extract-Transform-Load (ETL) <span class="citation" data-cites="Kimball2004">(Kimball and Caserta <a href="#ref-Kimball2004">2004</a>)</span>. ETL is a workflow for data preparation that is guided by the principle of the separation of concerns, as indicated by its compound name. It conceptualizes a sequence of data processing steps endowed with a single main responsibility. Each step is further subdivided into smaller steps endowed with a single responsibility. The self-describing nature of RDF can further contribute to cleaner separation of concerns in the ETL workflow, so that the coupling between the steps involved is reduced.</p>
          <p>The structure of this chapter roughly follows the steps of ETL. We extend them to modelling, extraction, transformation, linking, fusion, and loading. Modelling produces a target schema, onto which the data is mapped in the course of extraction and transformation. In our setting, extraction refers to the process of converting non-RDF data to RDF. Once data is available in RDF, its processing is described as transformation. Linking discovers co-referent identities, while fusion resolves them to the preferred identities, along with resolving conflicts in data that may arise. Linking and fusion are interleaved and executed iteratively, each building on the results of its previous step. Loading is concerned with making the data available in a way that the matchmaking methods can operate efficiently. The adopted ETL workflow evolved from the workflow that was previously described by this dissertation’s author <span class="citation" data-cites="Mynarz2014c">(<a href="#ref-Mynarz2014c">2014</a><a href="#ref-Mynarz2014c">a</a>)</span>. The fig. <a href="#fig:etl">2</a> summarizes the overall workflow.</p>
          <figure>
          <img src="resources/img/etl_workflow.png" alt="Figure 2: ETL workflow" id="fig:etl" /><figcaption>Figure 2: ETL workflow</figcaption>
          </figure>
          <p>We employed materialized data integration. Unlike virtual integration, materialized integration persists the integrated data. This allowed us to achieve the query performance required by data transformations and SPARQL-based matchmaking. Our approach to ETL can be regarded as Extract-Load-Transform (ELT). We first loaded the extracted data into an RDF store to make transformation, linking, and fusion via SPARQL Update operations feasible. Using RDF allows to load data first and integrate it later, while in the traditional context of relational databases, data integration typically precedes loading. We used a batch ETL approach, since our source data is published in subsets partitioned per year. Real-time ETL would be feasible if the source data was provided at a finer granularity, such as in the case of the profiles of contracting authorities, which publish XML feeds informing about current public contracts.</p>
          <!-- ## Benefits of linked data for data preparation -->
          <p>Using RDF provides several advantages to data preparation. Since there is no fixed schema in RDF, any RDF data can be merged and stored along with any other RDF data. Merge as union applies to schemas as well, because they too are formalized in RDF. Flexible data model of RDF and the expressive power of RDF vocabularies and ontologies enable to handle variation in the processed data sources. Vocabularies and ontologies make RDF into a self-describing data format. Producing RDF as the output of data extraction provides leverage for the subsequent parts of the ETL process, since the RDF structure allows to express complex operations transforming the data. Moreover, the homogeneous structure of RDF <em>“obsoletes the structural heterogeneity problem and makes integration from multiple data sources possible even if their schemas differ or are unknown”</em> <span class="citation" data-cites="Mihindukulasooriya2013">(Mihindukulasooriya et al. <a href="#ref-Mihindukulasooriya2013">2013</a>)</span>. Explicit, machine-readable description of RDF data enables to automate many data processing tasks. In the context of data preparation, this feature of RDF reduces the need for manual intervention in the data preparation process, which decreases its cost and increases its consistency by avoiding human-introduced errors. However, <em>“providing a coherent and integrated view of data from <a href="#sec:linked-data">linked data</a> resources retains classical challenges for data integration (e.g., identification and resolution of format inconsistencies in value representation, handling of inconsistent structural representations for related concepts, entity resolution)”</em> <span class="citation" data-cites="Paton2012">(Paton et al. <a href="#ref-Paton2012">2012</a>)</span>.</p>
          <p>Linked data provides a way to practice pay-as-you-go data integration <span class="citation" data-cites="Paton2012">(Paton et al. <a href="#ref-Paton2012">2012</a>)</span>. The pay-as-you-go principle suggests to reduce costs invested up-front into data preparation, recognize opportunities for incremental refinement of the prepared data, and revise which opportunities to invest in based on user feedback <span class="citation" data-cites="Paton2016">(Paton et al. <a href="#ref-Paton2016">2016</a>)</span>. The required investment in data preparation is inversely proportional to the willingness of users to tolerate imperfections in data. In our case, we used the feedback from evaluation of matchmaking as an indirect indication of the parts of data preparation that need to be improved.</p>
          <p>The principal goal of ETL is to add value to data. A key way to do so is to improve data quality. Since data quality is typically defined as fitness for use, we focus on the fitness of the prepared data for matchmaking in particular. Fitness for this use is affected by several data quality dimensions <span class="citation" data-cites="Batini2006">(Batini and Scannapieco <a href="#ref-Batini2006">2006</a>)</span>. The key relevant dimensions are duplication and completeness. Lack of duplicate entities reduces the search space that matchmaking has to explore. On the contrary, duplicates break links that can be leveraged by matchmaking. For instance, if there are unknown aliases for a bidder, then data linked from these aliases is unreachable. Incompleteness causes the features potentially valuable for matchmaking to be missing. It makes data less descriptive and increases its sparseness, in turn making matchmaking less effective. However, measuring both these dimensions is difficult. In case of duplication, we can only measure the relative improvement of the deduplicated dataset when compared to the input dataset. Measuring the duplication of the output dataset is unfeasible, since it may contain unknown entity aliases. Similarly, the evaluation of completeness requires either a reference dataset to compare to or reliable cardinalities to expect in the target dataset’s schema. Unfortunately, reference datasets are typically unavailable. Cardinalities of properties either cannot be relied upon or their computation is undermined by unknown duplicates.</p>
          <p>While the goals pursued by public disclosure and aggregation of procurement data are often undermined by insufficient data integration caused by heterogeneity of data provided by diverse contracting authorities, ETL can remedy some of the adverse effects of the heterogeneity and fragmentation of public procurement data. However, at many stages of data preparation we needed to compromise data quality due to the effort required to achieve it. We are explicit about the involved trade-offs, because it helps to understand the complexity of the data preparation endeavour. Moreover, for some issues of the data its source does not provide enough to be able to resolve them at all.</p>
          <!-- Impact on data analyses -->
          <p>Low data quality can undermine both matchmaking as well as data analyses. Data analyses are often based on aggregation queries, which can be significantly skewed by incomplete or duplicate data. Incompleteness of data introduces an involuntary influence of the sampling bias to analyses of such data. For instance, aggregated counts of duplicated entities are unreliable, as they count distinct identifiers instead of counting distinct real-world entities, which may be associated with multiple identifiers. Uncertain quality disqualifies the data from being used scenarios where publishing false positives is not an option. For example, probabilistic hypotheses are of no use for serious journalism, which cannot afford to make possibly untrue claims. Instead, such findings need to be considered as hinting where further exploration to produce more reliable outcomes could be done. On the contrary, in the probabilistic setting of matchmaking even imperfect data can be useful. Moreover, we assume that the impact of errors in data can be partially remedied by the volume of data. Finally, since we follow the pay-as-you-go approach, there is an opportunity to invest more in improving data quality if required.</p>
          <!-- Prepared datasets -->
          <p>Preparation of the dataset for matchmaking involved several sources. Selection of each of the data sources had a motive justifying the effort spent preparing the data. We selected the Czech public procurement register as our primary dataset, to which we linked the Common Procurement Vocabulary (CPV), Czech address data, Access to Registers of Economic Subjects/Entities (ARES), and zIndex. The Czech public procurement register provides historical data on Czech public contracts since 2006. CPV organizes the objects of public contracts in a hierarchical structure that allows to draw inferences about the similarity of the objects from their distance in the structure of the vocabulary. Czech address data offers geo-coordinates for the reference postal addresses in the Czech Republic. By matching postal addresses to their canonical forms from this dataset, postal addresses can be geocoded. ARES serves as a reference dataset for business entities. We used it to reconcile the identities of business entities in the Czech public procurement data. zIndex provides a fairness score to contracting authorities in the Czech public procurement. ETL of each of these datasets is described in more detail in the following sections.</p>
          <!-- Data and source code -->
          <p>The Czech public procurement dataset is available at <a href="https://linked.opendata.cz/dataset/isvz" class="uri">https://linked.opendata.cz/dataset/isvz</a>. The source code used for data preparation is openly available in a code repository.<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> This allows others to replicate and scrutinize the way we prepared data. The data preparation tasks were implemented via declarative programming using XSLT, SPARQL Update operations, and XML specifications of linkage rules. The high-level nature of declarative programming made the implementation concise and helped us to avoid bugs by abstracting from lower-level data manipulation. The work on data preparation started already in 2011, which may explain the diverse choices of the employed tools. Throughout the data preparation, as more suitable and mature tools appeared, we adopted them. A reference for the involved software is provided in the appendix <a href="#sec:software">6</a>.</p>
          <section id="sec:modelling" class="level2">
          <h2><span class="header-section-number">2.1</span> Modelling</h2>
          <p>The central dataset that we used in matchmaking is the Czech public procurement register.<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> <!--
          There are no unawarded contracts in the data dumps.
          This makes is unsuitable for the development of the matchmaking service that alerts about open calls for tenders.
          Data from electronic marketplaces also contains open calls for tenders (marked with `<VZstav>PH010003 - Zadávací řízení</VZstav>`).
          --> The available data on each contract in this dataset differs, although generally the contracts feature data such as their contracting authority, the contract’s object, award criteria, and the awarded bidder, altogether comprising the primary data for matchmaking demand and supply. As we described previously, viewed from the market perspective, public contracts can be considered as expressions of demand, while awarded tenders express the supply.</p>
          <p>We described this dataset with a semantic data model. One key goal of modelling this data was to establish a structure that can be leveraged by matchmaking. However, modelling data in RDF is typically agnostic of its expected use. Instead, it is guided by a conceptual model that opens the data to a wide array of ways to reuse the data. Nevertheless, the way we chose to model our data reflected our priorities.</p>
          <p>We focused on facilitating querying and data integration via the data model. Instead of enabling to draw logical inferences by reasoning with ontological constructs, we wanted to simplify and speed-up querying. In order to do that, for example, we avoided verbose structures to reduce the size of the queried data. For the sake of better integration with other data, we established IRIs as persistent identifiers and reused common identifiers where possible. Thanks to the schema-less nature of RDF, shared identifiers allowed us to merge datasets automatically.</p>
          <p>The extracted public procurement data was described using a mixture of RDF vocabularies, out of which the Public Contracts Ontology was the most prominent.</p>
          <section id="sec:pco" class="level3">
          <h3><span class="header-section-number">2.1.1</span> Public Contracts Ontology</h3>
          <p>Public Contracts Ontology<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> (PCO) is a lightweight RDF vocabulary for describing data about public contracts. The vocabulary has been developed by the Czech OpenData.cz initiative since 2011, while this dissertation’s author has been one of its editors. Its design is driven by what public procurement data is available, mostly in the Czech Republic and at the EU level. The data-driven approach <em>“implies that vocabularies should not use conceptualizations that do not match well to common database schemas in their target domains”</em> <span class="citation" data-cites="Mynarz2014a">(Mynarz <a href="#ref-Mynarz2014a">2014</a><a href="#ref-Mynarz2014a">b</a>)</span>.</p>
          <p>PCO establishes a reusable conceptual vocabulary to provide a consistent way of describing public contracts. This aim for reusability corresponds with the established principle of minimal ontological commitment <span class="citation" data-cites="Gruber1993">(Gruber <a href="#ref-Gruber1993">1993</a>)</span>. The vocabulary exhibits a simple snowflake structure oriented around contract as the central concept. It extensively reuses and links other vocabularies, such as Dublin Core Terms<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> or GoodRelations.<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> While direct reuse of linked data vocabularies is discouraged by Presutti et al. <span class="citation" data-cites="Presutti2016">(<a href="#ref-Presutti2016">2016</a>)</span>, because it introduces a dependency on external vocabulary maintainers and the consequences of the ontological constraints of the reused terms are rarely considered, we argue that these vocabularies are often maintained by organizations more stable than the organization of the vocabulary’s creator and that the mentioned ontological constraints are typically non-existent in lightweight linked data vocabularies, such as Dublin Core Terms. Several properties in PCO have their range restricted to values enumerated in code lists. For example, there is a code list for procedure types, including open or restricted procedures. These core code lists are represented using the Simple Knowledge Organization System (SKOS) <span class="citation" data-cites="Miles2009">(Miles and Bechhofer <a href="#ref-Miles2009">2009</a>)</span> and are a part of the vocabulary. The design of PCO is described in more detail in Klímek et al. <span class="citation" data-cites="Klimek2012">(<a href="#ref-Klimek2012">2012</a>)</span> and Nečaský et al. <span class="citation" data-cites="Necasky2014">(<a href="#ref-Necasky2014">2014</a>)</span>. The class diagram in fig. <a href="#fig:pco">3</a> shows the Public Contracts Ontology.</p>
          <figure>
          <img src="resources/img/pco.png" alt="Figure 3: Public Contracts Ontology" id="fig:pco" /><figcaption>Figure 3: Public Contracts Ontology</figcaption>
          </figure>
          <!-- TODO: Include criticism by LOTED2? [@Distinto2016] -->
          <p>The vocabulary was used to a large extent in the LOD2 project.<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> For example, it was applied to Czech, British, EU, or Polish public procurement data. In this way, we validated the portability of the vocabulary across various legal environments and ways of publishing public procurement data.</p>
          <!-- 
          # Out-takes:
          
          *Anti-SEO* coined by Jiří Skuhrovec: <http://blog.aktualne.cz/blogy/jiri-skuhrovec.php?itemid=13827>
          
          Reasoning with data was its explicit non-goal.
          As a result, the vocabulary does not feature sufficient ontological constructs to allow reasoning.
          *"The prime aim of vocabulary is communication instead of representation"* [@Mynarz2014a].
          While PCO strives for conceptual parsimony, this feature is missing in some source data models that needlessly replicate isomorphic data structures.
          For example, the Czech public procurement register numbers XML elements for award criteria (as `Kriterium1`, `Kriterium2` etc.) instead of using one element connected with a relation of higher arity.
          -->
          </section>
          <section id="sec:concrete-data-model" class="level3">
          <h3><span class="header-section-number">2.1.2</span> Concrete data model</h3>
          <p>The concrete data model of the Czech public procurement data uses the PCO mixed with terms cherry-picked from other linked open vocabularies, such as Public Procurement Ontology (PPROC) <span class="citation" data-cites="MunozSoro2016">(Muñoz-Soro et al. <a href="#ref-MunozSoro2016">2016</a>)</span>, which directly builds upon PCO. The data model’s class diagram is shown in fig. <a href="#fig:vvz">4</a>.</p>
          <figure>
          <img src="resources/img/vvz.png" alt="Figure 4: Class diagram of the Czech public procurement data" id="fig:vvz" /><figcaption>Figure 4: Class diagram of the Czech public procurement data</figcaption>
          </figure>
          <p>The data model of the extracted data departs from PCO in several ways. There are ad hoc terms in the <code>&lt;http://linked.opendata.cz/ontology/isvz.cz/&gt;</code> namespace to represent dataset-specific features of the Czech public procurement register. Some of these terms are intermediate and are subsequently replaced during data transformation.</p>
          <p>Contract objects expressed via the properties <code>pc:mainObject</code> and <code>pc:additionalObject</code> are qualified instead of linking CPV directly. A proxy concept that links a CPV concept via <code>skos:closeMatch</code> is created for each contract object to allow qualification by concepts from the CPV’s supplementary vocabulary. The proxy concepts link their qualifier via <code>skos:related</code>. For example, a contract may have <em>Electrical machinery, apparatus, equipment and consumables; lighting</em> (code <code>31600000</code>) assigned as the main object, which can be qualified by the supplementary concept <em>For the energy industry</em> (code <code>KA16</code>). This custom modelling pattern was adopted, since SKOS does not recommend any way to represent pre-coordination of concepts.<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a></p>
          <p>Data in the Czech public procurement register is represented using notices, such as prior information notices or contract award notices. Notices are documents that inform about changes in the life-cycle of public contracts. Using the terminology of Jacobs and Walsh <span class="citation" data-cites="Jacobs2004">(<a href="#ref-Jacobs2004">2004</a>)</span>, notices can be considered information resources describing contracts as non-information resources. Information resource is <em>“a resource which has the property that all of its essential characteristics can be conveyed in a message”</em> <span class="citation" data-cites="Jacobs2004">(Jacobs and Walsh <a href="#ref-Jacobs2004">2004</a>)</span>, so that it can be transferred via HTTP. On the contrary, non-information resources, such as physical objects or abstract notions, cannot be transferred via HTTP.</p>
          <!--
          TODO: Add a (simplified) diagram of public contract life-cycle in terms of public notices.
          -->
          <p>We represent contract notices as instances of the subclasses of <code>pproc:Notice</code> from PPROC, since PCO does not include the concept of a contract notice. PCO treats notices as mere artefacts of the document-based communication in public procurement. Each notice pertains to a single contract, while a contract may link several notices informing about its life-cycle. Notices thus provide a way to represent the temporal dimension of contracts. They serve as time-indexed snapshots tracking the evolution of contracts, based on the notice type and its timestamp. To a large extent, we treat notices as intermediate resources, the data of which are combined to form a unified view of contracts during data fusion. Nevertheless, in focusing on the central concept of the public contract, this modelling approach glances over the temporal dimension of other data. For example, it does not accommodate expressing that a contracting authority was renamed. Neither is it supported by PCO, which was designed as atemporal, since modelling temporal data remains an open research topic.</p>
          <p>Apart from reusing the code lists incorporated in PCO, we employed a few others. We extracted the code list standardizing categories of procured services as defined in the EU directive 2004/18/EC <span class="citation" data-cites="EU2004">(EU <a href="#ref-EU2004">2004</a>)</span>. This code list links CPV to the Central Product Classification (CPC).<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a> We also extracted several code lists enumerating the types of contract notices. The EU-wide standard types of notices, including the prior information notice or the design contest notice, were published in 2004 and updated in 2014, with a few types removed, such as the public works concession, or added, such as the modification notice.<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a> All these code lists were represented in RDF using SKOS.</p>
          <p>The diagram of the concrete data model in the fig. <a href="#fig:vvz">4</a> shows the Czech public procurement register data after the steps described in this chapter were applied. As is apparent from the cardinalities of many properties, the dataset’s quality is hardly optimal. Maximum cardinalities of several properties are higher than expected due to several reasons. Some entities were inadvertently merged due to their unreliable identifiers. For example, there are few public contracts that share the placeholder identifier <code>1</code>. We adopted several heuristic counter-measures to avoid fusing distinct entities, such as in case of the previous example, but we could not ensure the reliability of all identifiers in general. Another cause of the high cardinalities is the incomplete data fusion due to insufficient information needed to decide on which values to drop and which ones to keep. Once the hints for data fusion, such as the semantics or temporal order of contract notices, had been used up, there was no more guidance for preferring particular values. When this happened, we either resorted to random sampling or left the data as it was. Ultimately, further improvements in data quality can be made in line with the pay-as-you-go approach if the invested effort is offset by the gains obtained in matchmaking.</p>
          <!--
          The temporal nature of the public procurement domain is important because much of the value of this data is transient and decreases as the data ages.
          Due to the transient nature of public procurement data, the data that starts as a business opportunity ends up as a historic record.
          -->
          </section>
          </section>
          <section id="extraction" class="level2">
          <h2><span class="header-section-number">2.2</span> Extraction</h2>
          <p>Data from the Czech public procurement register was not initially available as structured data, so that the interested parties had to scrape the data from HTML. The dataset was eventually released as open data.<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> The data is published in exports to XML, CSV, and Microsoft Excel, each partitioned by year. However, the dataset exports contain only the past contracts that were already awarded, so they cannot be used for alerting bidders about the relevant opportunities in public procurement. Nevertheless, this historical data can be used for training and evaluation. Although published in structured formats, the data is structured poorly, so we had to spend substantial effort improving its structure. The portal publishing this open data also includes exports from the electronic marketplaces where some public contracts are published, such as purchases of commodities. Nonetheless, we did not use this dataset, since it follows a different schema than the Czech public procurement register, so that using it would require us to spend further effort on data preparation. Unfortunately, since data preparation is not a routine task, reliable estimates of the required effort are difficult to come by, so we avoid making them.</p>
          <p>We chose the XML version as the source for data extraction. XML allows us to leverage mature tooling, such as XSLT processors, for the extraction. The choice of the input data format also enabled us to explore the data by using the tools designed for manipulating XML.</p>
          <p>Ad hoc exploratory queries were done using XQuery. We ran queries to discover possible values of a given XML element or to verify assumptions about the data. Finding distinct values of XML elements helped us detect fixed enumerations, which can be turned into code lists. Queries verifying our assumptions about the data allowed us to tell if an error in data is present in its source or if it is made during data transformation. For example, we assumed that the awarded bidder’s registered identification number (RN) is always different from the contracting authority’s RN. This assumption turned out to be false, caused by errors in the source data.</p>
          <p>More systematic analysis of the dataset’s structure was implemented using an XSL transformation. For the purposes of development of the XSL stylesheet we implemented a transformation that computes the cardinalities of elements in the data. This allowed us to tell the always-present elements that can be used as keys identifying the entities described in the data. The tree of element cardinalities revealed the <em>empirical schema</em> of the data. When we looked at this schema, we saw that it follows a fixed structure. In fact, it exhibited a reductive use of XML. Cardinalities of all XML elements were strictly one-to-one, at the expense of empty elements for missing values. Instead of repeating an element in case of multiple values, each value was encoded in a different element named with a numerical index (e.g., <code>&lt;element_1&gt;</code>, <code>&lt;element_2&gt;</code> etc.). For instance, this pattern is used for award criteria, which are represented using the elements <code>&lt;Kriterium1&gt;</code>, <code>&lt;Kriterium2&gt;</code>, etc. Due to the fixed cardinalities, there were many empty elements of this type where less than the maximum number of values was present. To reduce the size of the processed data and simplify further processing we first applied an XSL transformation to remove the empty elements from the data. Doing so simplified the subsequent transformations, since they did not have to cater for the option of empty elements.</p>
          <p>We developed an XSL stylesheet to extract the source XML data to RDF/XML <span class="citation" data-cites="Gandon2014">(Gandon and Schreiber <a href="#ref-Gandon2014">2014</a>)</span>. The stylesheet maps the schema of the source data onto the target schema described in the sec. <a href="#sec:concrete-data-model">2.1.2</a>. During the extraction we validated the syntax of registered identification numbers, CPV codes, and literals typed with <code>xsd:date</code>. If possible, we established links in the extracted data by concatenating unambiguous identifiers to namespace IRIs. However, the majority of linking was offloaded to a dedicated phase in the ETL process, covered in the sec. <a href="#sec:linking">2.4</a>, since it typically required queries over the complete dataset. A trade-off we had to make due to our choice of an RDF store was to use plain literals in place of literals typed with <code>xsd:duration</code>, since Virtuoso<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> does not yet support this data type. We used LinkedPipes-ETL (LP-ETL) <span class="citation" data-cites="Klimek2016">(Klímek et al. <a href="#ref-Klimek2016">2016</a>)</span> to automate the extraction. LP-ETL provided us with a way to automate downloading and transforming the source data in a data processing pipeline. The syntax of the extracted output was validated via Apache Jena’s <em>riot</em><a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> to avoid common problems in RDF/XML, such as incorrect striping <span class="citation" data-cites="Brickley2002">(Brickley <a href="#ref-Brickley2002">2002</a>)</span>.</p>
          <p>The selected dataset spans Czech public contracts from June 1, 2006 to January 18, 2017. This selection amounts to 1.6 GB of raw data in XML and corresponds to 20.5 million extracted RDF triples. The dataset contains 186 965 public contracts.</p>
          <p>To aid the visual validation of the extracted data, we developed <em>sparql-to-graphviz</em><a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a> that produces a class diagram representing the empirical schema of the data it is provided with. It generates a description of the dataset’s class diagram in the DOT language, which can be rendered to images via Graphviz,<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a> an established visualization software for graph structures. The dataset’s summary in the diagram, shown in the fig. <a href="#fig:vvz">4</a>, contains the classes instantiated in the dataset, along with their datatype properties and object properties interconnecting the classes. Each property is provided with its most common range, such as <code>xsd:date</code> for a datatype property or <code>schema:Organization</code> for an object property, and its minimum and maximum cardinality. As mentioned before, the cardinality ranges may signalize errors in the data transformation, such as insufficient data fusion when the maximum cardinality surpasses an expected value.</p>
          <!--
          - Data validation is typically mentioned as an intrinsic part of extraction. However, it is also found in the transformation step.
          - We currently do "validation through use". Syntactical validation is performed when loading the data into an RDF store. However, the data breaks many assumptions of the Public Contracts Ontology to allow to automated validation using tools such as RDFUnit.
          -->
          </section>
          <section id="sec:transformation" class="level2">
          <h2><span class="header-section-number">2.3</span> Transformation</h2>
          <p>Since we practiced the principle of separation of concerns, the data extraction produced only intermediate data. This data needed to be transformed in order to reach a better quality and conform with our target data model.</p>
          <p>Even though the current documentation of the Czech public procurement register states that the collected data is validated by several rules, we found errors in the data that should have been prevented by the rules. A possible explanation for this issue is that the extracted dataset contains historical data as well, some of which might date to the past when the register did not yet employ as comprehensive validation as it does now. Alternatively, the <em>“errors in the published data may be caused by either negligence when filling out Journal forms or by deliberate obfuscation of sensitive information in order to reduce a contract’s traceability”</em> <span class="citation" data-cites="Soudek2016a">(Soudek <a href="#ref-Soudek2016a">2016</a><a href="#ref-Soudek2016a">a</a>)</span>. A large part of data transformation was therefore devoted to denoising. We dealt both with natural noise, such as the involuntarily introduced typos in literals, and likely malicious noise, such as deliberate omissions to obfuscate the data. Many other data quality problems of the Czech public procurement register are documented on the wiki of zIndex <span class="citation" data-cites="Soudek2016a">(Soudek <a href="#ref-Soudek2016a">2016</a><a href="#ref-Soudek2016a">a</a>)</span>. Similar problems in public procurement data were witnessed by Futia et al. <span class="citation" data-cites="Futia2017">(<a href="#ref-Futia2017">2017</a>)</span> in case of Italian procurement.</p>
          <p>Due to the messiness of the data, we had to make the data transformations defensive. The transformations needed to rely on fewer assumptions about the data and had to be able to deal with violations of these assumptions. For example, the identifiers of the entities involved in public procurement had to be treated as inherently unreliable.</p>
          <p>Since not all data is disclosed, we must assume that we have only a sample instead of the complete data. Moreover, given the incentives not to publish data, we cannot assume the sample is random. There may be systemic biases, such as particular kinds of contracting authorities not reporting public contracts properly. Therefore, in general, the findings from the sample cannot be extrapolated to generally valid findings without considering the biases.</p>
          <section id="challenges" class="level3">
          <h3><span class="header-section-number">2.3.1</span> Challenges</h3>
          <p>The key challenges of the data transformation were dealing with high heterogeneity of the data and achieving a workable performance of complex transformations affecting large subsets of data. Due to the volume of processed data and the complexity of the applied transformations, we have not used LP-ETL to orchestrate the transformations. LP-ETL materializes the output of each processing step and, in case of RDF, loads data into an in-memory RDF store, which leads to performance problems when working with higher volumes of data. Nonetheless, LP-ETL allows to execute SPARQL Update operations on data partitioned into chunks of smaller size, which can significantly speed up processing of larger datasets. However, this technique can be used only for transformations that require solely the data present in the chunk, which prevents it from being used in cases the whole dataset is needed by a transformation; e.g., for joins across many resources. An example where this technique is applicable is sequential processing of tabular data, in which data from each row can be processed separately in most cases. Because of its relational nature, our dataset cannot be effectively split to allow executing many kinds of transformations on smaller chunks of data.</p>
          <p>Instead of partitioning data, we partitioned the intermediate query bindings in SPARQL Update operations. Transformations using this technique follow the same structure. They contain a sub-query that selects the unprocessed bindings; either by requiring the bindings to match a pattern that is present only in the unprocessed data, e.g., using <code>FILTER NOT EXISTS</code> to eliminate bindings that feature data added by the transformation, or by selecting subsequent subsets from the sorted bindings. For instance, a transformation of instances of <code>schema:PostalAddress</code> can be divided into transformations of non-overlapping subsets of these instances. The latter option for filtering the unprocessed bindings cannot be used when the set of sorted bindings is modified during the transformation. For example, when a transformation deletes some bindings, the offsets of subsets in the ordered set cease to be valid. Additionally, since sorting a large set is a computationally expensive operation, this option may require the sub-query projecting the ordered bindings to be wrapped in another sub-query to be able to cache the sorted set, such as with the Virtuoso’s scrollable cursors.<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a> The selected unprocessed bindings from the sub-query are split into subsets by setting a limit. The outer update operation then works on this subset and transforms it.</p>
          <p>We developed <em>sparql-unlimited</em><a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a> that allows to run SPARQL update operations following the described structure using Virtuoso. This tool executes transformations rendered from Mustache<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a> templates that feature placeholders for <code>LIMIT</code>, and optionally <code>OFFSET</code>. Limit determines the size of a subset to be transformed in one update operation. In this way, the processed subset’s size can be adjusted based on the complexity of the transformation. Updates are executed repeatedly, the offset being incremented by the limit in each iteration, until their response reports zero modifications. This stopping condition is Virtuoso-specific, since the SPARQL 1.1 Update standard <span class="citation" data-cites="Gearon2013">(Gearon et al. <a href="#ref-Gearon2013">2013</a>)</span> leaves it unspecified, so that SPARQL engines differ in how they indicate zero modifications. Additionally, <em>sparql-unlimited</em> provides a few conveniences, including configurable retries of failed updates or the ability to restart transformations from a specified offset.</p>
          <p>While <em>sparql-unlimited</em> was used to automate parts of individual transformations, each transformation was launched manually. Virtuoso, the RDF store in which we executed the transformations, has an unpredictable runtime, which may be due to unresolved previous transactions or generally faulty implementation. Therefore, we started each transformation manually to allow to fine-tune the configuration for each run depending on the received response from Virtuoso.</p>
          <p>A good practice in ETL is to make checkpoints continuously during data processing. Checkpoints consist of persisting the intermediate data output from the individual processing steps, usually to disk. However, due to the large numbers of transformations in our case large disk space would be required if checkpoints were done for every transformation. To reduce disk consumption we persisted only the outputs of the major sub-parts of the data processing pipeline.</p>
          </section>
          <section id="transformations" class="level3">
          <h3><span class="header-section-number">2.3.2</span> Transformations</h3>
          <p>Overall, we developed tens of SPARQL Update operations for the data transformation. One of the principles we followed was to reduce data early in order to avoid needless processing in the subsequent transformation steps. For example, we deleted empty contract lots and the resources orphaned<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a> in other transformations. Several transformations were used to clean malformed literals; for example to regularize the common abbreviations for organizations types or convert <code>\/</code> into <code>V</code>. We removed award dates from the future. We added default values into the data. Since the dataset is of Czech origin, we used Czech koruna (CZK) as the default value in case currency was missing. The addresses without an explicitly stated country were assumed to be located in the Czech Republic. Nonetheless, it is important to acknowledge that adding default values was a trade-off favouring coverage over accuracy.</p>
          <p>We paid particular attention to structuring postal addresses in order to improve the results of the subsequent geocoding, described in the sec. <a href="#sec:geocoding">2.4.5</a>. The primary aim of the transformation of postal addresses was to minimize their variety to increase their chance for match with the reference postal addresses. We managed to extract postal codes, house numbers, and street names from otherwise unstructured data. Accidental variations in postal addresses, such as punctuation, were normalized where possible. Unfortunately, this effort was hindered by a Virtuoso’s bug in support for non-ASCII characters,<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a> which prevented us from using SPARQL Update operations with diacritical characters, for example when expanding Czech abbreviations in street names.</p>
          <p>We made several transformations to move data of select properties, which was difficult to achieve in XSLT during the data extraction. Since RDF/XML lacks a way to express inverse properties, we minted provisional properties in the data extraction, which were reversed as part of the data transformation. For example, a temporary property <code>:isLotOf</code> linking lots to contracts was reversed to <code>pc:lot</code> from PCO. We also corrected domains of some properties, because moving them in XSLT would require joins based on extra key indices.</p>
          <p>Some data transformations required additional data. A subset of these transformations leveraged background knowledge from vocabularies. For example, we used <code>rdfs:subClassOf</code> axioms from PPROC to distinguish subclasses of <code>pproc:Notice</code>, when we merged data from notices to contracts. We loaded the required vocabularies into separate named graphs via the SPARQL Update <code>LOAD</code> operation.</p>
          <p>In order to make prices comparable, we converted non-CZK currencies to CZK via exchange rates data from the European Central Bank (ECB).<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a> This dataset contains daily exchange rates of several currencies to EUR. We used an RDF version of the dataset<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a> prepared for the OpenBudgets.eu<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a> project. This derivative covers the rates from November 30, 1999 to April 7, 2016, so it allowed to convert most prices in our dataset. Prices in non-CZK currencies were converted using the exchange rates valid at their notice’s publication date. This was done as a two-step process, first converting the prices to EUR followed by the conversion to CZK. In order to automate the execution of this task we employed <em>sparql-to-csv</em>,<a href="#fn42" class="footnoteRef" id="fnref42"><sup>42</sup></a> a tool that we developed, which allows to pipe query results into another query or update operation.</p>
          <p>The normalized prices were winsorized<a href="#fn43" class="footnoteRef" id="fnref43"><sup>43</sup></a> at 99.5<sup>th</sup> percentile to remove the likely incorrect extreme prices. Due to the limited expressivity of SPARQL this task needed to be split into two SPARQL queries followed by a SPARQL Update operation. The first query retrieved the count of 0.5 % prices, the second query chose the minimum price in the highest 0.5 % prices using the count as a limit, and the final update capped the 0.5 % of highest prices at this minimum. As in the case of currency conversion, we automated the steps of this task using piped queries in <em>sparql-to-csv</em>.</p>
          <p>Finally, some estimated prices are expressed as ranges from minimum to maximum price. These prices were converted to arithmetic averages to simplify further processing.</p>
          <!--
          Out-takes:
          
          In the context of procurement and financial data it was reported that *"data conversion aspects of the integration project are estimated to take up to 50 % of the project team's time"* [@BestPractices2005, p. 19].
          We argue that a considerable share of this effort can be avoided if the integrated datasets are available in RDF. 
          In that case, data translation can skip the resolution of syntactical inconsistencies and instead focus on resolving semantic mismatches between the integrated sources.
          
          * Due to the messiness of the data it is unfit for logical reasoning, e.g., applying an OWL reasoner.
          
          * Order of transformations is determined by the dependencies of RDF resources.
            * At the moment, this is done manually. 
          
          Findings of data analyses:
          
          * There can be lots with no tenders if they are part of contracts that were successfully awarded.
          * Most findings are either caused by errors in source data or examples of corruption already covered by the media in the past.
          
          Mention linking EU projects?
          -->
          </section>
          </section>
          <section id="sec:linking" class="level2">
          <h2><span class="header-section-number">2.4</span> Linking</h2>
          <p>Linking is a process of discovering co-referent identifiers. Co-referent identifiers share the same referent, i.e. they refer to the same entity. The existence of co-referent identifiers is possible because linked data operates under the non-unique name assumption (non-UNA). This assumption allows to publish distributed data without the coordination required for agreeing on names. However, queries and data analyses usually operate under the unique name assumption (UNA), and therefore they require a unified dataset without aliases for entities. Consequently, the aim of linking is to discover explicit links between the non-unique names of entities, so that these entities can be unified in data fusion. In this way, linking addresses the accidental variety of data published on the Web.</p>
          <section id="content-based-addressing" class="level3">
          <h3><span class="header-section-number">2.4.1</span> Content-based addressing</h3>
          <p>In the absence of agreed-upon identifiers, entities are referred to by their description. Moreover, unlike RDF, some data formats, such as CSV, do not have a mechanism for linking. The lack of shared identifiers established by a reliable authority leads to proliferation of aliases for equivalent entities. Missing consensual identifiers are one of the key challenges in integration of public procurement data <span class="citation" data-cites="AlvarezRodriguez2014">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2014">2014</a>)</span>.</p>
          <p>If the descriptions with which entities are referred to are reliable and complete, we can use content-based addressing to discover which descriptions refer to the same entity. Content-based addressing is a general approach for identifying entities by using the content of their representations. In case of RDF entities, we assume that triples containing an entity’s identifier as subject or object to make up the entity’s description, also known as the concise bounded description <span class="citation" data-cites="Stickler2005">(Stickler <a href="#ref-Stickler2005">2005</a>)</span>. We typically restrict such triples to those in which an entity’s identifier is in the subject role. Various content signatures may be derived from such descriptions of entities.</p>
          <p>Simple keys of entities can be derived from values of specific properties. In case of subjects, their keys can be objects of outbound properties that may be interpreted as inverse functional properties; i.e. instances of <code>owl:InverseFunctionalProperty</code>. For example, the property <code>foaf:homepage</code>, which describes an entity’s home page, is defined as an inverse functional property and as such it is usable as a simple key of an entity. In case of objects, their keys can be subjects of inbound properties that may be interpreted as functional object properties; i.e. instances of both <code>owl:FunctionalProperty</code> and <code>owl:ObjectProperty</code>. For example, the property <code>pc:contractingAuthority</code>, which links a contract to its contracting authority, is defined as a functional object property, so that its subject can function as a simple key of the contracting authority. Both kinds of simple key properties may be chained in property paths and followed to obtain keys that do not directly describe the entities they identify. For example, the property path <code>pc:awardedTender/pc:bidder</code> can be treated as a functional object property, the subject of which may be used as a bidder’s key if we accept the assumption that a contract can be awarded to a single organization only. Simple keys are typically used directly as part of entity IRIs, which prevents creating multiple aliases for the entities in the first place. A caution must be given if schema axioms related to functional and inverse functional properties are unreliable or if instance data is diverging from them. In such case, it is better to skip inferring equivalence links via the described methods in order to avoid false results.</p>
          <p>Compound keys are more complex content signatures that can be derived from combinations of values of specific properties. In order to be eligible as keys, these combinations must be unique. For example, a contract and a lot number can serve as a compound key for a contract lot. Similarly to simple keys, such compound keys are commonly used as parts of IRIs of the entities they identify. We also employed this approach to merge the bidders sharing the same name and awarded with the same contract. Nevertheless, compound keys are perhaps used more often in a probabilistic setting, in which the degree of their match implies a probability of equivalence of the keyed entities. Fuzzy matches of combinations of values can approximate exact matches of simple keys. However, unlike identifying simple keys, identification of suitable compound keys and approaches for their matching usually requires an expert insight into the domain in question. A common scenario for probabilistic matching of compound keys uses combinations of simple keys that are unreliable identifiers on their own. In the context of our dataset, even when registered identification numbers (RNs) are available, they may be misleading as identifiers. For example, there are several public contracts each year that a contracting authority awards to itself according to the supplied RNs of the authority and the awarded bidder. Moreover, many RNs in the data are syntactically invalid and cannot be automatically coerced to the correct syntax. So, for example, an organization’s name may be combined with its syntactically invalid RN to produce an approximate compound key.</p>
          <p>Further extending the size of keys, we can use the complete descriptions of the keyed entities. Since such keys may be unwieldy, they can be substituted by their hashes to make them more manageable. Using hashes as keys is standard in content-based addressing. Hash functions, such as MD5, map variable length descriptions to a fixed length, while preserving their uniqueness. Unlike the previously described approaches for deriving keys, hashes do not require background knowledge to select the key properties, so their production can be fully automated. However, on the one hand, hash keys tend to be more brittle, since any change in the hashed descriptions produces a different hash, which may lead to many false negatives when comparing hashes. On the other hand, hashes can also produce false positives if they are used for underspecified entities. For example, postal addresses for which we know only that they are located in the Czech Republic are unlikely to be the same. The risk of false positives can be reduced by requiring a minimum description, similar to a compound key, to be present. For instance, we can hash only the postal addresses that feature at least a street address and an address locality.</p>
          <p>We also experimented with linking entities by discovering entities that are described with a subset of another entity’s description. Given some minimum description of entities to avoid false positives, we assumed that if a set of property-object pairs of a subject is a subset of such set of another subject, the subjects are co-referent. However, detecting subsets in SPARQL is problematic because defining subsets requires universal quantification. Since SPARQL is based on existential quantification instead, universally qualified predicates need to be reimplemented as double negation via nested <code>FILTER NOT EXISTS</code> clauses. Ultimately, we abandoned this linking method because of its poor performance, which makes it unusable for larger data.</p>
          <p>In case of entities for which no key can be used to construct IRIs directly during data extraction via XSLT, we employed blank nodes as identifiers. Subsequently, we converted these blank nodes to hash-based IRIs via SPARQL Update operations. The hashes were computed by concatenating the properties and objects of the identified subject and deriving an SHA1 hash from the concatenated string. We used this approach primarily for entities that can be interpreted as structured values, such as price specifications. The entities identified by blank nodes were processed in their inverse topological order. If a blank node linked another blank node, the linked blank node was rewritten first. This was done to ensure that the hashed descriptions of blank nodes do not contain blank nodes, which would cause different hashes to be computed from otherwise equivalent descriptions. Since no two blank nodes are the same, this procedure led to a significant reduction of aliases. <!-- TODO: Should we quantify this reduction? --></p>
          </section>
          <section id="technologies" class="level3">
          <h3><span class="header-section-number">2.4.2</span> Technologies</h3>
          <p>We employed four kinds of linking technologies. Simple keys and some compound keys were used directly to construct IRIs in XSLT. Linking based on hashes was done using SPARQL Update operations <span class="citation" data-cites="Gearon2013">(Gearon et al. <a href="#ref-Gearon2013">2013</a>)</span>. Update operations were also used when creating links required a join via a key, for example when reconciling code list values. Most linking tasks based on fuzzy matches of compound keys were done using the Silk link discovery framework <span class="citation" data-cites="Bryl2014">(Bryl et al. <a href="#ref-Bryl2014">2014</a>)</span>. Silk was used when links could not be established via exact matches. For example, we used it to compare syntactically invalid RNs via string distance metrics. We used Silk Workbench, a graphical user interface for Silk, for iterative development of the linkage rules. Silk Workbench displays the results of linking in a way the interlinked entities can be compared manually. This enables to examine a sample of links for false positives and negatives and adjust the linkage rules accordingly, tuning weights and thresholds to avoid the undesired results. An example linkage rule in Silk Workbench is shown in the fig. <a href="#fig:silk">5</a>. Elasticsearch<a href="#fn44" class="footnoteRef" id="fnref44"><sup>44</sup></a> was employed for matching postal addresses to reference addresses from the Czech addresses dataset.</p>
          <figure>
          <img src="resources/img/silk_workbench_linkage_rule.png" alt="Figure 5: Example linkage rule in Silk Workbench" id="fig:silk" /><figcaption>Figure 5: Example linkage rule in Silk Workbench</figcaption>
          </figure>
          <p>In general, linking was done iteratively, interposed with data fusion. Fusion reduced the size of the data, in turn reducing the search space for linking. Additionally, linking that followed fusion could build on the previously created links.</p>
          </section>
          <section id="tasks" class="level3">
          <h3><span class="header-section-number">2.4.3</span> Tasks</h3>
          <p>We worked on three main linking tasks. We reconciled the values in our dataset with standard code lists. Code lists provide common reference concepts with which values from our source data can be linked. For instance, we mapped different wordings of procedure types to the PCO’s code list for the procedures recognized by the Czech public procurement law.</p>
          <p>We linked organizations in the Czech public procurement register to ARES. Instead of deduplicating organizations directly in the public procurement dataset, we decided to reconcile them with ARES, which provided reference identities for organizations. We developed Silk linkage rules using combinations of several properties as compound keys. Syntactically invalid RNs were matched with valid RNs using the Levenshtein string distance metric to find the RNs containing typos. Normalized legal names of organizations were compared via the Jaro-Winkler distance metric with a high required similarity threshold. This metric was selected because it penalizes mismatches near the start of the name more than mismatches at the end. It also takes the lengths of the compared names into account, so that more mismatches are tolerated in longer names. Thanks to these features this distance metric is widely used when comparing names. Legal names were first normalized by converting to lowercase and removing both non-alphanumeric characters and stop-words (e.g., “Czech”), which were generated from the most frequent words appearing in the legal names. Exact matches via postal codes or normalized URLs of organizations were used to disambiguate homonymous organization names. Unfortunately, URLs were discovered to be unreliable as simple keys, because they can be assigned incorrectly, so that we used them as keys only in combination with other data. Geo-coordinates of organizations obtained by geocoding postal addresses were used to filter matches by maximum allowed geographic distance. The resulting equivalence links generated by Silk were serialized using the <code>owl:sameAs</code> property, loaded in a separate named graph, and resolved during data fusion. In total, we generated 6842 links for the 14177 business entities unlinked to reference entities from ARES. Resolution of these links thus reduced the share of the unlinked business entities from 33.38 % to 15.9 %.</p>
          <p>We geocoded postal addresses in ARES and in the Czech public procurement register by linking them to the Czech addresses dataset. Geocoding is the described in greater detail further in a separate section.</p>
          </section>
          <section id="evaluation" class="level3">
          <h3><span class="header-section-number">2.4.4</span> Evaluation</h3>
          <p>Evaluation of the quality of linking typically involves a clerical review of a sample of the resulting equivalence links <span class="citation" data-cites="Christen2012">(Christen <a href="#ref-Christen2012">2012</a>, p. 174)</span>. By using manual assessment, a randomly selected sample of links can be split into correct and incorrect matches. This allows to compute quality metrics, such as precision, which is defined as the ratio of correct links (true positives) to all links (positives). Results of the metrics computed on a sample may be then extrapolated to approximate the quality of the complete output of linking.</p>
          <p>We manually evaluated a randomly selected sample of 200 links to ARES generated by approximate matching in Silk. To a limited extent, the evaluation of this subset of links can substitute the evaluation of all links, which was unfeasible due to the manual effort involved in assessing link validity. Validity of each link was confirmed or rejected based on the data published in the PR, also taking into account its changes over time, or based on the web sites of the linked organizations. 9 links were determined to be false positives, while the rest was confirmed to be valid. This ratio of false positives produces the precision of 0.955. We consider such precision to be reasonable, given the low quality of the linked data. Some of the false positives were caused by ambiguous descriptions of business entities. For example, there are two distinct entities named <em>COMIMPEX spol. s r.o.</em> that also share the same organization type.</p>
          <p>Apart from the clerical review, there are also few automated measures that may indicate the quality of links. An example of such measure is the reduction ratio, defined as the number of generated equivalence links compared to all possible equivalence links. Effectiveness of linking measured in its total runtime compared to the number of the processed entities can also be determined without human input. A more detailed review of the evaluation methods for linking is presented by Christen <span class="citation" data-cites="Christen2012">(<a href="#ref-Christen2012">2012</a>, pp. 163–184)</span>.</p>
          <!--
          Findings:
          * A lot of missing addresses in ARES.
          * Some links are between the same IRIs even though this is prohibited in the source dataset. (Unsure why this happens.) This skews the link counts.
          * An example of two distinct business entities with the same name: <https://or.justice.cz/ias/ui/rejstrik-firma.vysledky?subjektId=312038&typ=UPLNY> and <https://or.justice.cz/ias/ui/rejstrik-firma.vysledky?subjektId=722550&typ=UPLNY>.
          
          ## Manual evaluation
          
          Sample size: 200
          False positives: 9
          Precision: 0.955
          -->
          <!--
          Out-takes:
          
          Linking can exploit both semantics (i.e. schema axioms) and statistics of data [@Hogan2012, p. 78].
          
          Appropriately enough, this process is also referred to by multiple terms, including instance matching, deduplication, or record linkage.
          
          Defragmentation of data
          
          Due to the transient nature of public procurement data it is necessary to integrate it in a timely manner, before the data loses relevance [@Harth2013].
          
          While the title suggests the blog post is about data fusion, it is more about linking.
          <http://blog.mynarz.net/2016/10/basic-fusion-of-rdf-data-in-sparql.html>
          Perhaps the confusion arises from fusion and linking being merged when dealing with blank nodes.
          
          # New linking, 2017-03-31
          
          Count of unlinked organizations: 2079
          Count of unlinked organizations with invalid registrations: 615
          Count of unlinked bidders: 1924
          Count of unlinked contracting authorities: 172 (some contracting authorities are bidders too)
          -->
          </section>
          <section id="sec:geocoding" class="level3">
          <h3><span class="header-section-number">2.4.5</span> Geocoding</h3>
          <p>Geocoding is the process of linking postal addresses to geographic locations. The locations are represented as coordinates corresponding to a place on the Earth’s surface. Geocoding can be considered a case of instance matching <span class="citation" data-cites="Christen2012">(Christen <a href="#ref-Christen2012">2012</a>, sec. 9.1)</span> that matches addresses from a dataset to reference addresses equipped with geo-coordinates. We geocoded the postal addresses of business entities in the Czech public procurement register, the Public Register (PR), and the Trade Licensing Register (TLR). In case of PR we geocoded only the addresses that were missing links to the Czech addresses dataset. Unlinked addresses in PR amounted for 12.42 % of all its addresses. No addresses in TLR were linked. In total, we geocoded over 180 thousand postal addresses from these registers. In case of the Czech public procurement register we geocoded the addresses of business entities that were not linked to the above-mentioned registers. The overall goal of this effort was to be able to locate the business entities for the purposes of linking, analyses, and matchmaking.</p>
          <p>The main challenge to address in geocoding was the lack of structure in the geocoded data. <!--
          // We don't do geocoding of non-organization addresses that contain only `schema:description`, hence commented out.
          87.22 % postal addresses in the Czech public procurement register have only unstructured `schema:description`.
          --> As described in the sec. <a href="#sec:transformation">2.3</a>, we attempted to parse the unstructured addresses to recover their structure. Nevertheless, many addresses contained just a name of a region or a municipality. This is why we started with simple geocoding based on matching region or municipality names.</p>
          <p>We used LP-ETL to extract the names of regions and municipalities along with their corresponding geo-coordinates from the RÚIAN SPARQL endpoint.<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a> The data provides geo-coordinates of centroids of each region and municipality. The geo-coordinates were reprojected from EPSG:5514 coordinate reference system (CRS) to EPSG:4326 to improve their interoperability, since the latter one is a de facto standard CRS on the Web. We loaded the data into our RDF store and ran a SPARQL Update operation to match the geo-coordinates to postal addresses via the names of regions and municipalities.</p>
          <p>In order to geocode other postal addresses, we built an Elasticsearch-based geocoder using the Czech addresses data, covered in the sec. <a href="#sec:czech-addresses">2.4.6.3</a>. We decided not to use an existing solution for several reasons. Some existing services for geocoding have restrictive licenses. For instance, the results of the Google Maps Geocoding API can be used only in conjunction with displaying the obtained geo-coordinates on a map from Google Maps.<a href="#fn46" class="footnoteRef" id="fnref46"><sup>46</sup></a> More liberal geocoding services often provide poor accuracy. For example, this is the case of OpenStreetMap’s Nominatim,<a href="#fn47" class="footnoteRef" id="fnref47"><sup>47</sup></a> both for its structured and unstructured search. Finally, we wanted to assess whether open data can help build a geocoder on par with the commercial offerings. This is why we based the developed geocoder on the gazetteer built from the Czech address data.</p>
          <p>During the development of the geocoder we leveraged the tooling we built for data preparation, described in the appendix <a href="#sec:software">6</a>. <em>sparql-to-jsonld</em><a href="#fn48" class="footnoteRef" id="fnref48"><sup>48</sup></a> was used to retrieve the Czech addresses data from a SPARQL endpoint, construct descriptions of the individual postal addresses, and frame them into JSON-LD documents. We used <em>jsonld-to-elasticsearch</em><a href="#fn49" class="footnoteRef" id="fnref49"><sup>49</sup></a> to index the addresses in Elasticsearch. In the index phase we applied a basic normalization and employed a synonym filter to expand the abbreviations commonly found in postal addresses.</p>
          <p>The geocoder <em>elasticsearch-geocoding</em><a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a> was implemented as a command-line tool that loads the addresses to geocode from a SPARQL endpoint using a paged SPARQL SELECT query provided by the user, and queries an Elasticsearch index with the Czech addresses data for each address. We adopted Elasticsearch for the geocoder because, unlike SPARQL, it allows to perform fuzzy searches, in which results are ranked by the degree to which they fulfil the search query. This is useful since the geocoded addresses may be incomplete, poorly structured, or contain misspellings. In case we obtained multiple results from the geocoder, we selected the first one, which ranked the best.</p>
          <p>Since we practice separation of concerns, the geocoder expects a reasonably clean input. It is the responsibility of data preparation to structure and normalize the geocoded postal addresses. This effort has benefits for many tasks, not geocoding only. Instead of ad hoc cleaning during geocoding we thus prepared the postal addresses as part of the ETL pre-processing, as described in sec. <a href="#sec:transformation">2.3</a>.</p>
          <p>The geocoder generates the queries to Elasticsearch from its input addresses. Since every property of the addresses is optional, the queries can be generated in several ways, depending on the semantics associated with the properties. If <code>schema:description</code> is the only available property, we search for it across all fields. A more complex query matching combinations of subqueries is generated if more properties are present. The objects of postal code (<code>schema:postalCode</code>) and address locality (<code>schema:addressLocality</code>) are treated as co-referent, so that it suffices if one matches if both are available.</p>
          <p>The design of the geocoding queries was guided by the level of accuracy required to support the envisioned use cases. For us errors in the range of tens to hundreds of meters are tolerable, so that we could trade in accuracy for increased recall. We made adjustments to the geocoding queries to better serve this objective. As house numbers and orientational numbers are often mixed up, we enabled the geocoding queries to match either number. We boosted the weight of postal codes because the match on their level is more important than the match on more specific levels, such as the house number. Prior to introducing the boost for postal codes, in some cases distant addresses sharing the same street address and house number were mixed in their geolocation. Moreover, unlike address localities, postal codes are usually regular, which makes them more reliable in retrieval. Further optimization of the geocoding queries was guided by the results of evaluation.</p>
          <section id="evaluation-1" class="level4">
          <h4><span class="header-section-number">2.4.5.1</span> Evaluation</h4>
          <p>We chose to evaluate the geocoder by using metrics adapted from <span class="citation" data-cites="Goldberg2013">Goldberg et al. (<a href="#ref-Goldberg2013">2013</a>)</span>. <em>Match rate</em> is defined as the share of addresses capable of being geocoded. If <span class="math inline">\(A\)</span> is a set of addresses and <span class="math inline">\(geocode()\)</span> is a geocoding function, we can define the match rate <span class="math inline">\(mr\)</span> as follows:</p>
          <p><span class="math display">\[mr = \frac{|\{a \in A, geocode(a) \neq \varnothing\}|}{|A|}\]</span></p>
          <p>We adapted the <em>spatial accuracy</em> metric as the share of addresses that are geocoded within a specified distance from the reference location. We chose to evaluate spatial accuracy at 50 meters, so that geo-coordinates found within 50 meters from the reference location are considered matching. Provided a set of addresses <span class="math inline">\(A\)</span> and ground truth <span class="math inline">\(G\)</span> containing the true geo-coordinates, we can define this metric <span class="math inline">\(sa\)</span> in the following way:</p>
          <p><span class="math display">\[sa = \frac{|\{a \in A, distance(geocode(a), G_a)&lt; 50\}|}{|A|}\]</span></p>
          <p>While match rate can be computed without a gold standard dataset, spatial accuracy needs one. Thanks to the links to the Czech addresses dataset from the Public Register, we had a dataset that could be used as a gold standard. Nevertheless, the provenance and quality of these links is undocumented, with a possibility of outdated or invalid links due to mismatched versions of the linked datasets. Therefore, we decided to verify them by comparing them to another datasets. We experimented with several geocoding services, including Google Maps Geocoding API,<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a> MapQuest Geocoding API,<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a> and Here Geocoding API,<a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a>, to assess their accuracy. Here Geocoding API turned out to deliver the best results while also providing a liberal licence allowing to use its geo-coordinates in our evaluation. When geocoding with this API, we used structured queries with a bounding box set to the Czech Republic to rule out the evident non-matches.</p>
          <p>We loaded 10 thousand randomly selected postal addresses from the PR that linked the Czech addresses dataset. Out of this sample, 73 % of the geo-coordinates provided by the Here Geocoding API were found no farther than 1 meter from the source geo-coordinates. In this way, we purified two “silver” standard datasets into a gold one, consisting of 7300 postal addresses with verified geo-coordinates. The match rate achieved by our geocoder on this dataset was 0.9893. The geocoder scored 0.9556 for the spatial accuracy at 50 meters, with median distance of 0.425 meters and mean average distance of 272 meters. <!-- Spatial accuracy at 1 meter = 0.9507 --></p>
          <p>We also evaluated our geocoder using a sample of 5 thousand addresses from the TLR, for which true location was unknown. The geocoder achieved a match rate of 0.9788, while Here Geocoding API scored 0.6278 on this sample. We sorted the postal addresses that were matched both by Here Geocoding API and our geocoder by the distance of the returned geo-coordinates in descending order. We manually checked the top geo-coordinates and found that the maximum distance where our geo-coordinates were invalid was 8 kilometers. The obtained median distance was 0.63 meters and the arithmetic mean distance was 290 meters. We deemed such results to be reasonable for our use case.</p>
          <!--
          We geocoded 49 635 postal addresses in the Czech public procurement register.
          -->
          <!--
          Match rate for a sample of 5000 postal addresses from the Trade Licensing Register:
          Here Geocoding API: 0.6278 
          - Our geocoder: 0.9788
          - Overlap: 0.6278
          -->
          </section>
          </section>
          <section id="linked-datasets" class="level3">
          <h3><span class="header-section-number">2.4.6</span> Linked datasets</h3>
          <p>We linked several datasets with the Czech public procurement register. In this section we describe how these datasets were prepared. The fig. <a href="#fig:cloud">6</a> shows how these datasets are linked. In this diagram, dataset size corresponds to the number of RDF triples in the dataset, while the thickness of lines between the datasets corresponds to the number of links connecting them. Both proportions are logarithmically scaled for display purposes.</p>
          <figure>
          <img src="resources/img/public_procurement_cloud.png" alt="Figure 6: Czech public procurement linked data cloud" id="fig:cloud" style="width:60.0%" /><figcaption>Figure 6: Czech public procurement linked data cloud</figcaption>
          </figure>
          <p>In the following we review these linked datasets and explain how they were obtained and prepared.</p>
          <section id="sec:cpv" class="level4">
          <h4><span class="header-section-number">2.4.6.1</span> Common Procurement Vocabulary</h4>
          <p>Common Procurement Vocabulary (CPV)<a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a> is a controlled vocabulary standardized by the EU for harmonizing the description of procured objects across the EU member states. Within the EU, CPV has been mandatory to use for public procurement since 2006. <!-- FIXME: Add citation? --> The most recent version of CPV is from 2008. Each CPV concept is provided with labels in 23 languages of the EU. The multilingual nature of CPV allows to localize public procurement data to support cross-country procurement. CPV consists of the main and the supplementary vocabulary. The main vocabulary provides primary concepts to describe public contracts, such as <code>90521400</code> that stands for <em>“Transport of radioactive waste”</em>. There are 9454 concepts in the main vocabulary, structured in 6 levels of hierarchy. The supplementary vocabulary can be used to qualify concepts from the main vocabulary. An example supplementary concept is <code>MF09</code>, meaning <em>“Using hovercraft”</em>. There are 903 concepts in the supplementary vocabulary, organized in a flat list. However, the supplementary vocabulary is rarely used. Only 3.25 % of objects in the Czech public procurement dataset are qualified with a supplementary concept. CPV has a monohierarchical structure in which the individual taxonomic links typically have the flavour of either subsumption<a href="#fn55" class="footnoteRef" id="fnref55"><sup>55</sup></a> or part-whole<a href="#fn56" class="footnoteRef" id="fnref56"><sup>56</sup></a> relations between the vocabulary’s concepts. The hierarchical structure allows to derive a correspondence between the concept’s location in the structure and its conceptual similarity to its neighbouring concepts, which makes it possible to perform basic reasoning and query expansion.</p>
          <p>The monohierarchical design may have caused conceptual duplication within distinct branches of the vocabulary. For instance, there are two concepts labelled as <em>“Transport equipment and auxiliary products to transportation”</em>. One is <code>34000000</code>, which constitutes its own branch in the vocabulary, and the other is <code>33952000</code>, which is nested in the branch <em>“Medical equipments, pharmaceuticals and personal care products”</em>. Apart from the sharing same label, there is no explicit link between these concepts. Moreover, CPV is published in tree (XML) or tabular (XLS) data formats, which may have encouraged the vocabulary’s monohierarchical design by making it simpler to implement. Polyhierarchy may solve the duplication by allowing concepts to have multiple parents. However, polyhierarchies are graphs, so RDF is more suitable to represent them.</p>
          <p>We may sidestep polyhierachy by creating associative links between similar concepts within different branches of the vocabulary’s hierarchical structure. In this way, graph distance within CPV can better approximate the semantic distance of the compared concepts and allow similarity-based retrieval. In order to achieve this goal, we experimented with link discovery tools to construct associative links within the vocabulary. In the absence of better features to anchor the sense of the concepts, we compared the concepts’ multilingual labels to determine their similarity. Even with the modest size of the vocabulary this exercise turned to be computationally expensive, since it would require over a trillion of pair-wise comparisons due to the number of languages involved. This naïve approach could be improved by using techniques, such as blocking <span class="citation" data-cites="Isele2011">(Isele et al. <a href="#ref-Isele2011">2011</a>)</span>, however, given its tenuous benefits, we decided to abandon this effort.</p>
          <p>In order to integrate CPV with the public procurement data, we converted it from XML to RDF. The transformation<a href="#fn57" class="footnoteRef" id="fnref57"><sup>57</sup></a> was done using an XSL transformation and SPARQL CONSTRUCT queries for enriching data. Its result is described using SKOS plus Dublin Core Terms<a href="#fn58" class="footnoteRef" id="fnref58"><sup>58</sup></a> for metadata. While the original CPV source expresses hierarchical relations implicitly using the structure of numerical notations of the vocabulary’s concepts, its RDF version makes these relations explicit using hierarchical relations from SKOS, such as <code>skos:broaderTransitive</code>. The transformation was originally orchestrated by a shell script, which was later replaced by a UnifiedViews<a href="#fn59" class="footnoteRef" id="fnref59"><sup>59</sup></a> <span class="citation" data-cites="Knap2017">(Knap et al. <a href="#ref-Knap2017">2017</a>)</span> pipeline. UnifiedViews is an ETL tool for producing RDF data, which can be considered a predecessor of LP-ETL.</p>
          <p>The Czech public procurement register mandates the use of the 2008 version of CPV since September 15, 2008. Since the data we processed goes back to 2006, we had to account for public contracts described with the previous version of CPV from 2003. In order to harmonize the description of the older contracts we used the correspondence table mapping CPV 2003 to CPV 2008 published by the EU Publications Office.<a href="#fn60" class="footnoteRef" id="fnref60"><sup>60</sup></a> We developed an LP-ETL pipeline to convert the correspondence table from Excel to CSV and map it to RDF using SKOS mapping relations, such as <code>skos:closeMatch</code>. The following part of the transformation turned out to be problematic. Cells that would duplicate the values of the cells above them were left empty in the source spreadsheet. Therefore, we had to create a “fill down blanks” functionality to duplicate cell values in following directly adjacent empty cells. The SPARQL Update operation implementing this functionality came off as taxing, notwithstanding the modest size of the processed data. LP-ETL had to be abandoned as it could not run the operation to completion. Instead, we adopted Apache Jena’s <em>arq</em><a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a> that was able to execute it. Provided the RDF version of the mappings from the correspondence table, concepts from CPV 2003 were resolved to their CPV 2008 counterparts by using a SPARQL Update operation that exploited the mappings.</p>
          <!-- 
          Out-takes:
          
          A fundamental problem that breaks the promise of controlled vocabularies is that of inter-indexer and intra-indexer consistency.
          A shared vocabulary does not help if the way it is used is not shared too.
          This is a to certain degree an unsolvable problem.
          To a limited extent it can be alleviated by strict rules or shared practices.
          Probably the most common inconsistency is assigning less specific concepts.
          
          A partial solution to inter-indexer consistency (assuming intra-indexer consistency): consider CPV as local tags. I.e. they are comparable only with respect to the contracting authority that assigned them. (This approach also ignores that several civil servants are likely assigning CPV for most contracting authorities, hence the intra-indexer consistency may be broken.)
          
          RDF version of CPV is also available at <http://cpv.data.ac.uk>.
          There is also the version from MOLDEAS.
          
          Discuss transitivity of hierarchical relations with `skos:broaderTransitive` vs. `skos:broader`?
          -->
          </section>
          <section id="sec:ares" class="level4">
          <h4><span class="header-section-number">2.4.6.2</span> Access to Registers of Economic Subjects/Entities</h4>
          <p>Access to Registers of Economic Subjects/Entities<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a> (ARES) is an information system about business entities. It is maintained by the Ministry of Finance of the Czech Republic. The data in this system describes business entities along with their registrations required to pursue their business. It contains legal entity names, registration dates, postal addresses, and classifications according to NACE. Thanks to these features ARES can serve as a reference dataset for the Czech business entities.</p>
          <p>This system is not the primary source of the data it provides. Instead, it mediates data from several source registers and links back to them where possible. The main sources of ARES are the Public Register<a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a> (PR) run by the Czech Ministry of Justice, the Trade Licensing Register<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a> (TLR) operated by the Czech Ministry of Industry and Trade, and the Business Register<a href="#fn65" class="footnoteRef" id="fnref65"><sup>65</sup></a> (BR) maintained by the Czech Statistical Office (CSO). Consequently, the data ARES provides may not be up-to-date or complete. In fact, ARES explicitly renounces any guarantees about the data. Its data is not to be treated as legally binding, instead, it serves only an informative purpose.</p>
          <p>The benefit of ARES that outweighs its drawbacks is that, unlike its source registers, it provides data in a structured format. It exposes an HTTP API<a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a> that allows to retrieve data in XML about one legal entity per request. The access to data is rate-limited to prevent high load from automated harvesters that may cause unavailability of the service for human users. The limits allow to issue a thousand requests per day and five thousand requests per night. Since ARES provides access to hundreds of thousands of business entities and no option for bulk download, harvesting a copy of its data may take many weeks. The rate-limiting and the prolonged execution thus need to be factored into account when designing an ETL pipeline that obtains the data.</p>
          <p>Since ARES wraps many registers, we narrowed our focus to two registers most relevant to the public procurement: PR and TLR. These registers are those that the awarded bidders of public contracts are registered in. We used only a subset of BR that links bidders to concepts from the NACE classification. A large share of business entities is present in both PR and TLR. It is nevertheless useful to obtain data from both registers, since they are complementary. For instance, while the PR contains a classification of organization activity, TLR naturally provides the trade licences entities have registered.</p>
          <p>Valid requests to the ARES API must contain a Registered Identification Number (RN) of a business entity. This design makes it difficult to obtain a complete copy of the ARES data without a complete list of valid RNs. We collected a subset of the entire datasets by requesting the RNs we found in other datasets. The Czech public procurement register was one such dataset, so we gathered data about all business entities participating in the Czech public procurement if their valid RN was published. The downside of the method is that it potentially leaves out much unidentified business entities, since there are almost 2.8 million business entities in total according to the BR as of September 2016.<a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a> Moreover, this number excludes the now defunct entities that could have been involved in the Czech public procurement before their dissolution date. In total, as of November 2016 we harvested data about 204 620 distinct entities either in PR or TLR. Out of these, 161 403 business entities were present in both registries.</p>
          <p>What we made was thus a snapshot of data valid at the harvest date. However, business entities change in time and so does the data in ARES that describes them. For instance, companies may move to different postal addresses. Without the complete history of the registers, access to the previous addresses is unavailable. Since we have obtained only a snapshot of the data, it was missing the historical data. This deficiency turned out to be detrimental to linking business entities by making it more difficult to identify the correct reference entities to link.</p>
          <p>Thanks to the uniform API that ARES provides the ETL of both registers differs only in the URL parameters and the XSL transformations that map XML data to RDF. The data transformation was done using UnifiedViews. A custom component of UnifiedViews, called a data processing unit<a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a> (DPU), was used to fetch data from ARES. The raw source data in XML was transformed into RDF/XML by using XSL stylesheets. A mixture of RDF vocabularies was used to describe the ARES data, with the key roles played by the GoodRelations <span class="citation" data-cites="Hepp2008">(Hepp <a href="#ref-Hepp2008">2008</a>)</span> and the Registered Organization Vocabulary <span class="citation" data-cites="Archer2013">(Archer et al. <a href="#ref-Archer2013">2013</a>)</span>. The retrieved data was relatively consistent, so it did not require much cleaning. However, we paid a special care to cleaning postal addresses, since we needed them for geocoding. SPARQL Update operations were employed to clean and structure the addresses. The data transformation<a href="#fn69" class="footnoteRef" id="fnref69"><sup>69</sup></a> was released as open source. Most of the transformation was done by Jakub Klímek from the Charles University in Prague with a contribution of this dissertation’s author, in particular regarding the XSL stylesheets and SPARQL Update operations.</p>
          <p>We used a subset of BR containing a classification of the registered business entities. The organizations in BR are assigned concepts from the Statistical Classification of Economic Activities in the European Community (NACE). NACE is a hierarchical classification that describes the economic activities pursued by business entities. A subset of BR in CSV that contained the links to NACE was provided to us via personal communication with Ondřej Kokeš who harvested it from ARES. We extracted 873 thousand links to NACE from this subset and converted them to RDF via Tarql<a href="#fn70" class="footnoteRef" id="fnref70"><sup>70</sup></a>, a command-line tool for converting tabular data to RDF via SPARQL CONSTRUCT queries. Links to NACE were available for 89.5 % of organizations in the Czech public procurement register that were linked to ARES.</p>
          <p>The version of NACE that these links use is CZ-NACE,<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a> a Czech extension to NACE Rev. 2 that adds specific leaf concepts. CZ-NACE is maintained by the CSO, which provided us with this classification in XML. We converted the source data to RDF by using a custom Python script.</p>
          <!--
          3209 organizations from the Czech public procurement register that are in ARES are missing links to NACE
          All links to NACE lead to valid codes.
          27359 organizations linked to NACE (30568 total ARES, 37322 total unlinked)
          -->
          </section>
          <section id="sec:czech-addresses" class="level4">
          <h4><span class="header-section-number">2.4.6.3</span> Czech addresses</h4>
          <p>In order to provide the postal addresses in the Czech public procurement data with geo-coordinates, we extracted the Czech addresses data from the Registry of territorial identification, addresses, and real estate<a href="#fn72" class="footnoteRef" id="fnref72"><sup>72</sup></a> (RÚIAN). The registry contains 2.9 million addresses<a href="#fn73" class="footnoteRef" id="fnref73"><sup>73</sup></a> located in the Czech Republic. <!-- Each address is linked to one of the Czech municipalities. --> The addresses refer to locations of buildings that can be assigned unambiguous addresses.<a href="#fn74" class="footnoteRef" id="fnref74"><sup>74</sup></a> Most addresses are provided with representative address points. For example, geo-coordinates of an address point may be located at the entrance of the building its address is assigned to.</p>
          <p>The Czech addresses data is available in CSV.<a href="#fn75" class="footnoteRef" id="fnref75"><sup>75</sup></a> We used LP-ETL to transform it to RDF. Each address was modelled as an instance of <code>schema:PostalAddress</code>. The RÚIAN-specific attributes, such as the orientational number or the building type, were described with the RÚIAN Ontology previously developed by the OpenData.cz initiative. Since each row in the source data is independent of the others, it was possible to use the chunked transformation in LP-ETL to process smaller batches of rows separately and thus decrease the execution time of the transformation. The resulting data, consisting of 42 million RDF triples, was loaded into a Virtuoso RDF store.</p>
          <p>The Czech addresses data uses Systém Jednotné trigonometrické sítě katastrální (S-JTSK)<a href="#fn76" class="footnoteRef" id="fnref76"><sup>76</sup></a> as its coordinate reference system (CRS). S-JTSK is based on the Křovák projection, which was designed specifically for the Czechoslovakia to provide more precise geo-coordinates than another reference system would. However, the standard CRS used in web applications is the World Geodetic System (WGS84). Data using S-JTSK is thus not directly interoperable with many existing datasets. If data adhering to multiple coordinate reference systems are to be used together, they must be reprojected to a single CRS to make their geo-coordinates comparable. Reprojection carries with it a loss of precision, but it is minute. The error in the conversion from S-JTSK to WGS84 using a transformation key is below 1 meter. The largest error, close to 1 meter, can be observed for geo-coordinates near the borders of the Czech Republic.<a href="#fn77" class="footnoteRef" id="fnref77"><sup>77</sup></a> We therefore decided to trade this minor loss in precision for increased interoperability and reprojected S-JTSK to WGS84.</p>
          <p>At the time the data was transformed (September 2016) LP-ETL did not support reprojection of geo-coordinates. In its current version (as of September 2017) it features a component<a href="#fn78" class="footnoteRef" id="fnref78"><sup>78</sup></a> that offers this functionality. We thus implemented the reprojection as a separate step following the data transformation in LP-ETL. We developed a command-line tool that requested the original geo-coordinates in paged batches by using SPARQL SELECT queries, reprojected them, and uploaded the batches back to the RDF store using SPARQL Update operations. The geo-coordinates were reprojected via the open source GeoTools<a href="#fn79" class="footnoteRef" id="fnref79"><sup>79</sup></a> Java library.</p>
          <p>According to its documentation, the Czech addresses dataset uses the EPSG:5514<a href="#fn80" class="footnoteRef" id="fnref80"><sup>80</sup></a> variant of the S-JTSK CRS since 2011. The variant in use till 2011 was EPSG:2065.<a href="#fn81" class="footnoteRef" id="fnref81"><sup>81</sup></a> Contrary to the documentation, we discovered that the reprojection delivered more precise results if EPSG:2065 was used instead of EPSG:5514, when compared to the results of the RÚIAN reprojection service.<a href="#fn82" class="footnoteRef" id="fnref82"><sup>82</sup></a> We may ascribe this difference to the precision of the transformation keys that were used for the compared variants. Nevertheless, the differences among the variants ranged in centimeters, so that they were negligible for the purposes we wanted to use the geo-coordinates.</p>
          <p>In fact, the reprojection of the Czech addresses geo-coordinates would not be necessary if we only computed distances within this dataset. However, the reprojection was needed in order to be able to compare the geo-coordinates with WGS84 geo-coordinates produced by existing geocoding services for the purpose of evaluation of geocoding, as described in the sec. <a href="#sec:geocoding">2.4.5</a>. Moreover, the reprojection to a standard coordinate reference system generally improved the ease of use of the data. For example, map visualizations, that are typically done using software libraries expecting WGS84 geo-coordinates, could thus avoid using to on-the-fly reprojections of the data.</p>
          </section>
          <section id="sec:zindex" class="level4">
          <h4><span class="header-section-number">2.4.6.4</span> zIndex</h4>
          <p>zIndex<a href="#fn83" class="footnoteRef" id="fnref83"><sup>83</sup></a> grades Czech contracting authorities with fairness scores. The scores are based on the contracting authority’s adherence to good practices in public procurement as observed from the data it discloses <span class="citation" data-cites="Soudek2016b">(Soudek <a href="#ref-Soudek2016b">2016</a><a href="#ref-Soudek2016b">b</a>)</span>. As its authors suggest, high zIndex score implies that there is less room for mismanagement of public funds, while a low score indicates the opposite. zIndex scores are normalized to the interval between 0 and 1, in which 1 represents the best score. The index is produced by the EconLab,<a href="#fn84" class="footnoteRef" id="fnref84"><sup>84</sup></a> a Czech economic NGO focused on public policy.</p>
          <p>Our case-based reasoning approach to matchmaking works under the assumption that the awarded bidders constitute cases of successful solutions to public contracts. As we discuss at length further in the sec. <a href="#sec:ground-truth">3.1</a>, this assumption may not be universally valid, considering that bidders may be awarded for reasons other than providing the best offer. zIndex gives us a counter-measure to balance this assumption by weighting each award by the fairness score of its contracting authority.</p>
          <p>However, the perceived fairness of contracting authorities may change over time and so do their zIndex scores that are based on a specific period of the contracting authority’s history. In our case, most scores zIndex scores we had were derived from the period from 2011 to 2013. As such, they are most relevant for public contracts dated at the end of this period, and may be misleading for the contracts awarded in years further apart.</p>
          <p>zIndex scores were initially supplied to the author by Datlab s.r.o.<a href="#fn85" class="footnoteRef" id="fnref85"><sup>85</sup></a> in September 2014. An updated snapshot of zIndex was provided upon request in January 2017. The data in CSV was transformed to RDF by using Tarql. The RDF version of the data was represented as a simple data cube using the Data Cube Vocabulary <span class="citation" data-cites="Cyganiak2014a">(Cyganiak and Reynolds <a href="#ref-Cyganiak2014a">2014</a>)</span>. Each zIndex score was modelled as a measure of an observation indexed by the dimensions of the scored contracting authority and the rating period. Contracting authorities in this dataset are identified by their IRIs from ARES, which are automatically derived from their RNs. The scores are available for 29.4 % of the contracting authorities in our dataset. <!-- 4989 out of 16982. --></p>
          </section>
          </section>
          </section>
          <section id="sec:fusion" class="level2">
          <h2><span class="header-section-number">2.5</span> Fusion</h2>
          <p>Data fusion can be defined as <em>“the process of integrating multiple data items representing the same real-world object into a single, consistent, and clean representation”</em> <span class="citation" data-cites="Bizer2009">(Bizer et al. <a href="#ref-Bizer2009">2009</a>)</span>. In order to reach this goal, data fusion removes invalid or non-preferred data, so that <em>“duplicate representations are combined and fused into a single representation while inconsistencies in the data are resolved”</em> <span class="citation" data-cites="Bleiholder2008">(Bleiholder and Naumann <a href="#ref-Bleiholder2008">2008</a>, p. 1:3)</span>. Fusion of RDF data can be considered a counter-measure to the effects of the principle of <em>Anyone can say anything about anything</em> (AAA). As Klyne and Carroll state, <em>“RDF cannot prevent anyone from making nonsensical or inconsistent assertions, and applications that build upon RDF must find ways to deal with conflicting sources of information”</em> <span class="citation" data-cites="Klyne2002">(<a href="#ref-Klyne2002">2002</a>)</span>.</p>
          <p>In line with the principle of separation of concerns, data fusion expects equivalence links between conflicting identities to be provided. However, it is not limited to a mechanical application of the equivalence links produced by linking. Its particular focus <em>“lies in resolving value-level contradictions among the different representations of a single real-world object”</em> <span class="citation" data-cites="Naumann2006">(Naumann et al. <a href="#ref-Naumann2006">2006</a>, p. 22)</span>.</p>
          <p>Viewed from the perspective of data fusion, linking is a way to discover identity conflicts. Identity conflicts arise when a single entity is provided with multiple identities. Identities in RDF correspond either to IRIs or blank nodes. Resolution of identity conflicts gives rise to data conflicts in turn. Rewriting an identity with another identity automatically merges the RDF triples in which the identities appear. Merging RDF triples may consequently cause functional properties to have multiple values, which constitutes a conflict. <!--
          Conflicts handled by data fusion are typically divided into contradictions, where multiple non-null values are provided for a functional property, and uncertainties, where a functional property has both null and a non-null value.
          However, since there are no nulls in RDF, conflicts in RDF are limited to contradictions.
          --></p>
          <p>Fusion may be executed iteratively, interleaved with linking. This is expedient in case of large datasets, which are computationally demanding to process. Iterating fusion with linking allows to shrink the size of the processed data and thus decrease the number of comparisons that linking needs to perform. Moreover, in case of large datasets, the steps of linking and data fusion may be limited to subsets of data in order to improve the performance of the whole workflow.</p>
          <!--
          Data from the Czech public procurement register has many characteristics of user-generated content.
          Uncoordinated civil servants are akin to the distributed user base of web applications.
          Lack of rules and constraints enforced on user input
          Exchanging data in self-contained documents
          *"the default mode of authoring is copy and edit"* [@Guha2013]
          
          Public procurement data also suffers from shortcomings similar to those of user-generated data.
          The users generating data for the public procurement registers usually comprise many contracting authorities.
          Each authority may produce data digressing from the mandated data standards in a different way.
          Due to the distinct interpretations of the extent of mandatory and discretionary data by contracting authorities, the resulting aggregated dataset may appear to be incomplete.
          Additionally, public procurement data is typically collected from forms filled out by people, who may inadvertently or purposely enter errors into the data they create.
          A shortcoming of public procurement data that becomes apparent in data integration is the lack of global, agreed-upon and well-maintained identifier schemes for values of attributes of public contracts; such as the award criteria employed in the course of selecting the winning bid for a contract.
          Coletta et al. claim that data integration is harder in the context of public sector data because important metadata is often missing [-@Coletta2012].
          Fazekas discusses a similar set of issues of public procurement data from Hungary and highlights missing identifiers, imprecise links, and structural weaknesses [@Fazekas2012, p. 14].
          A corollary of these issues is that tracking public contracts through the stages of their life-cycle, from their announcement over to completion, is difficult because of the lack of reliable identifiers.
          -->
          <p>In order to simplify the resolution of identity conflicts, we adopted a conventional directionality of the <code>owl:sameAs</code> links from a non-preferred IRI to the preferred IRI. This convention allowed us to use a uniform SPARQL Update operation to resolve non-preferred IRIs to their preferred counterparts. For example, if there is a triple <code>:a owl:sameAs :b</code>, <code>:a</code> as the non-preferred IRI will be rewritten to <code>:b</code>. Note that this convention is applicable only if you can distinguish between non-preferred and preferred IRIs, such as by preferring IRIs from a reference dataset.</p>
          <p>Data conflicts arose only in properties that can be interpreted as functional. Some of these properties explicitly instantiate <code>owl:FunctionalProperty</code>, such as <code>pc:kind</code> describing the kind of a contract, while others, such as <code>dcterms:title</code> expressing the contract’s title, can be endowed with this semantics for the purpose of attaining a unified view of the fused data. Most of our data fusion work was devoted to resolving data from contract notices. As was the case of identity conflicts, the resolution of data conflicts was done via SPARQL Update operations.</p>
          <p>Conflicts are resolved by using resolution functions. Resolution functions are either <em>deciding</em>, which pick one of their inputs, or <em>mediating</em>, which derive their output from the inputs. An example deciding function is picking the maximum value, while an example mediating function is computing the median value. We employed deciding conflict resolution functions.</p>
          <section id="conflict-resolution-strategies" class="level3">
          <h3><span class="header-section-number">2.5.1</span> Conflict resolution strategies</h3>
          <p>The conflict resolution strategies we implemented can be classified according to Bleiholder and Naumann <span class="citation" data-cites="Bleiholder2006">(<a href="#ref-Bleiholder2006">2006</a>)</span>. We used <em>Trust your friends</em> <span class="citation" data-cites="Bleiholder2006">(Bleiholder and Naumann <a href="#ref-Bleiholder2006">2006</a>, p. 3)</span> strategy to prefer values from ARES, since we consider it a trustworthy reference dataset. Leveraging the semantics of notice types, we preferred data from correction notices. A similar reason led us to remove syntactically invalid RNs in case valid RNs were present too. We used <em>Keep up to date</em> <span class="citation" data-cites="Bleiholder2006">(Bleiholder and Naumann <a href="#ref-Bleiholder2006">2006</a>, p. 3)</span> metadata-based conflict resolution strategy to prefer values from the most recent public notices. We determined the temporal order of notices from their submission dates and the semantics of their types, which represent an implicit order. For example, prior information notice comes before contract notice, which in turn precedes contract award notice. The order of notice types can be <em>learnt</em> from the most common order of notices with immediately following submission dates. We combined such distribution of subsequent notice types with manual assessment to rule out erroneous pairs. The order of notice types was provided as an inline table to the SPARQL Update operation resolving the conflicts. In line with this strategy, we also preferred the most recent values of <code>pc:awardDate</code>. We used <em>Most specific concept</em> <span class="citation" data-cites="Bleiholder2006">(Bleiholder and Naumann <a href="#ref-Bleiholder2006">2006</a>, p. 4)</span> strategy for resolution of conflicts in values from hierarchical concept schemes. In case a single functional property linked multiple concepts that were in a hierarchical relation, the most specific concepts were retained. For instance, we removed procedure types that can be transitively inferred by following <code>skos:broaderTransitive</code> links. We used <em>No gossiping</em> <span class="citation" data-cites="Bleiholder2006">(Bleiholder and Naumann <a href="#ref-Bleiholder2006">2006</a>, p. 3)</span> strategy for conflicting boolean values. If a boolean property has both <code>true</code> and <code>false</code> value, and there is no way to prioritize a value, we conclude the true value of the property is unknown, and therefore delete both conflicting values. Once the conflicts were resolved by the above-described strategies, we moved the remaining notice data to the associated contracts, which corresponds to the strategy <em>Take the information</em> <span class="citation" data-cites="Bleiholder2006">(Bleiholder and Naumann <a href="#ref-Bleiholder2006">2006</a>, p. 3)</span>. We excluded notice’s proper data, such as submission date or notice type, from this step. If all previous conflict resolution strategies failed, in select cases we followed <em>Roll the dice</em> <span class="citation" data-cites="Bleiholder2006">(Bleiholder and Naumann <a href="#ref-Bleiholder2006">2006</a>, p. 5)</span> strategy and picked a random value via the <code>SAMPLE</code> aggregate function in SPARQL. We did this for procedure types (values of <code>pc:procedureType</code>), contracting authorities (values of <code>pc:contractingAuthority</code>) without valid RNs, and actual prices (values of <code>pc:actualPrice</code>).</p>
          <p>As the final polishing touch we excised the resources orphaned during data fusion. Since removing orphans may create more orphans, we deleted orphans in the topological order based on their links. In this way we first removed orphans, followed by deleting their dependent resources that were orphaned next.</p>
          </section>
          <section id="evaluation-2" class="level3">
          <h3><span class="header-section-number">2.5.2</span> Evaluation</h3>
          <p>If we decide to evaluate the quality of data fusion, there are several measures available. One of the broadest measures for assessing data fusion is data reduction ratio, which represents the decrease of the number of fused entities. This figure corresponds to the measure of extensional conciseness defined by Bleiholder and Naumann <span class="citation" data-cites="Bleiholder2008">(<a href="#ref-Bleiholder2008">2008</a>, pp. 1:5–1:6)</span> as the <em>“percentage of real-world objects covered by that dataset.”</em> Many evaluation measures used for data fusion reflect the impact of this task on data quality. An example of those measures is completeness, which represents the ratio of instances having value for a specified property before and after fusion, and is sometimes rephrased as coverage and density <span class="citation" data-cites="Akoka2007">(Akoka et al. <a href="#ref-Akoka2007">2007</a>)</span>.</p>
          <p>Compared with the raw extracted datasets, fusion decreased the number of distinct entities by 61.68 % to 2 million. Overall, fusion reduced the data by 52.14 % from 20.5 million triples to 9.8 million.</p>
          <!--
          [@Bleiholder2008]
          Completeness
          Conciseness
          Consistency
          - Intensional and extensional
          -->
          </section>
          </section>
          <section id="loading" class="level2">
          <h2><span class="header-section-number">2.6</span> Loading</h2>
          <p>The final part of ETL is loading. In our case, the aim of loading is to expose data in a way our matchmaking methods can operate on efficiently. Our two approaches to matchmaking warrant two approaches to loading.</p>
          <section id="sparql-based-matchmakers" class="level3">
          <h3><span class="header-section-number">2.6.1</span> SPARQL-based matchmakers</h3>
          <p>The SPARQL-based matchmakers require data to be available via the SPARQL protocol <span class="citation" data-cites="Feigenbaum2013">(Feigenbaum et al. <a href="#ref-Feigenbaum2013">2013</a>)</span>. The SPARQL protocol describes the communication between clients and SPARQL endpoints, which provide query interfaces to RDF stores. Exposing data via the SPARQL protocol thus requires simply to load the data into an RDF store equipped with a SPARQL endpoint. We chose to use the open source version of Virtuoso<a href="#fn86" class="footnoteRef" id="fnref86"><sup>86</sup></a> from OpenLink as our RDF store. Even though Virtuoso lacks in stability and adherence to the SPARQL standard, it redeems that by offering a performance unparalleled by other open source RDF stores. We used Virtuoso’s bulk loader<a href="#fn87" class="footnoteRef" id="fnref87"><sup>87</sup></a> to ingest RDF data into the store.</p>
          </section>
          <section id="sec:loading-rescal" class="level3">
          <h3><span class="header-section-number">2.6.2</span> RESCAL-based matchmakers</h3>
          <p>The RESCAL-based matchmakers operate on tensors. Tensors are multidimensional arrays typically used to represent multi-relational data. The number of dimensions of a tensor, also known as ways or modes <span class="citation" data-cites="Kolda2009">(Kolda and Bader <a href="#ref-Kolda2009">2009</a>)</span>, is referred to as its order. Tensors usually denote the higher-order arrays: first-order tensors are vectors and second-order tensors are matrices.</p>
          <!--
          Further formalization of tensors (add if needed for the further explanations):
          
          - Don't use "rank", since it is also used for the number of rows of the latent factor matrix $A$ (and the dimensions of the latent factor tensor $\mathcal{R}$).
          
          $\mathcal{X}_{k}$ is a $k$-th frontal slice of tensor $\mathcal{X}$
          
          Third-order tensor $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$.
          
          Adjacency tensors
          Slices: two-dimensional subarrays/sections of tensors (i.e. matrices)
          - Frontal slices of tensors correspond to adjacency matrices of given predicates.
            - $\mathcal{X}_{::k}$
            - There are horizontal, lateral, and frontal slices.
          Fibers: one-dimensional subarrays of tensors (i.e. vectors)
          - Fibers fix all tensor indices but one.
          -->
          <!-- Tensor representation of RDF -->
          <p>Higher-order tensors provide a simple way to model multi-relational data, such as RDF. Since RDF predicates are binary relations, RDF data can be represented as a third-order tensor, in which two modes represent RDF resources in a domain and the third mode represents relation types; i.e. RDF predicates <span class="citation" data-cites="Tresp2014">(Tresp and Nickel <a href="#ref-Tresp2014">2014</a>)</span>. The two modes are formed by concatenating the subjects and objects in RDF data. The mode-3 slices of such tensors, also referred to as frontal slices, are square adjacency matrices that encode the existence of relation <span class="math inline">\(R_{k}\)</span> between RDF resources <span class="math inline">\(E_{i}\)</span> and <span class="math inline">\(E_{j}\)</span>, as depicted in the fig. <a href="#fig:third-order-tensor">7</a>. <!-- _b --> Consequently, RDF can be modelled as <span class="math inline">\(n \times n \times m\)</span> tensor <span class="math inline">\(\mathcal{X}\)</span>, where <span class="math inline">\(n\)</span> is the number of entities and <span class="math inline">\(m\)</span> is the number of relations. If the <span class="math inline">\(i\)</span><sup>th</sup> entity is related by the <span class="math inline">\(k\)</span><sup>th</sup> predicate to the <span class="math inline">\(j\)</span><sup>th</sup> entity, then the tensor entry <span class="math inline">\(\mathcal{X}_{ijk} = 1\)</span>. Otherwise, if such relation is missing or unknown, the tensor entry is zero.</p>
          <figure>
          <img src="resources/img/third_order_tensor.png" alt="Figure 7: Frontal slices of a third-order tensor, adopted from Nickel et al. (2011)" id="fig:third-order-tensor" style="width:50.0%" /><figcaption>Figure 7: Frontal slices of a third-order tensor, adopted from <span class="citation" data-cites="Nickel2011">Nickel et al. (<a href="#ref-Nickel2011">2011</a>)</span></figcaption>
          </figure>
          <p>There are a couple of things to note about tensors representing RDF data. Entities in these tensors are not assumed to be homogeneous. Instead, they may instantiate different classes. Moreover, no distinction between ontological and instance relations is maintained, so that both classes and instances are modelled as entities. In this way, <em>“ontologies are handled like soft constraints, meaning that the additional information present in an ontology guides the factorization to semantically more reasonable results”</em> <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 273)</span>. Tensors representing RDF are usually very sparse due to high dimensionality and incompleteness, calling in for algorithms that leverage their sparseness for efficient execution, in particular for large data. Scalable processing of large RDF datasets in the tensor form is thus a challenge for optimization techniques. Interestingly, unlike RDF, tensors can represent n-ary relations without decomposing them into binary relations. What would in RDF require reification or named graphs can be captured with greater tensor order. This presents an opportunity for more expressive modelling outside of the boundaries of RDF.</p>
          <p>We developed <em>sparql-to-tensor</em>, described in the sec. <a href="#sec:sparql-to-tensor">6.2.9</a>, to export RDF data from a SPARQL endpoint to the tensor form. The transformation is defined by SPARQL SELECT queries given to this export tool. Each query retrieves data for one or more RDF properties that constitute the relations in the output tensor. During the evaluation, we created and tested many tensors, each combining different properties and ways of pre-processing.</p>
          <p>In most cases the retrieved relations corresponded to explicit RDF properties found in the source data. However, in a few select cases we constructed new relations. This was done either to avoid intermediate resources, such as tenders relating awarded bidders or proxy concepts relating unqualified CPV concepts, or to relate numeric values discretized to intervals. Since the original RESCAL algorithm does not support continuous variables, we discretized such variables via <em>discretize-sparql</em>, which is covered in the sec. <a href="#sec:discretize-sparql">6.2.1</a>. We applied discretization to the actual prices of contracts, which we split into 15 equifrequent intervals having approximately the same number of members.</p>
          <p>Apart from binary numbers as tensor entries we used float numbers <span class="math inline">\(\mathcal{X}_{ijk} \in \mathbb{R} \colon 0 \leq \mathcal{X}_{ijk} \leq 1\)</span> to distinguish the degrees of importance of relations. Float entries were used to de-emphasize less descriptive RDF properties, such as <code>pc:additionalObject</code>, or to model information loss from ageing, so that older contract awards bear less relevance than newer ones. We reused the ageing function from <span class="citation" data-cites="Kuchar2016">(Kuchař et al. <a href="#ref-Kuchar2016">2016</a>, p. 212)</span> to compute the tensor entries:</p>
          <p><span class="math display">\[\mathcal{A}(t_{0}) = \mathcal{A}(t_{x}) \cdot e^{-\lambda t}; t_{0} &gt; t_{x}, t = t_{0} - t_{x}\]</span></p>
          <p>In this function <em>“<span class="math inline">\(\mathcal{A}(t_{0})\)</span> is the amount of information at the time <span class="math inline">\(t_{0}\)</span>. <span class="math inline">\(\mathcal{A}(t_{x})\)</span> is the amount of information at the time <span class="math inline">\(t_{x}\)</span> when the information was created, <span class="math inline">\(\lambda\)</span> is ageing/retention factor and <span class="math inline">\(t\)</span> is the age of the information.”</em> We assume <span class="math inline">\(\mathcal{A}(t_{x})\)</span> to be equal to 1, the same value used for relations encoded without ageing. Since our dataset covers a period of 10 years, we use <span class="math inline">\(\lambda = 0.005\)</span> that provides a distribution of values spanning approximately over this period. We used contract awards dates as values of <span class="math inline">\(t_{x}\)</span> and the latest award date as <span class="math inline">\(t_{0}\)</span>. Award dates were unknown for the 2.3 % of contracts, for which we used the median value of the known award dates. The ageing function was implemented in a SPARQL SELECT query. Since the required natural exponential function is not natively supported in SPARQL, we used the extension function <code>exp()</code><a href="#fn88" class="footnoteRef" id="fnref88"><sup>88</sup></a> built in the Virtuoso RDF store to compute it.</p>
          <!-- Feature selection
          
          `:awardedBidder` (i.e. pc:awardedTender/pc:bidder, weighted by pc:awardDate)
          `pc:mainObject`
          `pc:additionalObject`
          `skos:closeMatch`
          `skos:related`
          `skos:broaderTransitive`
          `rov:orgActivity`
          -->
          <p>Instead of exporting all RDF data to the tensor format, we selected few features from it that we deemed to be the most informative. There are 76 different relations in the Czech public procurement dataset in total. Even more relations are available if we add the linked data. We experimented with selecting individual relations as well as their combinations to find out which ones produce the best results. We guided this search by the assumption that the contributions of the individual relations do not cancel themselves.</p>
          </section>
          </section>
          <section id="sec:data-summary" class="level2">
          <h2><span class="header-section-number">2.7</span> Summary</h2>
          <p>Data preparation constituted a fundamental part of our research, since linked data offloads many concerns typically resolves on the application level to the data level. As a result of our data preparation effort a collection of interlinked datasets was created, as depicted in the fig. <a href="#fig:cloud">6</a>. The Czech public procurement dataset is central to this collection, including the primary data we used for matchmaking. The remaining datasets enrich the public procurement data with contextual information that can be turned into additional features for matchmaking. These datasets include the Common Procurement Vocabulary, three business registers mediated via the ARES system, Czech addresses dataset, NACE classification, and zIndex fairness scores. We encountered many challenges during the preparation of these datasets.</p>
          <p>Since it is collectively created by thousands of officials representing contracting authorities over time, the Czech public procurement dataset suffers from the same problems as user-generated data, resulting in inconsistency and heterogeneity. Standardization can counteract these problems, but the standardization of public procurement data is imperfect at best. Moreover, as discussed in the sec. <a href="#sec:public-procurement">1.5</a>, public procurement is laden with disincentives to publishing good data. A key data quality problem we encountered was missing data. In particular, shared identifiers of entities involved in public procurement were non-existent, missing, or unreliable. In other cases there were conflicting values in the data, without enough annotations to discern the correct values and resolve their conflicts. A more detailed description of the quality of the Czech public procurement data is available in <span class="citation" data-cites="Soudek2016a">Soudek (<a href="#ref-Soudek2016a">2016</a><a href="#ref-Soudek2016a">a</a>)</span>.</p>
          <!--
          Problems:
          - Reductive use of XML
          - Violations of the allegedly validated rules
          -->
          <p>In order to combat the afore-mentioned data quality problems, we invested a lot of effort into linking (sec. <a href="#sec:linking">2.4</a>) and fusion (sec. <a href="#sec:fusion">2.5</a>) of the data. The primary task we addressed was to reduce the variety of the data by conforming values, fusing aliases, or resolving value conflicts. Our approach to ETL adopted the separation of concerns as its basic design principle. In this way, we reduced the complexity of the data preparation and avoided bugs that could be caused by needless coupling. Moreover, the ETL procedures were specified in a declarative fashion, mostly by using XSLT and SPARQL Update operations, so that we could abstract from low-level implementation details that an imperative solution would need take into account. We made defensive data transformations with few assumptions about the processed data. The transformations usually checked if their input satisfied their assumptions and were able to cope with violations of the assumptions via fallback solutions. We designed a way of partitioning the transformations to allow scaling to larger data. We adopted the principles of content-based addressing for deduplication. Finally, when we could not remedy the problems of the data, we explicitly acknowledged the limitations of data, such as in case of the systemic biases manifest in public procurement data.</p>
          </section>
          </section>
          <section id="sec:methods" class="level1">
          <h1><span class="header-section-number">3</span> Matchmaking methods</h1>
          <!--
          Why we have chosen these methods?
          Should we formulate requirements for the matchmaking methods?
          -->
          <p>We applied two methods to matching public contracts to bidders: case-based reasoning (CBR) and statistical relational learning (SRL). We first review what these methods have in common and then discuss their differences. Both methods learn from the same ground truth and have to cope with its limitations and biases, described in the sec. <a href="#sec:ground-truth">3.1</a>, such as having only positive training examples. In this ground truth, public contracts represent explicit demands and contract awards model past behaviour of bidders offering products or services. Both methods learn only from their input data, not from user feedback. In order to incorporate user feedback, it would need to materialized as part of the input data. This approach is known as one-shot recommendation, and is typical for case-based recommenders in particular <span class="citation" data-cites="Smyth2007">(Smyth <a href="#ref-Smyth2007">2007</a>)</span>. We employed manual feature selection, corresponding to schema-aware matchmaking. Portability of the developed matchmakers is granted by the common data model underlain by the Public Contracts Ontology, which we covered in the sec. <a href="#sec:pco">2.1.1</a>. The matchmakers therefore work with any dataset described by the PCO, such as the Czech public procurement dataset that constitutes our use case. Both methods are evaluated on the task of predicting the awarded bidders. The inverse task of recommending relevant public contracts to bidders is feasible as well, but we have not focused on it, since it mirrors the evaluated task.</p>
          <!-- Case-based reasoning -->
          <p>The underlying technology we used to implement the matchmakers based on case-based reasoning, introduced in the sec. <a href="#sec:cbr">1.6.1</a>, is SPARQL <span class="citation" data-cites="Harris2013">(Harris and Seaborne <a href="#ref-Harris2013">2013</a>)</span>. Using the means of SPARQL we designed a custom-built matchmaking method, explained in detail in the sec. <a href="#sec:method-sparql">3.2</a>. In line with the CBR perspective, this method recasts data on awarded contracts as past cases to learn from. Viewed this way, awarded contracts can be considered as experiences of solved problems and contract awards can be thus interpreted as implicit positive ratings of the awarded bidders. Consequently, bidders awarded with most contracts similar to a given query contract can be recommended as potential awardees of the contract.</p>
          <p>The developed matchmakers implement only the <em>Retrieve</em> and <em>Reuse</em> steps from the CBR cycle. The retrieved matches are ranked to produce recommendations for reuse. Including the <em>Revise</em> step would require the matchmakers to incorporate user feedback. The <em>Retain</em> step is not applicable if the proposed matches are not approved or disapproved in the <em>Revise</em> step. Both the <em>Knowledge representation</em> and the <em>Problem formulation</em> steps can be considered to be incorporated in data preparation, as documented in the sec. <a href="#sec:data-preparation">2</a>, since both cases and queries are materialized as data.</p>
          <p>Matchmaking via SPARQL is conceived as a top-<span class="math inline">\(k\)</span> recommendation task. It produces a list of bidders sorted by their degree to which they match the requirements of a given query contract. Since there is no explicit model built by this method, it is a case of lazy learning. Having no model to create up front allows to answer matchmaking queries in real time and to update the queried data in an incremental fashion.</p>
          <!-- Statistical relational learning -->
          <p>Matchmakers based on statistical relational learning (SRL), which we presented in the sec. <a href="#sec:srl">1.6.2</a>, are built on RESCAL <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span>, In this case, we adopted an existing learning method for the matchmaking task, as explained in the sec. <a href="#sec:method-rescal">3.3</a>. Viewed from the perspective of SRL, matchmaking can be conceived as link prediction. In our setting, the task of matchmaking is predicting the most likely links between public contracts and their winning bidders.</p>
          <p>Unlike the method based on SPARQL, RESCAL is a latent feature model. Since it builds a prediction model up front, it is an example of eager learning. Consequently, it operates in a batch mode that allows to update data only in bulk.</p>
          <p>The key differences between the use of CBR and SRL for matchmaking are summarized in the tbl. <a href="#tbl:matchmaking-methods-differences">1</a>. Matchmaking can be also implemented via hybrid methods that combine multiple approaches. For instance, SPARQL can be used to pre-select matches and RESCAL can then re-rank this selection.</p>
          <div id="tbl:matchmaking-methods-differences">
          <table>
          <caption>Table 1: Differences of the adopted matchmaking methods</caption>
          <thead>
          <tr class="header">
          <th style="text-align: left;">Method</th>
          <th style="text-align: left;">CBR</th>
          <th style="text-align: left;">SRL</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td style="text-align: left;">Underlying technology</td>
          <td style="text-align: left;">SPARQL</td>
          <td style="text-align: left;">RESCAL</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;">Method origin</td>
          <td style="text-align: left;">custom-built</td>
          <td style="text-align: left;">reused</td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;">Learning method</td>
          <td style="text-align: left;">lazy learning</td>
          <td style="text-align: left;">eager learning</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;">Matchmaking conceived as</td>
          <td style="text-align: left;">top-<span class="math inline">\(k\)</span> recommendation</td>
          <td style="text-align: left;">link prediction</td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;">Features</td>
          <td style="text-align: left;">observable</td>
          <td style="text-align: left;">latent</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;">Mode of operation</td>
          <td style="text-align: left;">on demand query</td>
          <td style="text-align: left;">batch</td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;">Update</td>
          <td style="text-align: left;">incremental</td>
          <td style="text-align: left;">bulk</td>
          </tr>
          </tbody>
          </table>
          </div>
          <!--
          SPARQL and full-text matchmakers are "lazy learners", since they do not build explicit models.
          Since there is no model, performance might be worse. (Why?)
          We can consider database indices to be the "models".
          
          Limitation: CBR approach favours larger and longer-established suppliers.
          This is an opportunity to normalize by the bidder's age from ARES.
          
          Using the terminology of case-based reasoning, CPV provides a "bridge attribute" that allows to derive the similarity of contracts from the shared concepts in their descriptions.
          - The other properties can be considered bridge attributes too, right?
          
          Matchmaking basically learns the associations between CPV concepts and bidders from contract awards.
          - Potentially NACE concepts too.
          For each CPV concept the most associated bidders can be found.
          
          Diversity of results is often low in case-based recommenders based on similarity-based retrieval.
          There are several strategies to mitigate this issue:
          - Bounded greedy selection: minimizes total similarity in the result set, while maximizing total similarity of the result set to the query.
          
          Use a more content-based approach (leveraging data from ARES) for cold-start users (i.e. those without an awarded contract)?
          Alternative solutions:
          * Users may subscribe to recommendations for other users. For example, they may be asked to list their competitors, who were awarded public contracts, and be subscribed to their recommendations.
          * Ask users to rate a sample of public contracts either as relevant or irrelevant. The sample must be chosen in order to maximize the insight learnt from the rating, e.g., the sample should be generated dynamically to increase its overall diversity.
          
          If no matches are found:
          - Contracting authority can respecify the query contract.
            - Such contract may be overspecified or merge unrelated goods or services that can be better awarded via multiple contracts.
          - Bidder can ask for recommendation for its competitors.
          -->
          <!--
          Out-takes:
          
          Top-k recommendation: best matches are shown, but not their predicted ratings.
          
          Matchmaking public contracts to bidders can be framed as a task for case-based reasoning.
          Data on awarded contracts can be recast as past cases to learn from.
          
          Public contract ~ case
          Reinterpretation of the previously awarded public contracts as experiences of solved problems.
          Contract award can be interpreted an implicit rating of the awarded bidder.
          Reinterpretation of contract award as a positive rating (in the context of the awarded contract)
          Limitation: We have only positive ratings.
          
          The matchmaker learns from interactions between contracting authorities and bidders
          
          Collaborative recommender systems: explicit offers (product or services) + demand behaviour (user interactions)
          Our case-based recommender: explicit demands (contracts) + offer behaviour (histories of bidders)
          
          Comparison of CBR systems with databases in [@Richter2013, p. 524].
          Mismatch: SPARQL operates under the closed world assumption. CBR assumes open world.
          
          From the perspective of a contracting authority, the task seems like matchmaking.
          From the bidder's perspective, the task seems like recommendation.
          
          ## Modes of delivery
          
          - on demand queries (pull)
          - subscriptions (push, "persistent" queries)
            - Subscription to streams, notifications
            - Push-based recommendations ~ matchmaking subscriptions
              - Proactive recommendation: *"A proactive recommender system pushes recommendations to the user when the current situation seems appropriate, without explicit user request."* (<http://pema2011.cs.ucl.ac.uk/papers/pema2011_vico.pdf>)
          
          ## Notation conventions
          
          We employ conventional notation to describe the matchmaking methods.
          We use $\mathbb{P}$ to denote a power set of a set.
          We use $a \oplus b$ to denote concatenation of n-tuples $a$ and $b$.
          We mark the set of tuples of elements from the set $S$ as $(S)$.
          Composition of functions $f$ and $g$ is denoted $f \circ g$.
          
          Feature selection as a way of mitigating the curse of dimensionality?
          [@Ragone2017]
          
          ## One-shot recommendation
          
          A limitation of our approach is that it works as a one-shot recommendation that does not take user feedback on the generated recommendations into account.
          Since the matchmakers do not have a conversational interface with which users can iteratively refine their query, if no suitable match is found, users need to revise their query and start again, even though they may not be able to provide a detailed query from the start.
          This can be characterized as a query-based approach, in which users have to respecify their query in case no results are found.
          One-shot recommendation is typical for case-based recommenders [@Smyth2007].
          The opposite is true of conversational recommender systems that elicit user feedback to refine their recommendations.
          For example, users may provide a critique, such as requiring cheaper matches.
          Critiques can be interpreted as directional feature constraints [@Smyth2007, p. 361].
          
          Moreover, SPARQL requires *"users to express their needs in a single query"*. (FIXME: Missing a citation!)
          This is why the matchmaker employs a single-shot approach.
          
          Is there a way to provide user feedback?
          Browsing-based approaches: navigation of the item space, for example using critique-based navigation
          - Critiquing can be used to reformulate matchmaking queries (e.g., assign different weights) or query the results (e.g., filter to meet the critique).
          -->
          <section id="sec:ground-truth" class="level2">
          <h2><span class="header-section-number">3.1</span> Ground truth</h2>
          <p>The fundamental part of the proposed matchmaking methods is the ground truth they are based on. We use past contracts awards as the ground truth from which the methods learn to assess matches. As such, it warrants a dedicated section to discuss its characteristics, in order to provide a better context for the following treatment of the matchmaking methods.</p>
          <p>There are inherent downsides in our assumptions about the ground truth we used. The assumption that the awarded bidder is the best match for a contract is fundamentally problematic. We need to take into account that bidders may be awarded on the basis of adverse selection, e.g., caused by asymmetric distribution of information. Alternatively, tendering processes can suffer from collusion when multiple parties agree to limit open competition. In that case, rival bidders cooperate for mutual benefit, for instance, by bid rigging that involves submitting overpriced fake bids to make the real bids more appealing. Neither we can assume that bidders who were awarded multiple contracts from the same contracting authority have proven their quality. Instead, they may just be cases of clientelism.</p>
          <p>Moreover, we have to rely on contract awards only, since we do not have explicit evaluations of the awarded bidders after finishing the contracts. Unfortunately, the lack of post-award data is common in public procurement:</p>
          <blockquote>
          <p><em>“With a few exceptions such as Italy and Estonia, no government publishes information on contract implementation, making it impossible to know what happens after the contract is awarded — for example, did the suppliers deliver on time and budget?”</em> <span class="citation" data-cites="Mendes2017">(Mendes and Fazekas <a href="#ref-Mendes2017">2017</a>)</span></p>
          </blockquote>
          <p>Nor do we have any other relations between bidders and contracts in our dataset. Even though the profiles of contracting authorities link contracts with all bidders that submitted a valid bid, we have not included the profiles data in our dataset due to the effort involved in obtaining it.</p>
          <!--
          Cartels are explicit collusion agreements.
          A close problem: monopoly
          
          Can we identify "bad" bidders? Do they exhibit certain patterns that we can recognize in the data?
          (Perhaps we can use data from ÚOHS. However, Sbírka rozhodnutí by ÚOHS is not machine readable.)
          
          What we have is this: Similar contracts are usually awarded to these bidders.
          
          Matchmaking can therefore serve only as pre-filtering.
          The problem with filtering is that it potentially leaves relevant bidders behind, so that we cannot say that the bias will be dealt with by manual screening of the matches.
          
          Since learning from contracts awarded in the past is the fundamental part of our machine learning approach, the key question is this: Is the bias severe enough to make it better to avoid learning from past contracts?
          
          Nevertheless, how can matchmaking work without learning from the awarded contracts? Can it only employ similarity-based retrieval?
          -->
          <p>We devised several counter-measures to ameliorate the impact of adverse selection in our ground truth. We experimented with discounting contract awards by the zIndex scores, described in the sec. <a href="#sec:zindex">2.4.6.4</a>, of their contracting authorities. However, this is a blunt tool, since it applies across the board for all contracts by a contracting authority. Within large contracting authorities each contract may be administered by a different civil servant, who may change over time.</p>
          <p>We experimented with limiting our ground truth only to contracts awarded in open procedures. The intuition motivating this experiment is that a contract awarded in an open procedure enables fairer competition and thus avoids some risks of adverse selection.</p>
          <p>We also considered restricting the contract awards to learn from by their award criteria. While it may seem that the simple criterion of lowest price is fair, it may be skewed by intentionally inflated fake bids due to bidder collusion. Other, more complex award criteria, such as those emphasizing qualitative aspects, can be problematic too. Their evaluation leaves more room for deliberation of contracting authorities, and as such, they can be made less transparent. Faced with this uncertainty, we ultimately avoided limiting our ground truth by contract award criteria.</p>
          <p>Nevertheless, a likely outcome of these corrective measures is performance loss in the evaluation via retrospective data. Matchmakers may be underfitting, unable to sufficiently capture the underlying trends in the public procurement data, which too include the biases from adverse selection. On contrary, learning from all contract awards overfits, so that it includes the negative effects in public procurement as well. It may mistake random variability and systemic biases for causality. As a result, the inherent biases in our ground truth are difficult to account for.</p>
          </section>
          <section id="sec:method-sparql" class="level2">
          <h2><span class="header-section-number">3.2</span> SPARQL</h2>
          <p>The SPARQL-based matchmaker employs a case-based reasoning approach that learns from contracts awarded in the past. For each awarded contract a similarity to a given query contract is computed and the contracts are grouped by bidders who won them. The similarity scores in each group are aggregated and sorted in descending order. The matchmaker uses both semantic and statistical properties of data on which it operates. While the semantics of contract descriptions is employed in the similarity measurement, score aggregation reflects the statistics about the past participation of bidders in public procurement <span class="citation" data-cites="AlvarezRodriguez2013">(Alvarez-Rodríguez et al. <a href="#ref-AlvarezRodriguez2013">2013</a>, p. 122)</span>.</p>
          <!-- The publication activity around the matchmaker is inessential.
          It should therefore not start this section. -->
          <p>The initial version of the SPARQL-based matchmaker was introduced in <span class="citation" data-cites="Mynarz2014b">(Mynarz et al. <a href="#ref-Mynarz2014b">2014</a>)</span>. Our subsequent publication <span class="citation" data-cites="Mynarz2015">(Mynarz et al. <a href="#ref-Mynarz2015">2015</a>)</span> covers an improved version of the matchmaker. The hereby described version is thus the third iteration of the matchmaker with extended configurability.</p>
          <section id="benefits-and-drawbacks" class="level3">
          <h3><span class="header-section-number">3.2.1</span> Benefits and drawbacks</h3>
          <p>This matchmaker explores the use of SPARQL <span class="citation" data-cites="Harris2013">(Harris and Seaborne <a href="#ref-Harris2013">2013</a>)</span> for matchmaking. We introduced SPARQL in the sec. <a href="#sec:linked-data">1.4.2</a>. The choice of this technology for matchmaking has both benefits and drawbacks.</p>
          <section id="benefits" class="level4">
          <h4><span class="header-section-number">3.2.1.1</span> Benefits</h4>
          <!-- Nativeness -->
          <p>SPARQL is a native way of querying and manipulating RDF data. As it is designed for RDF, it is based on graph pattern matching. Graph patterns in SPARQL are based on data in the Turtle syntax <span class="citation" data-cites="Beckett2014">(Beckett et al. <a href="#ref-Beckett2014">2014</a>)</span> extended with variables. Consequently, there is little impedance mismatch between data and queries, which improves developer productivity.</p>
          <!-- Universality -->
          <p>The design of SPARQL makes it into a universal tool for working with RDF. Thanks to its expressivity and declarative formulation it can be used for many varied tasks. For example, besides matchmaking we also adopted it as our primary tool for data preparation, as described in the sec. <a href="#sec:data-preparation">2</a>.</p>
          <!-- Standardization -->
          <p>SPARQL is a standard <span class="citation" data-cites="Harris2013">(Harris and Seaborne <a href="#ref-Harris2013">2013</a>)</span>, so most RDF stores support it. The matchmaker can thus be set up simply by loading data into an RDF store. Since the matchmaker is limited to the standard SPARQL without proprietary add-ons or extension functions, it is portable across RDF stores compliant with the SPARQL specifications. As such it is not tied to any single RDF store vendor.</p>
          <!-- Real-time -->
          <p>SPARQL operates directly on indices of RDF databases, so there is no need to pre-process data or build a machine learning model. In terms of recommender systems, we can consider it a memory-based approach. Thanks to this feature, SPARQL can answer matchmaking queries in real time. In particular, this is useful for recommendations from streaming data. Public procurement data shares some of the characteristics of streaming data as it becomes quickly obsolete due to its currency bound on fixed deadlines for tender submission.</p>
          </section>
          <section id="drawbacks" class="level4">
          <h4><span class="header-section-number">3.2.1.2</span> Drawbacks</h4>
          <p>The benefits of SPARQL come with costs. As Maali <span class="citation" data-cites="Maali2014">(<a href="#ref-Maali2014">2014</a>, p. 57)</span> writes, the pure declarative nature and expressivity of SPARQL implies a high evaluation cost. RDF stores in general suffer from a performance penalty when compared to relational databases. Nevertheless, recent advancements in the application of the column store technology for RDF data brought on large performance improvements <span class="citation" data-cites="Boncz2014">(Boncz et al. <a href="#ref-Boncz2014">2014</a>, p. 23)</span>. SPARQL also lends itself to advanced query optimization that can avoid much of the performance costs.</p>
          <!-- Limitation to exact matches -->
          <p>In order to get the best performance of SPARQL, the matchmaker is limited to joins based on exact matches. SPARQL supports just exact matches natively. Exact matches can distinguish only between identical and non-identical resources. Fuzzy matches are needed to differentiate the degrees of similarity between resources. However, fuzzy matches have to be implemented on top of the default graph pattern matching in SPARQL. For example, the <code>FILTER</code> clauses can match partially overlapping strings or numbers within a given distance. SPARQL is not designed to perform such matches efficiently. Although SPARQL engines can optimize fuzzy matches, e.g., by using additional indices for literals, if literals are not indexed, they have to be analysed at query time, which incurs a significant performance penalty for queries employing fuzzy matches.</p>
          <!-- Materialize-then-sort -->
          <p>Performance of the matchmakers is also degraded by the unnecessary work SPARQL does for top-<span class="math inline">\(k\)</span> queries. SPARQL employs the materialize-then-sort query execution scheme <span class="citation" data-cites="Magliacane2012">(Magliacane et al. <a href="#ref-Magliacane2012">2012</a>, p. 345)</span>, which implies that the matchmaker needs to compute scores for all matched solutions prior to sorting them, even though only top <span class="math inline">\(k\)</span> matches are retrieved. Matchmaking in SPARQL depends on aggregations and sorting, both of which are examples of operations called the pipeline breakers in the query execution model. Such operations prevent lazy execution, since they require their complete input to be realized. For example, SPARQL treats sorting as a result modifier, which needs to be provided with all results.</p>
          </section>
          </section>
          <section id="ranking" class="level3">
          <h3><span class="header-section-number">3.2.2</span> Ranking</h3>
          <p>SPARQL queries retrieve exact matches satisfying the query conditions. Since SPARQL can tell only matches from non-matches, matches that satisfy the query partially are left out. Ranking of matches by their degree to which they satisfy the query thus needs to be implemented on top of SPARQL. Hence, we need to relax the match conditions to avoid filtering partial matches and then compute scores to rank the matches.</p>
          <p>The matchmaker operates with a given query contract <span class="math inline">\(c_{q}\)</span>, which is matched to contracts from the set <span class="math inline">\(C\)</span>. <!-- _b --> It retrieves contract objects that overlap with the object of the query contract, which are optionally expanded to include related CPV concepts. Components of contract objects are weighted and these weights are combined into partial similarity scores. Partial similarities are then aggregated per bidder to produce the bidder’s match score.</p>
          <section id="sec:contract-objects" class="level4">
          <h4><span class="header-section-number">3.2.2.1</span> Contract objects</h4>
          <p>Contract objects describe what products or services are sought by contracts. There are many ways how a contract object can be described. The matchmakers leverage contract objects described by terms from controlled vocabularies, such as CPV or the code list of contract kinds. Concretely, the matchmakers can use CPV concepts, either as main or additional objects or their qualifiers (<code>pc:mainObject</code>, <code>pc:additionalObject</code>), contract kinds (<code>pc:kind</code>), and service categories (<code>isvz:serviceCategory</code>). Accordingly, we define the set of properties <span class="math inline">\(P = \{\texttt{pc:mainObject}, \texttt{pc:additionalObject}, \texttt{pc:kind}, \texttt{isvz:serviceCategory}\}\)</span> that associate concepts with contracts. The range of each of these properties is enumerated by a controlled vocabulary. We define the union of concepts in these vocabularies as <span class="math inline">\(Con = Con_{CPV} \cup Con_{kind} \cup Con_{\substack{service \\ category}}\)</span>. <!-- _b --> A concept can be either explicitly assigned to a contract or inferred via query expansion. To capture this distinction we use concept assignment <span class="math inline">\(ConA = \{\text{explicit}, \text{inferred}\}\)</span>. Contract object <span class="math inline">\(cobj\)</span> is then a tuple <span class="math inline">\(((con, p), cona) \colon con \in Con, p \in P, cona \in ConA\)</span>, in which a concept <span class="math inline">\(con\)</span> is paired with property <span class="math inline">\(p\)</span> that associates the concept to a contract and this pair is qualified with the concept assignment <span class="math inline">\(cona\)</span>. Contract objects are represented as sets <span class="math inline">\(Cobj\)</span> of these tuples. In order to obtain contract objects we use the function <span class="math inline">\(obj \colon C \cup \left\{c_{q}\right\} \to \mathbb{P}(Cobj)\)</span>. <!-- _b --> Here, <span class="math inline">\(\mathbb{P}(Cobj)\)</span> denotes the power set of the set <span class="math inline">\(Cobj\)</span>. Accessing the elements of contract objects is in turn done by the function <span class="math inline">\(ccobj(cobj) = con \iff cobj = ((con, p), cona)\)</span> for concepts and by the function <span class="math inline">\(pcobj(cobj) = p \iff cobj = ((con, p), cona)\)</span> for properties.</p>
          </section>
          <section id="sec:query-expansion" class="level4">
          <h4><span class="header-section-number">3.2.2.2</span> Query expansion</h4>
          <p>Controlled vocabularies that describe contract objects can be semantically structured, such as via hierarchical or associative relations. Since relevance of a concept may entail relevance of concepts in its neighbourhood, we can leverage the structure of these vocabularies and perform expansion to include the related concepts in the query. In particular, we expand CPV concepts by following transitive hierarchical relations in this vocabulary. We follow either links to narrower concepts via <code>skos:narrowerTransitive</code>, links to broader concepts via <code>skos:broaderTransitive</code>, or links in both directions. Query expansion can be parameterized by the maximum number of hops followed to obtain a graph neighbourhood of the expanded concept. When a concept is expanded, its inferred concepts include those that are one to the maximum hops away from the expanded concept. Note that it is possible to infer a concept already included in the explicitly assigned concepts when these are hierarchically related. In such case, the concept appears twice in the contract object, distinguished by its concept assignment. Similarly, the same inferred concept can be reached more times by expanding different concepts. Such concept is present once in the results of query expansion since the results form a set. The figures <a href="#fig:expand-to-broader">8</a> and <a href="#fig:expand-to-narrower">9</a> illustrate the query expansion, showing expansions to two-hop neighbourhoods.</p>
          <figure>
          <img src="resources/img/query_expansion_broader.png" alt="Figure 8: Expansion to broader concepts" id="fig:expand-to-broader" style="width:50.0%" /><figcaption>Figure 8: Expansion to broader concepts</figcaption>
          </figure>
          <figure>
          <img src="resources/img/query_expansion_narrower.png" alt="Figure 9: Expansion to narrower concepts" id="fig:expand-to-narrower" style="width:50.0%" /><figcaption>Figure 9: Expansion to narrower concepts</figcaption>
          </figure>
          <p>The arguments of the query expansion function <span class="math inline">\(exp\)</span> are a set of contract objects, a direction of expansion, and a distance of the expansion. The direction of expansion <span class="math inline">\(Dir\)</span> is the set <span class="math inline">\(\{\texttt{skos:broaderTransitive}, \texttt{skos:narrowerTransitive}\}\)</span> indicating either the expansion to broader or narrower concepts. The distance is the maximum number of hops followed in the expansion. Consequently, the query expansion function can be defined as <span class="math inline">\(exp \colon \mathbb{P}(Cobj) \times Dir \times \mathbb{N}_{&gt; 0} \to \mathbb{P}(Cobj)\)</span>. <!-- _b --> Bidirectional expansion of the set of contract objects <span class="math inline">\(\{cobj\} \subset Cobj\)</span> to the distance <span class="math inline">\(dis\)</span> can thus be computed as <span class="math inline">\(exp(\{cobj\}, \texttt{skos:broaderTransitive}, dis) \cup exp(\{cobj\}, \texttt{skos:narrowerTransitive}, dis)\)</span>. We only require <span class="math inline">\(exp\)</span> to be monotonous, so that for every contract object <span class="math inline">\(cobj \in Cobj\)</span> holds that <span class="math inline">\(((con, p), cona) \in cobj \;\; \Rightarrow \;\; ((con, p), cona) \in exp(cobj, dir, dis)\)</span>, and hence the function <span class="math inline">\(exp\)</span> returns a union of its provided contract objects with the inferred contract objects. Concrete instantiations of <span class="math inline">\(exp\)</span> can limit which input contract objects are expanded. In our case, either no contract objects are expanded or we only expand the explicitly assigned contract objects where <span class="math inline">\(p = \texttt{pc:mainObject}\)</span> and <span class="math inline">\(con \in Con_{CPV}\)</span>. <!-- _b --></p>
          </section>
          <section id="matching" class="level4">
          <h4><span class="header-section-number">3.2.2.3</span> Matching</h4>
          <p>The matchmakers examine only the exact matches between concepts of contract objects. Instead of matching complete contract descriptions or sets of concepts, matching on the finer level of individual concepts allows to capture partial overlaps between contracts. The predicate <span class="math inline">\(matches \colon Cobj \times Cobj \to \{T, F\}\)</span> returns the boolean value true, denoted as <span class="math inline">\(T\)</span>, if concepts in the compared contract objects are the same, otherwise returning the boolean value false, denoted as <span class="math inline">\(F\)</span>.</p>
          <p><span class="math display">\[matches(cobj_{a}, cobj_{b}) =
            \begin{cases}
              T &amp; \text{if}\ ccobj(cobj_{a}) = ccobj(cobj_{b}) \\
              F &amp; \textrm{otherwise} \\
            \end{cases}\]</span> <!-- _b --></p>
          <p>Here, <span class="math inline">\(ccobj(cobj_{a})\)</span> accesses the concept in the contract object <span class="math inline">\(cobj_{a}\)</span>. As is evident, in order to achieve a match, the ranges of the properties in the compared contract objects must be the same.</p>
          <p>Matching considers its input as the query contract, while the other contracts are treated as potential matches. The function <span class="math inline">\(match \colon \left\{c_{q}\right\} \times Dir \times \mathbb{N}_{&gt; 0} \to \mathbb{P}(CMA)\)</span> retrieves concept-mediated associations matching a given query contract <span class="math inline">\(c_{q}\)</span>.</p>
          <p><span class="math display">\[match(c_{q}, dir, dis) = \bigcup \left\{ \begin{split}
            &amp; (ccobj(o_{q}), pcobj(o_{q}), pcobj(o_{m}), c_{m}) \colon \\
            &amp; o_{q} \in exp(obj(c_{q}), dir, dis), \\
            &amp; c_{m} \in C, \\
            &amp; o_{m} \in obj(c_{m}), \\
            &amp; matches(o_{q}, o_{m})
            \end{split}\right\}\]</span> <!-- _b --></p>
          <p>The direction of expansion <span class="math inline">\(dir\)</span> and the distance <span class="math inline">\(dis\)</span> are passed as arguments to the query expansion function <span class="math inline">\(exp\)</span>. The function <span class="math inline">\(match\)</span> produces a set of concept-mediated associations <span class="math inline">\(CMA\)</span> that are defined as 4-tuples <span class="math inline">\((con, p_{q}, p_{m}, c_{m}) \colon con \in Con, p_{q} \in P, p_{m} \in P, c_{m} \in C\)</span>. We call them concept-mediated associations since they connect the query contract with the matched contracts via concepts. In each association <span class="math inline">\(p_{q}\)</span> is a property associating a concept <span class="math inline">\(con\)</span> to the query contract and <span class="math inline">\(p_{m}\)</span> is a property associating <span class="math inline">\(con\)</span> to a matched contract <span class="math inline">\(c_{m}\)</span>. <!-- _b --></p>
          <figure>
          <img src="resources/img/matchmaking_overall_diagram.png" alt="Figure 10: Overall diagram of concept-mediated associations" id="fig:matchmaking-overall-diagram" style="width:75.0%" /><figcaption>Figure 10: Overall diagram of concept-mediated associations</figcaption>
          </figure>
          <figure>
          <img src="resources/img/concept_associations.png" alt="Figure 11: Concept-mediated associations between contracts" id="fig:concept-associations" style="width:75.0%" /><figcaption>Figure 11: Concept-mediated associations between contracts</figcaption>
          </figure>
          <p>The fig. <a href="#fig:matchmaking-overall-diagram">10</a> shows an overall diagram of concept-mediated associations. The query contract <span class="math inline">\(c_{q}\)</span> is associated to the matched contracts <span class="math inline">\(c_{1}, c_{2}, c_{3} \in C\)</span> via concepts that are assigned to the query contract via <span class="math inline">\(p_{q_{i}}\)</span> and to the matched contracts via <span class="math inline">\(p_{m_{i}}\)</span>. As shown in fig. <a href="#fig:concept-associations">11</a>, contracts may be associated through different kinds of concepts. The matched contracts in turn lead to bidders <span class="math inline">\(b_{1}, b_{2} \in B\)</span>. Here, <span class="math inline">\(B\)</span> is the set of known bidders. Contracting authority of <span class="math inline">\(c_{q}\)</span> is marked as <span class="math inline">\(a_{q}\)</span>, while the contracting authority of <span class="math inline">\(c_{3}\)</span> is denoted as <span class="math inline">\(a_{3}\)</span>. For <span class="math inline">\(a_{3}\)</span> a zIndex score <span class="math inline">\(z_{3}\)</span> is available.</p>
          </section>
          <section id="weighting" class="level4">
          <h4><span class="header-section-number">3.2.2.4</span> Weighting</h4>
          <p>The matchmaker can translate each part of concept-mediated associations into a weight <span class="math inline">\(w \in \mathbb{R} \colon 0 \leq w \leq 1\)</span>. In certain variants of the matchmakers the reference to concept <span class="math inline">\(con\)</span> is transformed to an inverse document frequency (IDF), in particular when dealing with concepts obtained via query expansion. Similarly, the properties <span class="math inline">\(p_{q}\)</span> and <span class="math inline">\(p_{m}\)</span> can be weighted according to the degree in which they contribute to the similarity between contracts. Likewise, the contract <span class="math inline">\(c_{m}\)</span> can be turned into a weight corresponding to its contracting authority’s fairness score. <!-- _b --> Some weights are given by data, such as the fairness scores, or derived from it, such as IDFs. Others can be provided as configuration of the matchmaker, such as the inhibiting weight of <code>pc:additionalObject</code>.</p>
          <p>There are several concrete ways in which weights can be applied to CPV concepts. The matchmaker may apply an inhibiting weight to de-emphasize the concepts associated with contracts via the <code>pc:additionalObject</code> property in contrast to the <code>pc:mainObject</code>. These weights are applied both to <span class="math inline">\(p_{q}\)</span> and <span class="math inline">\(p_{m}\)</span>. <!--
          We could allow different weights for associations with the query contract and the matched contracts.
          For example, an additional object of the query contract can be weighted lower than an additional object of a matched contract.
          Such distinction may be motivated by different levels of trust in the consistency of contract descriptions of among the contract authorities.
          For instance, while an authority may assign a smaller weight to additional objects of its contracts, it may assume a higher weight of additional objects of contract by other authorities.
          --> Similarly, qualifying concepts from the CPV’s supplementary vocabulary can be discounted via a lower weight. Concepts inferred by query expansion can be weighted either by a fixed inhibiting weight or their IDF.</p>
          <p>Inverse document frequency is used to reduce the impact of popular CPV concepts on matchmaking. Unlike infrequent and specific concepts, the popular ones may have lesser discriminative power to determine the relevance of contracts described by them. Raw IDF of CPV concepts is defined as <span class="math inline">\(idf \colon Con_{CPV} \to \mathbb{R}^{+}\)</span> and is computed as follows: <!-- _b --></p>
          <p><span class="math display">\[idf(con) = \log\frac{\left\vert{C}\right\vert}{1 + \left\vert{\{c \in C \colon ((con, p), cona) \in obj(c)}\right\vert}\]</span></p>
          <p>The denominator in the formula is incremented by 1 to avoid division by zero in case of concepts unused in contract objects. Subsequently, we normalize IDF into the range of <span class="math inline">\(\left[0, 1\right]\)</span> by using its maximum value in order to be able to use it as a weight.</p>
          <p><span class="math display">\[idf&#39;(con) = \frac{idf(con)}{max(\{con&#39; \in Con_{CPV} \colon idf(con&#39;)\})}\]</span> <!-- _b --></p>
          <p>Besides CPV, weights can be applied to specific properties from <span class="math inline">\(P\)</span>. In particular, the matchmaker can inhibit the objects of <code>pc:kind</code> when used in combination with CPV. This property indicates the kinds of contracts, such as works or supplies, which classify contracts into broad categories.</p>
          <p>The matchmaker also allows to weight the matched contracts indirectly via weights of their contracting authorities. We use zIndex scores as weights of contracting authorities. These scores are taken from the dataset covered in the sec. <a href="#sec:zindex">2.4.6.4</a>. We assume the function <span class="math inline">\(authority \colon C \to Auth\)</span> returns the contracting authority of a given contract. Here, <span class="math inline">\(Auth\)</span> denotes the set of known contracting authorities. The function <span class="math inline">\(zindex \colon Auth \to \left[0, 1\right]\)</span> produces a weight given to a contracting authority by the zIndex score. The function weighting by zIndex can then be defined by composing these functions; i.e. <span class="math inline">\(zindex \circ authority\)</span>.</p>
          </section>
          <section id="sec:aggregation-functions" class="level4">
          <h4><span class="header-section-number">3.2.2.5</span> Aggregation functions</h4>
          <p>We use aggregation functions to turn weights into match scores. Weights of components in each concept-mediated association are combined using the function <span class="math inline">\(comb\)</span>. The combined weights are aggregated via the function <span class="math inline">\(agg\)</span>.</p>
          <p>Aggregation functions take multiple numeric inputs and combine them into a single output. The matchmaker uses these functions to combine weights and partial similarity scores to form a match score. As such, aggregation functions constitute an important part of ranking. In terms of fuzzy logic, aggregation functions can be interpreted as generalizations of logical conjunction and disjunction. Instead of only boolean values, their inputs can be treated as degrees of probability, where 0 indicates impossibility and 1 indicates certainty. Aggregation function <span class="math inline">\(f\)</span> can thus be defined as <span class="math inline">\(f \colon \left[0, 1\right] \times \left[0, 1\right] \to \left[0, 1\right]\)</span> <span class="citation" data-cites="Beliakov2015">(Beliakov et al. <a href="#ref-Beliakov2015">2015</a>, p. 785)</span>.</p>
          <p>The typical examples of these functions are triangular norms (t-norms) and conorms (t-conorms). T-norms generalize conjunction and t-conorms generalize disjunction. The basic t-norms can be defined as follows <span class="citation" data-cites="Beliakov2015">(Beliakov et al. <a href="#ref-Beliakov2015">2015</a>, p. 792)</span>:</p>
          <ul>
          <li>Gödel’s t-norm (minimum t-norm): <span class="math inline">\(T_{min}(x, y) = min(x, y)\)</span></li>
          <li>Product t-norm: <span class="math inline">\(T_{P}(x, y) = x \cdot y\)</span></li>
          <li>Łukasiewicz’s t-norm: <span class="math inline">\(T_{L}(x, y) = max(x + y - 1, 0)\)</span> <!-- _b --></li>
          </ul>
          <p>We use these t-norms to combine weights by the function <span class="math inline">\(comb\)</span>. The basic t-conorms, complementary to the mentioned t-norms, are the following:</p>
          <ul>
          <li>Gödel’s t-conorm (maximum t-conorm): <span class="math inline">\(S_{max}(x, y) = max(x, y)\)</span></li>
          <li>Product t-conorm (probabilistic sum): <span class="math inline">\(S_{P}(x, y) = x + y - x \cdot y\)</span></li>
          <li>Łukasiewicz’s t-conorm (bounded sum): <span class="math inline">\(S_{L}(x, y) = min(x + y, 1)\)</span> <!-- _b --></li>
          </ul>
          <p>We use these t-conorms to aggregate contract similarities into the match scores of bidders by the function <span class="math inline">\(agg\)</span>. Both t-norms and t-conorms are associative and commutative, so their computation can be extended to arbitrary collections of weights.</p>
          <p>Given these formulas we can summarize how the matchmaker works. The matchmaker retrieves concept-mediated associations <span class="math inline">\(cma \in match(c_{q}, dir, dis)\)</span> for a query contract <span class="math inline">\(c_{q}\)</span> using a configuration of query expansion and weighting. Concept associations are partitioned into subsets by the bidder awarded with the contract <span class="math inline">\(c_{m}\)</span> from the association. <!-- _b --> <!-- We can also create subpartitions by contract within these partitions. The aggregated weights in these subpartitions can be considered contract similarity scores. This may be closer to the informal explanation of matchmaking, yet it is unnecessary. --> Each concept-mediated association in these partitions is subsequently weighted to produce an n-tuple of weights. The obtained weights are combined to a single weight of each concept-mediated association via the <span class="math inline">\(comb\)</span> function. An n-tuple of the combined weights from a partition by bidder can be then aggregated by the <span class="math inline">\(agg\)</span> function to produce match scores. Finally, the matches are sorted by their score in descending order and the top-<span class="math inline">\(k\)</span> matches are output.</p>
          <!-- FIXME: Do we need to formalize sorting and accessing the bidders from concept associations? -->
          </section>
          </section>
          <section id="sec:blind-matchmakers" class="level3">
          <h3><span class="header-section-number">3.2.3</span> Blind matchmakers</h3>
          <p>Apart from the above-described matchmaker, we also implemented three blind approaches for matchmaking, none of which considers the query contract. The most basic is the random matchmaker that recommends bidders at random. While it is hardly going to deliver a competitive accuracy, it produces diverse results. An approach contrary to random matchmaking is the recommendation of the top-most popular bidders. For each contract this matchmaker recommends the same bidders that were awarded the most contracts. A similar approach is employed in the matchmaker that recommends bidders with the highest score computed by the PageRank-like algorithm implemented by the Virtuoso-specific <code>IRI_RANK</code> <span class="citation" data-cites="Virtuoso2017">(OpenLink Software <a href="#ref-Virtuoso2017">2017</a>)</span>. Since this score uses a proprietary extension of SPARQL, it is an exception from our constraint to standard SPARQL. These conceptually and computationally simpler approaches are used as baselines to which we can contrast the more sophisticated ones in evaluation.</p>
          </section>
          <section id="implementation" class="level3">
          <h3><span class="header-section-number">3.2.4</span> Implementation</h3>
          <p>The matchmaker is implemented using SPARQL query templates. Each template receives a configuration and produces a SPARQL query. The generated queries are executed on the configured SPARQL endpoint and return ordered sets of matches. Each kind of matchmaker corresponds to a query template. It may also expose specific parameters that can be provided via the configuration.</p>
          <p>The basic graph pattern considered in most configurations of the matchmaker is illustrated in lst. <a href="#lst:property-path">3</a> using the SPARQL 1.1 Property Path syntax. The path is complicated by intermediate resources proxying CPV concepts connected via <code>skos:closeMatch</code>, as described in the sec. <a href="#sec:concrete-data-model">2.1.2</a>.</p>
          <div id="lst:property-path" class="listing">
          <p>Listing 3: Matchmaker's basic SPARQL property path</p>
          <pre><code>?queryContract ^pc:lot/pc:mainObject/skos:closeMatch/
                         ^skos:closeMatch/^pc:mainObject/pc:lot/
                         pc:awardedTender/pc:bidder ?matchedBidder .</code></pre>
          </div>
          <p>Apart from our baseline matchmaker, which uses the property path in lst. <a href="#lst:property-path">3</a>, the implementation of the matchmakers is based on nested sub-queries and <code>VALUES</code> clauses used to associate the considered properties with weights.</p>
          <p>We implemented query expansion via SPARQL 1.1 property paths. Property paths allow us to retrieve concepts reachable within a given maximum number of hops transitively following the hierarchical relations in CPV. We use the short-hand notation <code>{1, max}</code> for these property paths. It defines a graph neighbourhood at most <code>max</code> hops away. This notation is not a part of the SPARQL standard, but it is formally defined by Seaborne <span class="citation" data-cites="Seaborne2014">(<a href="#ref-Seaborne2014">2014</a>)</span>, and several RDF stores, including Virtuoso, support it. However, it can be rewritten to the more verbose standard SPARQL notation if full standards-compliance is required.</p>
          <p>Score aggregation via aggregation functions is done using SPARQL 1.1 aggregates. However, probabilistic sum requires aggregation by multiplication, which cannot be implemented directly in SPARQL since it lacks an operator to multiply grouped bindings. Therefore, we implemented this aggregation function via post-processing of SPARQL results. Eventually, since the difference on the evaluated metrics between probabilistic sum and summation (<span class="math inline">\(a + b\)</span>) turned out to be statistically insignificant, we opted for summation, which can be computed in SPARQL and is marginally faster. A side effect of this implementation is that the match scores in the matchmaker are not normalized.</p>
          <!-- Optimization -->
          <p>The execution time of the matchmaker can be improved by common optimization techniques for SPARQL. We reordered triple patterns in the matchmaking queries in order to minimize cardinality of the intermediate results. We reduced unnecessary intermediate bindings via blank nodes and property paths. Performance can be also enhanced by materialization of pre-computed data. While there is no need for data pre-processing, derived data that changes infrequently can be materialized and stored in RDF. Doing so can improve performance by avoiding the need to recompute the derived data at query time. This benefit is offset by increased use of storage space and an overhead with updates, since materialized data has to be recomputed when the data it is derived from changes. We used materialization for pre-computing inverse document frequencies (IDF) of CPV concepts. While IDF can be computed on the fly, we decided to pre-compute it and store it as RDF. Computation of IDF is implemented via two declarative SPARQL Update operations, the first of which uses a Virtuoso-specific extension function for logarithm (<code>bif:log10()</code>), and the second normalizes the IDFs using the maximum IDF.</p>
          <p>The matchmaker, described in the sec. <a href="#sec:matchmaker-sparql">6.2.4</a>, is implemented as a wrapper over the Virtuoso RDF store. Example SPARQL queries used by the matchmaker can be found at <a href="https://github.com/opendatacz/matchmaker/wiki/SPARQL-query-examples" class="uri">https://github.com/opendatacz/matchmaker/wiki/SPARQL-query-examples</a>.</p>
          <!--
          Out-takes:
          
          Resources in an RDF graph can be considered similar if their neighbourhoods are similar.
          This leads to iterative computation and propagation of similarities through the RDF graph.
          That is not what SPARQL is suited for.
          -->
          </section>
          </section>
          <section id="sec:method-rescal" class="level2">
          <h2><span class="header-section-number">3.3</span> Tensor factorization</h2>
          <p>Tensor factorization is a method for decomposing tensors, described in the sec. <a href="#sec:loading-rescal">2.6.2</a>, into lower-rank approximations. The rank of a tensor <span class="math inline">\(\mathcal{X}\)</span> is “*the smallest number of rank one tensors that generate <span class="math inline">\(\mathcal{X}\)</span> as their sum“* <span class="citation" data-cites="Kolda2009">(Kolda and Bader <a href="#ref-Kolda2009">2009</a>)</span>. <span class="math inline">\(\mathcal{X}\)</span> is an <span class="math inline">\(N\)</span><sup>th</sup> order rank one tensor when it <em>“can be written as the outer product of <span class="math inline">\(N\)</span> vectors”</em> <span class="citation" data-cites="Kolda2009">(Kolda and Bader <a href="#ref-Kolda2009">2009</a>)</span>: <span class="math inline">\(\mathcal{X} = \mathbf{a}^{(1)} \circ \mathbf{a}^{(2)} \circ \cdots \circ \mathbf{a}^{(N)}\)</span>. Determining the tensor’s rank is known to be an NP-hard problem <span class="citation" data-cites="Sidiropoulos2017">(Sidiropoulos et al. <a href="#ref-Sidiropoulos2017">2017</a>)</span>, so in practice low-rank approximations are used instead. As such, tensor factorization can be considered a dimensionality reduction technique based on the assumption that there exists a low-dimensional embedding of entities in tensors. In fact, computing a tensor factorization is possible because most tensors exhibit latent structure. A theoretical generalization of the abilities of tensor factorization is provided in <span class="citation" data-cites="Nickel2013b">Nickel and Tresp (<a href="#ref-Nickel2013b">2013</a><a href="#ref-Nickel2013b">a</a>)</span>.</p>
          <p>Tensor factorization can be regarded as a generalization of matrix factorization for higher-dimensional arrays. Unlike matrices, tensor representation offers a greater fidelity, since it can preserve the structure of higher order relations that would be lost were these relations collapsed into a matrix representation <span class="citation" data-cites="Morup2011">(Mørup <a href="#ref-Morup2011">2011</a>)</span>. Tensor factorization is also referred to as tensor decomposition. Here, for clarity, we use tensor factorization to denote the process of computing its product, the tensor decomposition.</p>
          <p>Statistical relational learning, introduced in the sec. <a href="#sec:srl">1.6.2</a>, employs tensor factorization for link prediction. Viewed from this perspective, the input of factorization is considered to be a noisy, partially observed tensor. Tensor decomposition produced by the factorization can be in turn used to reconstruct an approximation of the complete tensor. In this way, we can use tensor decompositions as prediction models that explain the predicted links by latent features of entities. Tensor factorization typically yields good results for link prediction in domains characterized by high dimensionality, sparseness <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span>, and noise <span class="citation" data-cites="Zhiltsov2013">(Zhiltsov and Agichtein <a href="#ref-Zhiltsov2013">2013</a>, p. 1254)</span>. So far, it has found applications in many domains, including chemometrics or social network mining. There were also a few attempts to apply tensor factorization to RDF, such as TripleRank <span class="citation" data-cites="Franz2009">(Franz et al. <a href="#ref-Franz2009">2009</a>)</span>, the dominant one being RESCAL <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span>. We reused RESCAL for matchmaking via tensor factorization.</p>
          <!--
          linear combination
          generalization of matrix singular value decomposition
          
          *"The rank of a tensor is given by its minimal sum of rank one components $R$ such that"* [@Morup2011]:
          
          $$\mathcal{X} = \sum_{r=1}^{R} a_{r} \circ b_{r} \circ c_{r}$$
          
          Decomposition addresses high dimensionality and sparseness.
          Tensor factorization is inefficient if data contains a lot of strongly connected graph components.
          
          Relations are functions of a linear combination of latent factors.
          
          matrices $R$ are low-rank matrix approximations
          
          Perspective of probabilistic databases
          
          Hybrid approaches combining multiple methods
          - E.g., re-ranking
          
          Dealing with noise in the data
          -->
          <section id="rescal" class="level3">
          <h3><span class="header-section-number">3.3.1</span> RESCAL</h3>
          <p>RESCAL is a machine learning algorithm for factorization of third-order tensors. It factorizes a tensor <span class="math inline">\(\mathcal{X}\)</span> with <span class="math inline">\(n\)</span> entities to a rank-<span class="math inline">\(r\)</span> representation, so that each frontal slice <span class="math inline">\(\mathcal{X}_{k}\)</span> of the tensor can be approximately reconstructed via matrix product from the decomposition to latent components, as shown in the fig. <a href="#fig:rescal-decomposition">12</a>: <!-- _b --></p>
          <p><span id="eq:rescal-decomposition"><span class="math display">\[X_{k} \approx AR_{k}A^{T}\qquad(1)\]</span></span></p>
          <figure>
          <img src="resources/img/rescal_decomposition.png" alt="Figure 12: RESCAL decomposition, adopted from Nickel et al. (2012)" id="fig:rescal-decomposition" /><figcaption>Figure 12: RESCAL decomposition, adopted from <span class="citation" data-cites="Nickel2012">Nickel et al. (<a href="#ref-Nickel2012">2012</a>)</span></figcaption>
          </figure>
          <p>In this formula, <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times r\)</span> matrix containing the latent component representation of entities in <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(A^{T}\)</span> is its transposition, and <span class="math inline">\(R_{k}\)</span> is a square <span class="math inline">\(r \times r\)</span> matrix modelling the interactions of the latent components in the <span class="math inline">\(k\)</span><sup>th</sup> predicate <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span>. <!-- _b --> Using this decomposition, RESCAL <em>“explains triples via pairwise interactions of latent features”</em> <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 17)</span>. Unlike other latent feature models, latent variables in RESCAL do not describe entity classes but are latent entity factors <span class="citation" data-cites="Tresp2014">(Tresp and Nickel <a href="#ref-Tresp2014">2014</a>)</span>. The rank <span class="math inline">\(r\)</span> is a <em>“a central parameter of factorization methods that determines generalization ability as well as scalability”</em> <span class="citation" data-cites="Nickel2014">(Nickel et al. <a href="#ref-Nickel2014">2014</a>)</span>. While higher <span class="math inline">\(r\)</span> increases the expressiveness of the latent features, it also increases the runtime of tensor factorization as well as its propensity for overfitting. Consequently, setting <span class="math inline">\(r\)</span> to an appropriate value is a key trade-off to be made when tuning RESCAL.</p>
          <p>RESCAL uses distinct latent representations of entities as subjects and objects, which enables efficient information propagation to capture correlations over long-range relational chains <span class="citation" data-cites="Nickel2013c">(Nickel and Tresp <a href="#ref-Nickel2013c">2013</a><a href="#ref-Nickel2013c">b</a>, p. 619)</span> that may span heterogeneous relations. In this way, RESCAL is able to leverage contextual data that is more distant in the relational graph for collective learning, which we described in the sec. <a href="#sec:srl">1.6.2</a>. Unlike other factorization methods that cannot model collective learning sufficiently, <em>“the main advantage of RESCAL, if compared to other tensor factorizations, is that it can exploit a collective learning effect when applied to relational data”</em> <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 272)</span>.</p>
          <p>RESCAL achieves leading performance for link prediction tasks. It was shown to be superior for link prediction tasks on several datasets. Moreover, it scales better to large data than many traditional methods for statistical relational learning, such as Markov logic networks. <span class="citation" data-cites="Nickel2012">Nickel et al. (<a href="#ref-Nickel2012">2012</a>)</span> demonstrated how the execution of RESCAL can be parallelized and distributed across multiple computing nodes. RESCAL is also fundamentally simpler than other tensor factorization methods. Unlike similar algorithms, RESCAL stands out by low Kolmogorov complexity. It is implemented only in 120 lines of code of Python <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span> using only the NumPy<a href="#fn89" class="footnoteRef" id="fnref89"><sup>89</sup></a> library.</p>
          <!-- Extensions -->
          <p>Many extensions of RESCAL were proposed. Its state-of-the-art results and conceptual simplicity invite improvements. The aspects that the extensions deal with include negative training examples, handling literals, or type constraints. We employed some of the discussed extensions for matchmaking.</p>
          <!-- Negative examples -->
          <p>RESCAL adopts the local closed world assumption (LCWA), which is used often for training relational models <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 13)</span>. It <em>“approaches the problem of learning from positive examples only, by assuming that missing triples are very likely not true, an approach that makes sense in a high-dimensional but sparse domain”</em> <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 273)</span>. However, <em>“training on all-positive data is tricky, because the model might easily over generalize”</em> <span class="citation" data-cites="Nickel2016">(Nickel et al. <a href="#ref-Nickel2016">2016</a>, p. 24)</span>. In order to avoid underfitting, negative examples can be generated via type constraints for predicates or valid ranges of literals. <span class="citation" data-cites="Nickel2016">Nickel et al. (<a href="#ref-Nickel2016">2016</a>)</span> proposes generating negative examples by perturbing true triples. Basically, switching subjects in triples sharing the same functional property produces false, but type-consistent triples.</p>
          <!-- Handling literals -->
          <p>The original version of RESCAL <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span> uses only object properties as relations. Datatype properties with literal objects can only be used if the literals are treated as entities. When literals are included as entities, although they never appear as subjects, the tensor’s sparseness grows. Moreover, since the number of distinct literals may significantly surpass the number of entities, this naïve treatment will greatly expand the dimensionality of the input tensor. Both high dimensionality and sparseness thereby increase the complexity of computing the factorization. Minor improvements can be attained by pre-processing literals, such as by discretizing ordinal values, tokenizing plain texts, and stemming the generated tokens. Nevertheless, treatment of literals warrants a more sophisticated approach. To address this issue, <span class="citation" data-cites="Nickel2012">Nickel et al. (<a href="#ref-Nickel2012">2012</a>)</span> introduced an extension of RESCAL to handle literals via an attribute matrix that is factorized conjointly with the tensor with relations between entities. In a similar vein, <span class="citation" data-cites="Zhiltsov2013">Zhiltsov and Agichtein (<a href="#ref-Zhiltsov2013">2013</a>)</span> proposed Ext-RESCAL, an approach using term-based entity descriptions that include names, other datatype properties as attributes, and outgoing links.</p>
          <!-- Type constraints -->
          <p>Several researchers (<span class="citation" data-cites="Chang2014">Chang et al. (<a href="#ref-Chang2014">2014</a>)</span>, <span class="citation" data-cites="Krompass2014">Krompaß et al. (<a href="#ref-Krompass2014">2014</a>)</span>, <span class="citation" data-cites="Krompass2015">Krompaß et al. (<a href="#ref-Krompass2015">2015</a>)</span>) investigated adding type constraints to RESCAL. These constraints improve RESCAL by preventing type-incompatible predictions. The type compatibility can be determined by interpreting the <code>rdfs:domain</code> and <code>rdfs:range</code> axioms under LCWA or by evaluating custom restrictions, such as requiring the subject entity to be older than the object entity when predicting parents. These type constraints can be represented as binary matrices <span class="citation" data-cites="Krompass2014">(Krompaß et al. <a href="#ref-Krompass2014">2014</a>)</span> indicating compatibility of entities. The original RESCAL considers all entities for possible relations, notwithstanding their type, which increases its model complexity and leads to <em>“an avoidable high runtime and memory consumption”</em> <span class="citation" data-cites="Krompass2014">(Krompaß et al. <a href="#ref-Krompass2014">2014</a>)</span>. Even though RESCAL is faster than the type-constrained approach with the same rank, using type constraints typically requires a lower rank to produce results that RESCAL is able of achieving only at higher ranks.</p>
          <!-- Other extensions -->
          <p>Other notable extensions of RESCAL add time awareness or tensor slice similarities. <span class="citation" data-cites="Kuchar2016">Kuchař et al. (<a href="#ref-Kuchar2016">2016</a>)</span> enhanced link prediction via RESCAL to be time-aware. We used this approach in data pre-processing, as described in the sec. <a href="#sec:loading-rescal">2.6.2</a>, to model decaying relevance of older contract awards. <span class="citation" data-cites="Padia2016">Padia et al. (<a href="#ref-Padia2016">2016</a>)</span> computed RESCAL with regard to the similarity of tensor slices to obtain better results.</p>
          </section>
          <section id="ranking-1" class="level3">
          <h3><span class="header-section-number">3.3.2</span> Ranking</h3>
          <p>We applied link prediction via RESCAL to matchmaking, assuming that the tensor decomposition produced by RESCAL can accurately model the affinities between contracts and bidders. Probabilities of links predicted in the tensor slice representing contract awards can be obtained by reconstructing the slice from the tensor decomposition. Given the slice <span class="math inline">\(R_{award}\)</span> for contract awards from the latent factor tensor <span class="math inline">\(\mathcal{R}\)</span> produced by RESCAL, we can obtain predictions of entities awarded with the contract <span class="math inline">\(c\)</span> by following the eq. <a href="#eq:rescal-decomposition">1</a> and computing the vector <span class="math inline">\(p = A_{c}R_{award}A^{T}\)</span>. <!-- _b This vector is also a mode-2 fiber. --> Entries in <span class="math inline">\(p\)</span> can be interpreted as probabilities of the contract <span class="math inline">\(c\)</span> being awarded to entities at corresponding indices in <span class="math inline">\(p\)</span>. Using the indices of bidders we can filter the entries in <span class="math inline">\(p\)</span> and then rank them in descending order to obtain the best matches for <span class="math inline">\(c\)</span>.</p>
          <p>We used no minimal threshold to filter out irrelevant matches. As reported in <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>)</span>, determining a reasonable threshold is difficult, because the high sparseness of the input tensors causes a strong bias towards zero <span class="citation" data-cites="Nickel2012">(Nickel et al. <a href="#ref-Nickel2012">2012</a>, p. 274)</span>. Consequently, instead of setting an arbitrary threshold, we ranked the predictions by their likelihood and projected the top-ranking predictions as the matches. This decision is a trade-off erring on the side of delivering less relevant results instead of producing no results.</p>
          <!--
          Link prediction ranks entries in the reconstructed tensor by their values (components/factors?).
          
          matrix slice $R_{award}$ for contract awards from the latent factor tensor $\mathcal{R}$
          $C' \subset C$ are the withheld contracts
          contract $c \in C'$
          
          select only the entries for indices of bidders
          sort in descending order
          select top 10 entries
          -->
          <!-- Limitations -->
          <p>Unlike SPARQL, RESCAL is a batch approach that cannot produce results in real time. First, it needs to factorize the input tensor to a decomposition that models the tensor. Once this model is built, predictions for individual contracts can be computed on demand. RESCAL is hence slow to cope with changing data. Matchmaking via RESCAL is thus more appropriate if the matches are delivered via periodic subscriptions instead of on-demand queries.</p>
          <!--
          FIXME: Mention blind matchmaker implemented by generating random predictions?
          -->
          </section>
          <section id="implementation-1" class="level3">
          <h3><span class="header-section-number">3.3.3</span> Implementation</h3>
          <p>We implemented <em>matchmaker-rescal</em>, described in the sec. <a href="#sec:matchmaker-rescal">6.2.5</a>, a thin wrapper of RESCAL that runs our evaluation protocol, explained in the sec. <a href="#sec:evaluation-protocol">4.2</a>. Instead of extending RESCAL, our contribution lies in the data preparation and pre-processing described in the sec. <a href="#sec:loading-rescal">2.6.2</a>.</p>
          <p>When developing the RESCAL wrapper, we needed to take several aspects of performance into consideration. Due to the size of the processed data it is important to leverage its sparseness, which is why we employ efficient data structures for sparse matrices from the SciPy<a href="#fn90" class="footnoteRef" id="fnref90"><sup>90</sup></a> library. Reconstructing the whole predictions slice is unfeasible for larger datasets due to its size in memory. In order to reduce the memory footprint of the matchmakers, we avoid reconstructing the whole predictions slice from the RESCAL factorization, but instead reconstruct only the top-<span class="math inline">\(k\)</span> results. Predictions are computed for each row separately, so that the rows can be garbage-collected to free memory. In order to enable parallelization it was important to compile the underlying NumPy library with the OpenBLAS<a href="#fn91" class="footnoteRef" id="fnref91"><sup>91</sup></a> back-end, which allows to leverage multi-core machines for computing low-level array operations, such as the matrix product that is central to RESCAL.</p>
          <!--
          numpy.dot implements the matrix product (i.e. sum of rows times columns)
          -->
          <!--
          Out-takes:
          
          Experiments with YAGO in @Nickel2012 also include materialized `rdf:type` inferences.
          - Should we do the same?
          
          Alternative method for link prediction using tensor representation of RDF:
          <http://semdeep.iiia.csic.es/files/SemDeep-17_paper_3.pdf>
          
          Alternative approach: Markov Random Fields (very flexible, but computationally expensive)
          
          RESCAL exploits idiosyncratic properties of relational data.
          *"RESCAL can be regarded as a latent-variable model for multi-relational data"* [@Nickel2012, p. 273]
          
          RESCAL does not require strict feature modelling.
          RESCAL may be used to generate similarities between entities that may be then used in non-relational methods.
          
          **Comparison with matrix factorization**
          
          RESCAL is similar to matrix factorization (MF) methods used in recommender systems [@Nickel2016, p. 18].
          MF offers good scalability, predictive accuracy, and modelling flexibility [@Koren2009, p. 44].
          MF allows to incorporate both explicit and implicit feedback.
          However, reshaping tensors into matrices causes data loss.
          
          Switching objects in triples sharing the same predicate (under LCWA) is valid for functional properties.
          @Nickel2016 proposes an approach that assumes the generated triples to be likely false.
          -->
          </section>
          </section>
          </section>
          <section id="sec:evaluation" class="level1">
          <h1><span class="header-section-number">4</span> Evaluation</h1>
          <!--
          We evaluated both the statistical and practical significance of our contributions.
          Evaluation of statistical significance was used to rule out the differences between the evaluated matchmakers that could be attributed to random error.
          We considered practical importance of our research as part of the qualitative evaluation.
          -->
          <p>We attempted to demonstrate the utility of the developed matchmakers by evaluating several metrics. We chose metrics that approximate the accuracy and diversity of matchmaking. The metrics were evaluated in an offline setup.</p>
          <!--
          We used a combination of offline evaluation and qualitative evaluation.
          -->
          <!-- TODO: Frame within the context of design science.
          
          The evaluation includes both experimental and non-experimental design.
          The offline evaluation follows an experimental design, while the qualitative evaluation is non-experimental.
          
          In terms of Wieringa [-@Wieringa2014, p. 31], both our offline evaluation and qualitative evaluation are validation, because both use a model of how the developed matchmakers would be used in the real world.
          -->
          <!-- Offline evaluation -->
          <p>Offline evaluation is an experimental setting in which past user interactions are used as ground truth. In this setting, some interactions are withheld and the evaluated system is assessed on its ability to fill in the missing interactions. Offline evaluation is defined in the recommender systems research in contrast to online evaluation. While online evaluation involves users in real-time, offline evaluation approximates actual user behaviour by using pre-recorded user interactions. Offline evaluation then <em>“consists of running several algorithms on the same datasets of user interactions (e.g., ratings) and comparing their performance”</em> <span class="citation" data-cites="Ricci2011">(Ricci et al. <a href="#ref-Ricci2011">2011</a>, p. 16)</span>.</p>
          <!-- Limitations of offline evaluation -->
          <p>While using historical user interaction data for evaluation is a common practice <span class="citation" data-cites="Jannach2010">(Jannach et al. <a href="#ref-Jannach2010">2010</a>, p. 169)</span>, it has several flaws that reduce its predictive power. In addition to the domain-specific limitations of the ground truth, which we described in sec. <a href="#sec:ground-truth">3.1</a>, the datasets used for offline evaluation can be incomplete and may contain systemic biases. Ground truth in the datasets is incomplete, since it typically contains only a fraction of true positives. In most cases, users review only few possible matches, excluding the rest, notwithstanding its relevance, from the true positives. Consequently, if the evaluated system recommends relevant items that are not in the ground truth, these matches are ignored. In other words, <em>“when incomplete datasets are used as ground-truth, recommender systems are evaluated based on how well they can calculate an incomplete ground-truth”</em> <span class="citation" data-cites="Beel2013">(Beel et al. <a href="#ref-Beel2013">2013</a>, p. 11)</span>.</p>
          <p>Due to these limitations offline evaluation has a weak prediction power. As such, <em>“offline evaluations of accuracy are not always meaningful for predicting the relative performance of different techniques”</em> <span class="citation" data-cites="Garcin2014">(Garcin et al. <a href="#ref-Garcin2014">2014</a>, p. 176)</span>. Offline evaluation can tell which of the evaluated approaches provides better results, but it cannot tell if an approach is useful. There is a limited correspondence between the evaluated metrics and usefulness in the real world. Whether an approach is useful can be evaluated only by real users. This is what online evaluation or qualitative evaluation can help with.</p>
          <!-- Upsides -->
          <p>However, one can also argue that <em>“offline evaluations are based on more thorough assessments than online evaluations”</em> <span class="citation" data-cites="Beel2013">(Beel et al. <a href="#ref-Beel2013">2013</a>)</span>. Ground truth in offline evaluation may be derived from more thorough examination of items, involving multiple features in tandem, while online evaluation may rely on superficial assessment, such as click-throughs based on titles only.</p>
          <!-- Online evaluation -->
          <p>Online evaluation is commonly recommended as a remedy to the afore-mentioned limitations of offline evaluation. As a matter of fact, results of online evaluation can differ widely from the results of offline evaluation. Several studies found that <em>“results of offline and online evaluations often contradict each other”</em> <span class="citation" data-cites="Beel2013">(Beel et al. <a href="#ref-Beel2013">2013</a>, p. 7)</span> or acknowledged that <em>“there remains a discrepancy in the offline evaluation protocols, and the online deployment and accuracy estimate of the algorithms in a real-life setting”</em> <span class="citation" data-cites="Said2013">(<a href="#ref-Said2013">2013</a>)</span>. However, conducting online evaluation is expensive since it requires an application with real users. In order to attract a sufficient mass of users to make the findings from the evaluation statistically significant, the application must be relatively mature and proven useful. Moreover, we wanted to explore a large space of different matchmaker configurations, for which carrying out online evaluation would be prohibitively expensive. Ultimately, due to the large effort involved in setting up online evaluation we restricted our work to offline evaluation.</p>
          <!-- Qualitative evaluation -->
          <!--
          Due to the large effort required by setting up an online evaluation, we decided to balance the shortcoming of offline evaluation with qualitative evaluation.
          We used offline evaluation to pre-screen viable matchmaking methods and configurations to select fewer promising variants that we subsequently consulted using semi-structured interviews with domain experts and prospective users of the matchmakers.
          -->
          <!--
          Alternative evaluation protocol, widely used in top-k recommendation: <http://dl.acm.org/citation.cfm?id=1864721>
          
          Evaluated dimensions:
          * Effectiveness (quality)
          * Efficiency (speed)
            - Additional indices may speed up retrieval.
            - Complexity of the distance function.
              - Blocking may be done by lower-bounding distance functions. Such functions are less complex and produce approximate lower distance.
          -->
          <!--
          Out-takes:
          
          Experimental design (experimental evaluation, controlled experiment)
          - Lab studies
          - Matchmaking as a classification task that produces a ranked list of relevant items.
          
          Non-experimental design: qualitative research via interviews with users (or domain experts)
          
          + Descriptive evaluation via example scenarios?
          + Cost-benefit analysis discussing the matchmaker's value compared with the costs in sustaining it (keeping it operable)?
          -->
          <!-- ## Offline evaluation -->
          <!--
          Do we basically do grid search? Configuration can be considered as hyper-parameters. We are basically trying to find the most important (sensitive) hyper-parameters.
          Idea: distribution of the predicted bidders should be equal to the distribution of the bidder frequencies. (Suggested in <https://static.googleusercontent.com/media/research.google.com/en/pubs/archive/43146.pdf>.)
          
          Effect size:
          - Effect size measures substantive significance.
          - To evaluate effect size, should we use Mann-Whitney's test? <http://yatani.jp/teaching/doku.php?id=hcistats:mannwhitney#effect_size>
          - Compare effect sizes relative to the baseline?
          -->
          <p>We used offline evaluation to filter matchmaking methods and configurations to find the most promising ones. Since we test different matchmakers in the same context, this evaluation can be considered a trade-off analysis <span class="citation" data-cites="Wieringa2014">(Wieringa <a href="#ref-Wieringa2014">2014</a>, p. 260)</span>, in which we balance the differences in the evaluated measures.</p>
          <section id="ground-truth" class="level2">
          <h2><span class="header-section-number">4.1</span> Ground truth</h2>
          <p>We conducted offline evaluation using retrospective data about awarded public contracts. As we described previously in the sec. <a href="#sec:ground-truth">3.1</a>, the ground truth poses several challenges and limitations that the matchmakers have to deal with. Matchmaking was tested on the task of predicting the awarded bidder. In our case, we treat contract awards as explicit positive user feedback. Thus, in terms of <span class="citation" data-cites="Beel2013">(Beel et al. <a href="#ref-Beel2013">2013</a>)</span>, we use a “user-offline-dataset”, since it contains implicit ratings inferred from contract awards.</p>
          <!-- Adjustments of the ground truth -->
          <p>Due to the design of the chosen evaluation task, we had to adjust our ground truth data. Since we evaluate matchmaking as a prediction of the awarded bidders, we need each public contract to have a single winner. However, that is not the case for around 1 % of public contracts in our dataset. This may be either in error or when members of the winning groups of bidders are listed separately. For example, framework agreements may be awarded to multiple bidders. For the sake of simplicity we decided to exclude these contracts from our ground truth.</p>
          </section>
          <section id="sec:evaluation-protocol" class="level2">
          <h2><span class="header-section-number">4.2</span> Evaluation protocol</h2>
          <!-- N-fold cross-validation -->
          <p>Our evaluation protocol is based on n-fold cross-validation. We split the evaluation data into training and testing dataset. The testing dataset contains the withheld contract awards that a matchmaker attempts to predict. We used 5-fold cross-validation, so that we divided the evaluation data into five non-overlapping folds, each of which was used as a testing dataset, while the remaining folds were used for training the evaluated matchmakers. In this way we tested prediction of each contract award in the ground truth.</p>
          <!-- Time series cross-validation -->
          <p>When evaluating matchmakers that take time into account, we split the ground truth so that the training data precedes the testing data. First, we sort the ground truth by contract award date in ascending order. When the award data is unknown, we use the median award date. The sorted ground truth is then split in 5 folds. The second or later folds are consecutively used as testing data, while all the previous folds constitute training data. The first fold is therefore never used for testing, so we test only 4 folds. In this way we avoid training on data from the future relative to the tested data.</p>
          <!--
          Should we split by time? For example, use 8 years (2006-2014) as training and 2 years (2015-2016) for testing?
          Should we add an explanation why we did not split folds by time?
          -->
          </section>
          <section id="sec:evaluated-metrics" class="level2">
          <h2><span class="header-section-number">4.3</span> Evaluated metrics</h2>
          <!-- Metrics and objectives -->
          <p>The objectives we focus on in offline evaluation are accuracy and diversity of the matchmaking results. The adopted evaluation metrics thus go beyond those that reflect accuracy. We aim to maximize the metrics of accuracy. In case of the non-accuracy metrics we strive to increase them without degrading the accuracy.</p>
          <!-- Evaluation of performance?
          Perhaps a rough overall assessment can suffice. E.g., both the SPARQL-based and Elasticsearch-based matchmakers deliver real-time performance, while the RESCAL-based one has to be used in batch mode.
          Performance ~ efficiency in time and space (e.g., speed and RAM consumption)
          Mention restrictions by the computational cost of an evaluation protocol? E.g., not using a learning to rank algorithm?
          -->
          <p>We define the evaluation metrics using the following notation. Let <span class="math inline">\(C\)</span> be the set of public contracts and <span class="math inline">\(B\)</span> the set of bidders who were awarded at least one contract. The function <span class="math inline">\(\mathit{match10}_{m}\colon C \to \mathbb{P}(B)\)</span>, where <span class="math inline">\(\mathbb{P}(B)\)</span> is the powerset of <span class="math inline">\(B\)</span>, returns an ordered set of 10 best-matching bidders recommended for a given public contract by matchmaker <span class="math inline">\(m\)</span>. We considered only the first 10 results due to the primacy effect, which describes that the items at the beginning of a recommendation list are analyzed more frequently.<a href="#fn92" class="footnoteRef" id="fnref92"><sup>92</sup></a> The function <span class="math inline">\(winner\colon C \to B\)</span> returns the winning bidder to whom a contract was awarded. The function <span class="math inline">\(wrank\colon C \to \mathbb{N}_{&gt; 0} \cup \{ \text{nil} \}\)</span> gives the rank of the bidder who won a given public contract.</p>
          <p><span class="math display">\[wrank(c) =
            \small
            \begin{cases}
              n \colon winner(c)\, \textrm{is in position}\, n\, \textrm{in}\, \mathit{match10}_{m}(c)
              &amp; \text{if}\ winner(c) \in \mathit{match10}_{m}(c) \\
              \quad\textrm{nil} &amp; \textrm{otherwise} \\
            \end{cases}
            \normalsize\]</span></p>
          <p>The function <span class="math inline">\(awards\colon B \to \mathbb{N}\)</span> returns the number of contracts awarded to a given bidder.</p>
          <p><span class="math display">\[awards(b) = \left\vert{c \in C : winner(c) = b}\right\vert\]</span></p>
          <p>We measured accuracy using hit rate at 10 (HR@10) and mean reciprocal rank at 10 (MRR@10). HR@10 <span class="citation" data-cites="Deshpande2004">(Deshpande and Karypis <a href="#ref-Deshpande2004">2004</a>, p. 159)</span> is the share of queries for which hits are found in the top 10 results. We consider hits to be the results that include the awarded bidder. We adopted HR@10 as the primary metric that we aim to increase. This metric can be calculated for the matchmaker <span class="math inline">\(m\)</span> as follows:</p>
          <p><span class="math display">\[HR@10 = \frac{\left\vert{c \in C : winner(c) \in \mathit{match10}_{m}(c)}\right\vert}{\left\vert{C}\right\vert}\]</span> <!-- _b --></p>
          <p>MRR@10 <span class="citation" data-cites="Craswell2009">(Craswell <a href="#ref-Craswell2009">2009</a>)</span> is the arithmetic mean of multiplicative inverse ranks. Multiplicative inverse rank <span class="math inline">\(mir\colon C \to \mathbb{Q}_{&gt; 0}\)</span> can be defined as such:</p>
          <p><span class="math display">\[mir(c)=\begin{cases}
                   \frac{1}{wrank(c)} &amp; \text{if}\ winner(c) \in \mathit{match10}_{m}(c) \\
                   0 &amp; \text{nil}
                 \end{cases}\]</span></p>
          <p>This metric is used for evaluating systems where <em>“the user wishes to see one relevant document”</em> <span class="citation" data-cites="Craswell2009">(Craswell <a href="#ref-Craswell2009">2009</a>)</span> and it is <em>“equivalent to Mean Average Precision in cases where each query has precisely one relevant document”</em> <span class="citation" data-cites="Craswell2009">(Craswell <a href="#ref-Craswell2009">2009</a>)</span>. This makes it suitable for our evaluation setup, since for each query (i.e. a contract) we know only one true positive (i.e. the awarded bidder). MRR@10 reflects how prominent the position of the hit is in the matchmaking results. We aim to increase MRR@10, corresponding to a lower rank the hit has. MRR@10 for the matchmaker <span class="math inline">\(m\)</span> can be defined as follows:</p>
          <p><span class="math display">\[MRR@10 = \frac{1}{\left\vert{C}\right\vert}\sum_{c \in C} mir(c)\]</span> <!-- _b --></p>
          <p>The adopted metrics that go beyond accuracy include prediction coverage (PC), catalog coverage at 10 (CC@10), and long-tail percentage at 10 (LTP@10). PC <span class="citation" data-cites="Herlocker2004">(Herlocker et al. <a href="#ref-Herlocker2004">2004</a>, p. 40)</span> measures the amount of items for which the evaluated system is able to produce recommendations. We strive to increase PC to achieve a near-complete coverage. PC for the matchmaker <span class="math inline">\(m\)</span> is defined as the share of queries for which non-empty results are returned.</p>
          <p><span class="math display">\[PC = \frac{\left\vert{c \in C : \mathit{match10}_{m}(c) \neq \varnothing}\right\vert}{\left\vert{C}\right\vert}\]</span> <!-- _b --></p>
          <p>CC@10 <span class="citation" data-cites="Ge2010">(Ge et al. <a href="#ref-Ge2010">2010</a>, p. 258)</span> reflects diversity of the recommended items. Systems that recommend a limited set of items have a low catalog coverage, while systems that recommend diverse items achieve a higher catalog coverage. We compute CC@10 for the matchmaker <span class="math inline">\(m\)</span> as the number of distinct bidders in the top 10 results for all contracts divided by the number of all bidders.</p>
          <p><span class="math display">\[CC@10 = \frac{\left\vert{\bigcup_{c \in C} \mathit{match10}_{m}(c)}\right\vert}{\left\vert{B}\right\vert}\]</span> <!-- _b --></p>
          <p>LTP@10 <span class="citation" data-cites="Adomavicius2012">(Adomavicius and Kwon <a href="#ref-Adomavicius2012">2012</a>)</span> is a metric of novelty, which is based on the distribution of the recommended items. Concretely, it measures the share of items from the long tail in the matchmaking results. If we sort bidders in descending order by the number of contracts awarded to them, the first bidders that account for 20 % of contract awards form the <em>short head</em> and the remaining ones constitute the <em>long tail</em>. In case of the Czech public procurement data, 20 % of the awarded contracts concentrates among the 101 most popular bidders from the total of 14388 bidders in the dataset. To avoid awarding contracts only to a few highly successful bidders, we aim to increase the proportion of recommendations from the long tail of bidders. This is especially important for evaluation of the case-based matchmakers, which tend to favour the most popular bidders. Let <span class="math inline">\((b_{1}, \dots, b_{n})\)</span> be a list of all bidders <span class="math inline">\(b_{i} \in B\)</span>, so that <span class="math inline">\((i \prec j) \implies awards(b_{i}) \geq awards(b_{j})\)</span>, so that the bidders are sorted in descending order by the number of contracts awarded to them. <!-- _b --> The short head <span class="math inline">\(SH\)</span> of this ordered list can be then defined as:</p>
          <p><span class="math display">\[SH = (b_{1},\dots,b_{e});\quad \textrm{so that}\, e : \sum_{k = 1}^{e - 1} awards(b_{k}) &lt; \frac{\left\vert{C}\right\vert}{5} \leq \sum_{l = 1}^{e} awards(b_{l})\]</span> <!-- _b --></p>
          <p>The formula defines <span class="math inline">\(SH\)</span> as delimited by the index <span class="math inline">\(e\)</span> of the bidder with the awards of whom the short head accumulates 20 % of all awarded contracts (i.e. <span class="math inline">\(\frac{\left\vert{C}\right\vert}{5}\)</span>). Long tail <span class="math inline">\(LT\)</span> is the complement of the short head (<span class="math inline">\(LT = B \setminus SH\)</span>). We then calculate LTP@10 for the matchmaker <span class="math inline">\(m\)</span> as follows:</p>
          <p><span class="math display">\[LTP@10 = \frac{\sum_{c \in C} \left\vert{\mathit{match10}_{m}(c) \cap LT}\right\vert}{\sum_{c \in C} \left\vert{\mathit{match10}_{m}(c)}\right\vert}\]</span></p>
          <!--
          We can also evaluate novelty in terms of time.
          One way to assess novelty could be the average age of the recommended bidders.
          We can compute the bidder's age from its establishment date.
          The mean average age of the top 10 most popular bidders is 21.7 years.
          The mean average age of all bidders is 17.5 years (median 19 years).
          This is probably not such a large difference, since the maximum age is only
          -->
          <!-- Unused evaluation metrics -->
          <p>Due to our evaluation setup we avoided some of the usual metrics from information retrieval in general and from recommender systems in particular. Both precision and recall have limited prediction power in our case, since only one true positive is known. If we consider top 10 results only, precision would be either 1/10 or 0, while recall would either be 1 or 0. This problem is known as class imbalance <span class="citation" data-cites="Christen2012">(Christen <a href="#ref-Christen2012">2012</a>)</span>. Results with the status of non-match are much more prevalent in matchmaking than those with the status of match, which skews the evaluation measures that take non-matches into account.</p>
          <!-- Evaluation of statistical significance -->
          <!--
          We used Wilcoxon signed-rank test [@Rey2014] to evaluate the statistical significance of differences between the distributions of ranks produced by the evaluated matchmakers.
          We chose it because we compare ranks for the whole dataset and this test is suited for paired samples from the same population.
          Moreover, it does not require the compared samples to follow normal distribution, which is the case of the distributions of ranks.
          -->
          <p>The rest of this chapter features the results obtained from SPARQL-based and RESCAL-based matchmakers in the evaluation. All reported evaluation results are rounded to three decimal places. The best results for each metric in each table are highlighted by using a bold font.</p>
          <!--
          ### Out-takes:
          
          *"Offline evaluations use pre-compiled offline datasets from which some information has been removed. Subsequently, the recommender algorithms are analyzed on their ability to recommend the missing information"* [~Beel2013, p. 8].
          
          This is more about online evaluation:
          
          Moreover, the limitations of the chosen ground truth have to be considered with respect to the internal validity of the proposed design of the offline evaluation.
          Internal validity in the context of recommender systems can be defined as the *"extent to which the effects observed are due to the controlled test conditions (e.g., the varying of a recommendation algorithm's parameters) instead of differences in the set of participants (predispositions) or uncontrolled/unknown external effects"* [@Jannach2010, p. 168].
          
          *"External validity refers to the extent to which results are generalizable to other user groups or situations"*  [@Jannach2010, p. 168]
          
          Unused evaluation metrics:
          
          http://videolectures.net/eswc2014_di_noia_linked/?q=di%20noia
          The task 2 of the challenge used F1-measure @ top 5.
          The evaluation of task 3 on diversity is evaluated using intra-list diversity (ILD) with only dcterms:subject and dbo:author. We can also restrict the ILD to few properties (or property paths).
          
          User coverage: a share of bidders for which the system is able of recommending contracts.
          -->
          </section>
          <section id="results-of-sparql-based-matchmakers" class="level2">
          <h2><span class="header-section-number">4.4</span> Results of SPARQL-based matchmakers</h2>
          <p>We chose SPARQL-based matchmaking via the <code>pc:mainObject</code> property without weighting as our baseline. The developed matchmakers and configurations were assessed by comparing their evaluation results with the results obtained for the baseline configuration. <!-- "hill climbing" --> In this way, we assessed the progress beyond the baseline that various matchmaking factors were able to achieve. We tested several factors involved in the matchmakers. These factors included weighting, query expansion, aggregation functions, and data reduction.</p>
          <section id="blind-matchmakers" class="level3">
          <h3><span class="header-section-number">4.4.1</span> Blind matchmakers</h3>
          <p>As a starting point, we evaluated the blind matchmakers described in the sec. <a href="#sec:blind-matchmakers">3.2.3</a>. The results of their evaluation are summarized in the tbl. <a href="#tbl:blind-matchmakers">2</a>. Since these matchmakers ignore the query contract, they are able to produce matches for any contract, and thus score the maximum PC. They cover the extremes of the diversity spectrum. On the one hand, the random matchmaker can recommend practically any bidder, most of whom come from the long tail. On the other hand, recommending the top winning bidders yields the lowest possible catalog coverage, the intersection of which with the long tail is empty by the definition of this matchmaker. Since 7 % of contracts is awarded to the top 10 most winning bidders, recommending them produces the same HR@10. Recommending the bidders that score the highest page rank is not as successful as simply recommending the top winning bidders, achieving an HR@10 of 0.03.</p>
          <div id="tbl:blind-matchmakers">
          <table>
          <caption>Table 2: Evaluation of blind matchmakers</caption>
          <thead>
          <tr class="header">
          <th style="text-align: left;">Matchmaker</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td style="text-align: left;">Random</td>
          <td style="text-align: right;">0.001</td>
          <td style="text-align: right;">0</td>
          <td style="text-align: right;"><strong>1</strong></td>
          <td style="text-align: right;"><strong>1</strong></td>
          <td style="text-align: right;"><strong>0.992</strong></td>
          </tr>
          <tr class="even">
          <td style="text-align: left;">Top winning bidders</td>
          <td style="text-align: right;"><strong>0.07</strong></td>
          <td style="text-align: right;"><strong>0.03</strong></td>
          <td style="text-align: right;">0.001</td>
          <td style="text-align: right;"><strong>1</strong></td>
          <td style="text-align: right;">0</td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;">Top page rank bidders</td>
          <td style="text-align: right;">0.03</td>
          <td style="text-align: right;">0.007</td>
          <td style="text-align: right;">0.001</td>
          <td style="text-align: right;"><strong>1</strong></td>
          <td style="text-align: right;">0.80</td>
          </tr>
          </tbody>
          </table>
          </div>
          <!-- There are also papers that consider multiple baselines, such as [@Garcin2014]. -->
          </section>
          <section id="aggregation-functions" class="level3">
          <h3><span class="header-section-number">4.4.2</span> Aggregation functions</h3>
          <p>We evaluated the aggregation functions from the sec. <a href="#sec:aggregation-functions">3.2.2.5</a>. In each case, we used the t-norm and t-conorm from the same family, e.g., the Gödel’s t-norm was used with the Gödel t-conorm. The functions were applied to matchmaking via the <code>pc:mainObject</code> property with the weight of 0.6. This weight was chosen in order to allow the differences between the functions to manifest. For instance, if we used the weight of 1, Łukasiewicz’s aggregation would not distinguish between bidders who won one matching contract and those who won more. The results of this comparison are shown in the tbl. <a href="#tbl:norms-conorms">3</a>. Product aggregation clearly outperforms the other functions in terms of accuracy. Both Gödel’s and Łukasiewicz’s aggregation functions do not learn sufficiently from the extent of matched data. Similar findings were obtained in our previous work in <span class="citation" data-cites="Mynarz2015">Mynarz et al. (<a href="#ref-Mynarz2015">2015</a>)</span>. This outcome led us to use the product aggregation in all other matchmakers we evaluated.</p>
          <div id="tbl:norms-conorms">
          <table>
          <caption>Table 3: Evaluation t-norms and t-conorms</caption>
          <thead>
          <tr class="header">
          <th>Aggregation functions</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td>Gödel</td>
          <td style="text-align: right;">0.18</td>
          <td style="text-align: right;">0.07</td>
          <td style="text-align: right;"><strong>0.602</strong></td>
          <td style="text-align: right;"><strong>0.978</strong></td>
          <td style="text-align: right;">0.828</td>
          </tr>
          <tr class="even">
          <td>Product</td>
          <td style="text-align: right;"><strong>0.248</strong></td>
          <td style="text-align: right;"><strong>0.124</strong></td>
          <td style="text-align: right;">0.567</td>
          <td style="text-align: right;"><strong>0.978</strong></td>
          <td style="text-align: right;">0.684</td>
          </tr>
          <tr class="odd">
          <td>Łukasiewicz</td>
          <td style="text-align: right;">0.159</td>
          <td style="text-align: right;">0.068</td>
          <td style="text-align: right;">0.582</td>
          <td style="text-align: right;"><strong>0.978</strong></td>
          <td style="text-align: right;"><strong>0.858</strong></td>
          </tr>
          </tbody>
          </table>
          </div>
          </section>
          <section id="individual-features" class="level3">
          <h3><span class="header-section-number">4.4.3</span> Individual features</h3>
          <p>As we described in the sec. <a href="#sec:contract-objects">3.2.2.1</a>, we used several properties that describe contract objects. We evaluated these properties separately, without weighting, to determine their predictive power. Evaluation results of the matchmakers based on the four considered properties are given in the tbl. <a href="#tbl:properties-evaluation">4</a>. The best-performing property is the <code>pc:mainObject</code>. As the fig. <a href="#fig:cumulative-hr">13</a> illustrates, its HR@k grows logarithmically with <span class="math inline">\(k\)</span>, starting at 7 % chance of finding the contact’s winner as the first hit. We chose this property as our baseline that we tried to improve further on. The other properties achieved worse results. While the <code>pc:additionalObject</code> covers the long tail better, its prediction coverage is low because it is able to produce matches only for the few contracts that are described with this property. The <code>pc:kind</code> fails in diversity metrics, covering only a minute fraction of the bidders. Since there are only few distinct kinds of contracts in our dataset, this property is unable to sufficiently distinguish the bidders and thus concentrates only on recommending the most popular ones. The weak performance of the <code>isvz:serviceCategory</code> may be attributed to its limit to contracts for services.</p>
          <div id="tbl:properties-evaluation">
          <table>
          <caption>Table 4: Evaluation of individual properties</caption>
          <thead>
          <tr class="header">
          <th style="text-align: left;">Property</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td style="text-align: left;"><code>pc:mainObject</code></td>
          <td style="text-align: right;"><strong>0.248</strong></td>
          <td style="text-align: right;"><strong>0.124</strong></td>
          <td style="text-align: right;"><strong>0.567</strong></td>
          <td style="text-align: right;">0.978</td>
          <td style="text-align: right;">0.684</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;"><code>pc:additionalObject</code></td>
          <td style="text-align: right;">0.073</td>
          <td style="text-align: right;">0.035</td>
          <td style="text-align: right;">0.384</td>
          <td style="text-align: right;">0.359</td>
          <td style="text-align: right;"><strong>0.69</strong></td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;"><code>pc:kind</code></td>
          <td style="text-align: right;">0.103</td>
          <td style="text-align: right;">0.043</td>
          <td style="text-align: right;">0.003</td>
          <td style="text-align: right;"><strong>0.993</strong></td>
          <td style="text-align: right;">0</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;"><code>isvz:serviceCategory</code></td>
          <td style="text-align: right;">0.094</td>
          <td style="text-align: right;">0.042</td>
          <td style="text-align: right;">0.036</td>
          <td style="text-align: right;">0.797</td>
          <td style="text-align: right;">0.282</td>
          </tr>
          </tbody>
          </table>
          </div>
          <figure>
          <img src="resources/img/evaluation/cumulative_hr.png" alt="Figure 13: HR@k for pc:mainObject" id="fig:cumulative-hr" style="width:75.0%" /><figcaption>Figure 13: HR@k for <code>pc:mainObject</code></figcaption>
          </figure>
          </section>
          <section id="combined-features" class="level3">
          <h3><span class="header-section-number">4.4.4</span> Combined features</h3>
          <p>Having evaluated the properties individually we examined whether their combinations could perform better. We combined the properties with the baseline property <code>pc:mainObject</code>, using a reduced weight of 0.1 for the added properties. Besides the properties evaluated above, we also experimented with including the qualifiers of CPV concepts described in the sec. <a href="#sec:cpv">2.4.6.1</a>. The evaluation results of the matchmakers based on the combinations of properties are presented in the tbl. <a href="#tbl:combined-properties">5</a>. None of the properties produced a synergistic effect with <code>pc:mainObject</code>. If there was an improvement, it was not practically meaningful. We also experimented with a larger range of weights for the combination with <code>pc:additionalProperty</code>, however, none of the weights led to a significant difference in the evaluation results. Our conclusion is in line with Maidel et al., who found in similar circumstances that <em>“the inclusion of item concept weights does not improve the performance of the algorithm”</em> <span class="citation" data-cites="Maidel2008">(<a href="#ref-Maidel2008">2008</a>, p. 97)</span>.</p>
          <div id="tbl:combined-properties">
          <table style="width:100%;">
          <caption>Table 5: Evaluation of combined properties</caption>
          <colgroup>
          <col style="width: 32%" />
          <col style="width: 13%" />
          <col style="width: 14%" />
          <col style="width: 13%" />
          <col style="width: 13%" />
          <col style="width: 13%" />
          </colgroup>
          <thead>
          <tr class="header">
          <th style="text-align: left;">Property</th>
          <th style="text-align: center;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td style="text-align: left;"><code>pc:additionalObject</code></td>
          <td style="text-align: center;">0.253</td>
          <td style="text-align: right;">0.124</td>
          <td style="text-align: right;"><strong>0.57</strong></td>
          <td style="text-align: right;">0.99</td>
          <td style="text-align: right;">0.645</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;"><code>pc:kind</code></td>
          <td style="text-align: center;">0.162</td>
          <td style="text-align: right;">0.075</td>
          <td style="text-align: right;">0.092</td>
          <td style="text-align: right;">0.996</td>
          <td style="text-align: right;">0.144</td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;"><code>isvz:serviceCategory</code></td>
          <td style="text-align: center;">0.197</td>
          <td style="text-align: right;">0.092</td>
          <td style="text-align: right;">0.392</td>
          <td style="text-align: right;">0.995</td>
          <td style="text-align: right;">0.368</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;">Qualifier</td>
          <td style="text-align: center;">0.249</td>
          <td style="text-align: right;"><strong>0.125</strong></td>
          <td style="text-align: right;">0.568</td>
          <td style="text-align: right;">0.978</td>
          <td style="text-align: right;"><strong>0.685</strong></td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;"><code>pc:additionalObject</code>, qualifiers</td>
          <td style="text-align: center;"><strong>0.254</strong></td>
          <td style="text-align: right;">0.123</td>
          <td style="text-align: right;">0.557</td>
          <td style="text-align: right;">0.99</td>
          <td style="text-align: right;">0.639</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;"><code>pc:additionalObject</code>, <code>pc:kind</code>, <code>isvz:serviceCategory</code></td>
          <td style="text-align: center;">0.154</td>
          <td style="text-align: right;">0.072</td>
          <td style="text-align: right;">0.088</td>
          <td style="text-align: right;"><strong>0.996</strong></td>
          <td style="text-align: right;">0.129</td>
          </tr>
          </tbody>
          </table>
          </div>
          </section>
          <section id="query-expansion" class="level3">
          <h3><span class="header-section-number">4.4.5</span> Query expansion</h3>
          <p>Apart from using combinations of properties, we can also extend the baseline matchmaker via query expansion, as documented in the sec. <a href="#sec:query-expansion">3.2.2.2</a>. We evaluated the expansion to related CPV concepts connected via hierarchical relations, both in the direction to broader concepts, to narrower concepts, or in both directions. The query expansion followed a given maximum number of hops in these directions. Following too many hops to related concepts can introduce noise <span class="citation" data-cites="DiNoia2012a">(Di Noia et al. <a href="#ref-DiNoia2012a">2012</a><a href="#ref-DiNoia2012a">b</a>)</span>, so we weighted the concepts inferred by query expansion either by a fixed inhibition or by a weight derived from their IDF. The results of the experiments with query expansion are gathered in the tbl. <a href="#tbl:query-expansion">6</a>. Expansion to broader concepts was able to improve on the accuracy metrics slightly, although the difference was too small to be meaningful. Overall, we found that introducing query expansion led only to minuscule changes in the performance of the baseline matchmaker.</p>
          <div id="tbl:query-expansion">
          <table>
          <caption>Table 6: Evaluation of matchmakers using query expansion</caption>
          <thead>
          <tr class="header">
          <th>Broader</th>
          <th>Narrower</th>
          <th>Weight</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td>1</td>
          <td>0</td>
          <td>1</td>
          <td style="text-align: right;">0.245</td>
          <td style="text-align: right;">0.119</td>
          <td style="text-align: right;">0.51</td>
          <td style="text-align: right;">0.99</td>
          <td style="text-align: right;">0.67</td>
          </tr>
          <tr class="even">
          <td>1</td>
          <td>0</td>
          <td>0.5</td>
          <td style="text-align: right;">0.252</td>
          <td style="text-align: right;">0.124</td>
          <td style="text-align: right;">0.533</td>
          <td style="text-align: right;">0.99</td>
          <td style="text-align: right;">0.673</td>
          </tr>
          <tr class="odd">
          <td>1</td>
          <td>0</td>
          <td>0.1</td>
          <td style="text-align: right;">0.257</td>
          <td style="text-align: right;">0.127</td>
          <td style="text-align: right;">0.563</td>
          <td style="text-align: right;">0.99</td>
          <td style="text-align: right;">0.682</td>
          </tr>
          <tr class="even">
          <td>2</td>
          <td>0</td>
          <td>0.1</td>
          <td style="text-align: right;">0.258</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;">0.545</td>
          <td style="text-align: right;">0.994</td>
          <td style="text-align: right;">0.672</td>
          </tr>
          <tr class="odd">
          <td>3</td>
          <td>0</td>
          <td>0.1</td>
          <td style="text-align: right;">0.257</td>
          <td style="text-align: right;">0.125</td>
          <td style="text-align: right;">0.517</td>
          <td style="text-align: right;"><strong>0.996</strong></td>
          <td style="text-align: right;">0.65</td>
          </tr>
          <tr class="even">
          <td>1</td>
          <td>0</td>
          <td>IDF</td>
          <td style="text-align: right;">0.249</td>
          <td style="text-align: right;">0.125</td>
          <td style="text-align: right;">0.565</td>
          <td style="text-align: right;">0.978</td>
          <td style="text-align: right;">0.684</td>
          </tr>
          <tr class="odd">
          <td>2</td>
          <td>0</td>
          <td>IDF</td>
          <td style="text-align: right;">0.249</td>
          <td style="text-align: right;">0.125</td>
          <td style="text-align: right;">0.565</td>
          <td style="text-align: right;">0.978</td>
          <td style="text-align: right;">0.684</td>
          </tr>
          <tr class="even">
          <td>3</td>
          <td>0</td>
          <td>IDF</td>
          <td style="text-align: right;">0.249</td>
          <td style="text-align: right;">0.12</td>
          <td style="text-align: right;">0.57</td>
          <td style="text-align: right;">0.98</td>
          <td style="text-align: right;"><strong>0.68</strong></td>
          </tr>
          <tr class="odd">
          <td>0</td>
          <td>1</td>
          <td>1</td>
          <td style="text-align: right;">0.248</td>
          <td style="text-align: right;">0.123</td>
          <td style="text-align: right;">0.527</td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.677</td>
          </tr>
          <tr class="even">
          <td>0</td>
          <td>1</td>
          <td>0.5</td>
          <td style="text-align: right;">0.252</td>
          <td style="text-align: right;">0.125</td>
          <td style="text-align: right;">0.549</td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.677</td>
          </tr>
          <tr class="odd">
          <td>0</td>
          <td>1</td>
          <td>0.1</td>
          <td style="text-align: right;">0.253</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;"><strong>0.569</strong></td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.677</td>
          </tr>
          <tr class="even">
          <td>0</td>
          <td>2</td>
          <td>0.1</td>
          <td style="text-align: right;">0.253</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;">0.565</td>
          <td style="text-align: right;">0.979</td>
          <td style="text-align: right;">0.679</td>
          </tr>
          <tr class="odd">
          <td>0</td>
          <td>3</td>
          <td>0.1</td>
          <td style="text-align: right;">0.254</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;">0.562</td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.677</td>
          </tr>
          <tr class="even">
          <td>0</td>
          <td>1</td>
          <td>IDF</td>
          <td style="text-align: right;">0.253</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;">0.572</td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.684</td>
          </tr>
          <tr class="odd">
          <td>0</td>
          <td>2</td>
          <td>IDF</td>
          <td style="text-align: right;">0.254</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;">0.572</td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.68</td>
          </tr>
          <tr class="even">
          <td>0</td>
          <td>3</td>
          <td>IDF</td>
          <td style="text-align: right;">0.254</td>
          <td style="text-align: right;">0.126</td>
          <td style="text-align: right;">0.569</td>
          <td style="text-align: right;">0.982</td>
          <td style="text-align: right;">0.682</td>
          </tr>
          <tr class="odd">
          <td>1</td>
          <td>1</td>
          <td>0.1</td>
          <td style="text-align: right;"><strong>0.259</strong></td>
          <td style="text-align: right;"><strong>0.128</strong></td>
          <td style="text-align: right;">0.563</td>
          <td style="text-align: right;">0.991</td>
          <td style="text-align: right;">0.678</td>
          </tr>
          <tr class="even">
          <td>1</td>
          <td>1</td>
          <td>IDF</td>
          <td style="text-align: right;">0.249</td>
          <td style="text-align: right;">0.125</td>
          <td style="text-align: right;">0.565</td>
          <td style="text-align: right;">0.978</td>
          <td style="text-align: right;">0.684</td>
          </tr>
          </tbody>
          </table>
          </div>
          </section>
          <section id="data-reduction" class="level3">
          <h3><span class="header-section-number">4.4.6</span> Data reduction</h3>
          <p>We evaluated the impact of data reduction on HR@10 for the baseline matchmaker and the blind matchmaker that constantly recommends the top winning bidders. Prior to running the evaluation we reduced the number of links between contracts and bidders to a given fraction. For example, if the level of data reduction was set to 0.4, 60 % of the links were removed. Links to remove were selected randomly. The fig. <a href="#fig:data-reduction">14</a> shows HR@10 per level of data reduction for the two compared matchmakers. In general, we decreased the data reduction level by 0.1 for each evaluation run, but a smaller step was used for the lower levels to better distinguish the impact of data reduction at smaller data sizes. The evaluation showed that HR@10 grows logarithmically with the size of the data, while the blind matchmaker performs the same no matter the data size. As can be expected, the baseline matchmaker improves its performance as the data it learns from accrues. Both approaches suffer from the cold start problem, although the baseline matchmaker improves rapidly with the initial data growth and demonstrates diminishing returns as data becomes larger.</p>
          <figure>
          <img src="resources/img/evaluation/data_reduction.png" alt="Figure 14: HR@10 per level of data reduction" id="fig:data-reduction" style="width:75.0%" /><figcaption>Figure 14: HR@10 per level of data reduction</figcaption>
          </figure>
          </section>
          <section id="data-refinement" class="level3">
          <h3><span class="header-section-number">4.4.7</span> Data refinement</h3>
          <p>Of the data refinement steps undertaken, as described in the sec. <a href="#sec:transformation">2.3</a>, we evaluated what impact better deduplication and mapping CPV 2003 to CPV 2008 had on the baseline matchmaker. Both steps improved the evaluation results of the matchmaker, as can be seen in the tbl. <a href="#tbl:data-refinement">7</a>. Better deduplication and fusion of bidders reduces the search space of possible matches, so that the probability of finding the correct match increases. Mapping CPV 2003 to CPV 2008 enlarges the dataset the matchmaker can learn from by 15.31 %, accounting for older contracts described by CPV 2003. However, while HR@10 improves after this mapping, CC@10 decreases, which may be explained by more data affirming the few established bidders. Better deduplication improves the accuracy metrics only slightly, which may be due to the original data already being free of most duplicates. Nevertheless, in our prior work <span class="citation" data-cites="Mynarz2015">(Mynarz et al. <a href="#ref-Mynarz2015">2015</a>)</span>, deduplication produced the greatest improvement in the evaluation of the baseline matchmaker.</p>
          <div id="tbl:data-refinement">
          <table>
          <caption>Table 7: Impact of data refinement on the baseline matchmaker</caption>
          <thead>
          <tr class="header">
          <th style="text-align: left;">Dataset</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td style="text-align: left;">Prior to CPV 2003 mapping</td>
          <td style="text-align: right;">0.237</td>
          <td style="text-align: right;">0.12</td>
          <td style="text-align: right;"><strong>0.595</strong></td>
          <td style="text-align: right;">0.931</td>
          <td style="text-align: right;">0.798</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;">Prior to better deduplication</td>
          <td style="text-align: right;">0.245</td>
          <td style="text-align: right;">0.121</td>
          <td style="text-align: right;">0.554</td>
          <td style="text-align: right;">0.955</td>
          <td style="text-align: right;"><strong>0.858</strong></td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;">Final</td>
          <td style="text-align: right;"><strong>0.248</strong></td>
          <td style="text-align: right;"><strong>0.124</strong></td>
          <td style="text-align: right;">0.567</td>
          <td style="text-align: right;"><strong>0.978</strong></td>
          <td style="text-align: right;">0.684</td>
          </tr>
          </tbody>
          </table>
          </div>
          </section>
          <section id="counter-measures-to-limits-of-ground-truth" class="level3">
          <h3><span class="header-section-number">4.4.8</span> Counter-measures to limits of ground truth</h3>
          <p>We evaluated two approaches devised as counter-measures to address the limits of our ground truth. One of them weighted contract awards by the zIndex fairness score of the contracting authority, the other limited the training dataset to contracts awarded in open procedures. The proposed counter-measures were not successful. Both approaches fared worse than our baseline, as documented in the tbl. <a href="#tbl:counter-measures-evaluation">8</a>. While the impact of weighting by zIndex is barely noticeable, the restriction to open procedures decreased most of the observed metrics. The decrease may be attributed to the smaller size of training data, even though the majority of contracts in our dataset were awarded via an open procedure.</p>
          <div id="tbl:counter-measures-evaluation">
          <table>
          <caption>Table 8: Evaluation of counter-measures to limits of the ground truth</caption>
          <thead>
          <tr class="header">
          <th style="text-align: left;">Matchmaker</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">PC</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td style="text-align: left;"><code>pc:mainObject</code></td>
          <td style="text-align: right;"><strong>0.248</strong></td>
          <td style="text-align: right;"><strong>0.124</strong></td>
          <td style="text-align: right;"><strong>0.567</strong></td>
          <td style="text-align: right;"><strong>0.978</strong></td>
          <td style="text-align: right;">0.684</td>
          </tr>
          <tr class="even">
          <td style="text-align: left;"><code>pc:mainObject</code>, zIndex</td>
          <td style="text-align: right;">0.243</td>
          <td style="text-align: right;">0.121</td>
          <td style="text-align: right;">0.566</td>
          <td style="text-align: right;"><strong>0.978</strong></td>
          <td style="text-align: right;">0.687</td>
          </tr>
          <tr class="odd">
          <td style="text-align: left;"><code>pc:mainObject</code>, open procedures</td>
          <td style="text-align: right;">0.214</td>
          <td style="text-align: right;">0.106</td>
          <td style="text-align: right;">0.469</td>
          <td style="text-align: right;">0.964</td>
          <td style="text-align: right;"><strong>0.702</strong></td>
          </tr>
          </tbody>
          </table>
          </div>
          <p>In conclusion, rather than improving on the baseline matchmaker, we managed to analyze what makes it perform well. It benefits mostly from refining and extending the training data and from using the product aggregation function. Other extensions of the baseline matchmaker were found to have no practical benefits. Simply put, the evaluation indicated that <em>“simple models and a lot of data trump more elaborate models based on less data”</em> <span class="citation" data-cites="Halevy2009">(Halevy et al. <a href="#ref-Halevy2009">2009</a>, p. 9)</span>.</p>
          </section>
          </section>
          <section id="results-of-rescal-based-matchmakers" class="level2">
          <h2><span class="header-section-number">4.5</span> Results of RESCAL-based matchmakers</h2>
          <p>The approach for exploring the space of configurations of RESCAL-based matchmakers was similar to the one used for SPARQL-based matchmakers. We started with <code>pc:mainObject</code> as the principal feature and examined what improvements can be achieved via adjustments of hyper-parameters, combinations with additional features, or other treatments. The adopted heuristic for tuning the matchmakers’ performance can be considered a manually guided grid search. Note that RESCAL exhibits a greater degree of non-determinism than the SPARQL-based method, so that its evaluation results have greater variance.</p>
          <p>We measured the same evaluation metrics for the RESCAL-based matchmakers as for the SPARQL-based ones. Since we do not use any threshold for the RESCAL-based matchmakers, their prediction coverage is always maximum. Consequently, for brevity, we omit this metric from the evaluation results.</p>
          <section id="hyper-parameters" class="level3">
          <h3><span class="header-section-number">4.5.1</span> Hyper-parameters</h3>
          <p>The central hyper-parameter of RESCAL is the rank of its decomposition. As reported in existing research, RESCAL’s accuracy improves with increasing rank of the factorization. With higher ranks we observe diminishing returns and, eventually, HR@10 ceases to improve at around rank 500, as the fig. <a href="#fig:hrs-per-rank">15</a> displays. An analogous impact can be observed for CC@10, although its growth is much more subtle. We tested ranks ranging from 10 to 1000, using smaller intervals for greater resolution in low ranks. Runtime of tensor factorization with RESCAL increases approximately linearly with the rank, so there is a need to balance the quality of RESCAL’s results with the available time to compute them.</p>
          <figure>
          <img src="resources/img/evaluation/hrs_per_rank.png" alt="Figure 15: HR@10 and CC@10 per rank" id="fig:hrs-per-rank" /><figcaption>Figure 15: HR@10 and CC@10 per rank</figcaption>
          </figure>
          <p>We observed that performance improves with rank only for more selective properties, e.g., <code>pc:mainObject</code>. Properties that have fewer distinct values, such as <code>pc:kind</code>, reach their peak performance already at lower ranks.</p>
          <p>RESCAL allows to tune its generalization ability via the regularization parameters <span class="math inline">\(\lambda_{A}\)</span> for the latent factor matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(\lambda_{R}\)</span> for the tensor <span class="math inline">\(R\)</span> that captures the interactions of the latent components. Increasing the amount of regularization helps avoid overfitting. Optimal values of the regularization parameters are dataset-specific. While <span class="citation" data-cites="Padia2016">Padia et al. (<a href="#ref-Padia2016">2016</a>)</span> achieved the best results with <span class="math inline">\(\lambda_{A} = 10\)</span> and <span class="math inline">\(\lambda_{R} = 0.2\)</span>, <span class="citation" data-cites="Kuchar2016">Kuchař et al. (<a href="#ref-Kuchar2016">2016</a>)</span> obtained the peak performance by setting both to 0.01. In our case, we found that relatively high values of the regularization parameters tend to achieve the best results. We set both <span class="math inline">\(\lambda_{A}\)</span> and <span class="math inline">\(\lambda_{R}\)</span> to be 10. A comparison of a few selected values of the regularization parameters is shown in the tbl. <a href="#tbl:regularization-parameters">9</a> for <code>pc:mainObject</code> at rank 50.</p>
          <div id="tbl:regularization-parameters">
          <table>
          <caption>Table 9: Evaluation of regularization parameters</caption>
          <thead>
          <tr class="header">
          <th><span class="math inline">\(\lambda_{A}\)</span></th>
          <th><span class="math inline">\(\lambda_{R}\)</span></th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td>0</td>
          <td>0</td>
          <td style="text-align: right;">0.049</td>
          <td style="text-align: right;">0.024</td>
          <td style="text-align: right;"><strong>0.016</strong></td>
          <td style="text-align: right;"><strong>0.493</strong></td>
          </tr>
          <tr class="even">
          <td>0.01</td>
          <td>0.01</td>
          <td style="text-align: right;">0.066</td>
          <td style="text-align: right;">0.028</td>
          <td style="text-align: right;">0.01</td>
          <td style="text-align: right;">0.272</td>
          </tr>
          <tr class="odd">
          <td>10</td>
          <td>0.2</td>
          <td style="text-align: right;">0.077</td>
          <td style="text-align: right;">0.032</td>
          <td style="text-align: right;">0.006</td>
          <td style="text-align: right;">0.163</td>
          </tr>
          <tr class="even">
          <td>10</td>
          <td>10</td>
          <td style="text-align: right;"><strong>0.081</strong></td>
          <td style="text-align: right;"><strong>0.032</strong></td>
          <td style="text-align: right;">0.006</td>
          <td style="text-align: right;">0.049</td>
          </tr>
          <tr class="odd">
          <td>20</td>
          <td>20</td>
          <td style="text-align: right;">0.081</td>
          <td style="text-align: right;">0.032</td>
          <td style="text-align: right;">0.006</td>
          <td style="text-align: right;">0.042</td>
          </tr>
          </tbody>
          </table>
          </div>
          <p>The remaining hyper-parameters exposed by RESCAL include initialization methods and convergence criteria. RESCAL can initialize the latent matrices either randomly or by eigenvalues of the input tensor, the latter method being clearly superior, as shown in the tbl. <a href="#tbl:initialization-methods">10</a> for <code>pc:mainObject</code> at rank 50. RESCAL stops when it reaches the given convergence criteria, which can be specified either as the maximum number of iterations or as the maximum residual. We used the default values for these hyper-parameters.</p>
          <div id="tbl:initialization-methods">
          <table>
          <caption>Table 10: Evaluation of initialization methods</caption>
          <thead>
          <tr class="header">
          <th>Initialization method</th>
          <th style="text-align: right;">HR@10</th>
          <th style="text-align: right;">MRR@10</th>
          <th style="text-align: right;">CC@10</th>
          <th style="text-align: right;">LTP@10</th>
          </tr>
          </thead>
          <tbody>
          <tr class="odd">
          <td>Random</td>
          <td style="text-align: right;">0.002</td>
          <td style="text-align: right;">0.001</td>
          <td style="text-align: right;">0.003</td>
          <td style="text-align: right;"><strong>1</strong></td>
          </tr>
          <tr class="even">
          <td>Eigenvalues</td>
          <td style="text-align: right;"><strong>0.081</strong></td>
          <td style="text-align: right;"><strong>0.032</strong></td>
          <td style="text-align: right;"><strong>0.006</strong></td>
          <td style="text-align: right;">0.049</td>
          </tr>
          </tbody>
          </table>
          </div>
          </section>
          <section id="feature-selection" class="level3">
          <h3><span class="header-section-number">4.5.2</span> Feature selection</h3>
          <!-- Individual features -->
          <p>We evaluated the predictive power of descriptive features that can be obtained from our dataset. We started by assessing the results of the individual features. We combined each feature with the ground truth comprising contract awards and observed how well it can help predicting the awarded bidders.</p>
          <!-- Link the improvement gained by increasing rank to higher selectivity of the evaluated properties? -->
          <!-- `pc:mainObject` + additional features -->
          <p>As in evaluation of the SPARQL-based matchmakers, we adopted <code>pc:mainObject</code> as our pivot feature that we combined with additional features. Our next step after evaluating the features separately was thus to see how they perform in combination with <code>pc:mainObject</code>.</p>
          <!-- Larger combinations of features -->
          <p>Ultimately, we considered using larger sets of features including those that improved the results of <code>pc:mainObject</code> when combined with it.</p>
          <!--
          `pc:mainObject`
          `pc:additionalObject`
          `pc:mainObject` + `skos:broaderTransitive` (approximating query expansion)
          `pc:kind`
          `isvz:serviceCategory`
          `rov:orgActivity`
          `rov:orgActivity` + `skos:broaderTransitive`
          -->
          </section>
          <section id="ageing" class="level3">
          <h3><span class="header-section-number">4.5.3</span> Ageing</h3>
          <p>Evaluation of ageing was done by time series cross-validation, as described in the sec. <a href="#sec:evaluation-protocol">4.2</a>.</p>
          <p>Ageing was applied to the tensor slice containing links between public contracts and awarded bidders, as described in the sec. <a href="#sec:loading-rescal">2.6.2</a>.</p>
          <!--
          Compare `pc:mainObject` normal and aged, in both cases using time series cross-validation, at ranks 50 and 500.
          The main difference is in the mode of cross-validation.
          Time series cross-validation achieves much lower results than n-fold cross-validation even when no ageing is used.
          This can be explained in part by lower volume of training data.
          However, it may hint a bug in the evaluation protocol.
          -->
          </section>
          <section id="literals" class="level3">
          <h3><span class="header-section-number">4.5.4</span> Literals</h3>
          <!--
          Actual prices (i.e. `pc:actualPrice`) are known for 91.5 % of contracts in the evaluated dataset.
          -->
          <p>If there is no improvement or a decrease in performance, it might be explainable by noisy data about prices. Prices may be reported as coefficients to be multiplied by an implicit factor that is not part of the structured data.</p>
          <!-- Summary -->
          <!--
          TODO: Add a comparison of the best-performing configurations of SPARQL-based and RESCAL-based matchmakers.
          -->
          </section>
          </section>
          <section id="comparison-of-the-results" class="level2">
          <h2><span class="header-section-number">4.6</span> Comparison of the results</h2>
          <!-- Compare the results of SPARQL-based and RESCAL-based matchmakers. -->
          <!--
          Overall, the RESCAL-based matchmakers produce results with very low diversity, especially when considering their CC@10.
          The low diversity means that they tend to recommend the same bidders repeatedly.
          -->
          </section>
          </section>
          <section id="sec:conclusions" class="level1">
          <h1><span class="header-section-number">5</span> Conclusions</h1>
          <!--
          See p. 3, <http://fis.vse.cz/wp-content/uploads/2014/02/Standardy_zpracovani_doktorskych_praci.pdf>
          -->
          <!-- Summary of the main contributions -->
          <p>We developed and documented methods to match public contracts to bidders. These methods leverage linked open data that describes the entities involved in matchmaking as being a part of a semantically described knowledge graph, which includes descriptions of the entities, as well as their interactions, relations, or contextual data. We implemented the proposed matchmaking methods by using existing technologies, namely SPARQL, an RDF query language, and RESCAL, a tensor factorization algorithm. The implementations served as artefacts that we experimented with. We examined their usefulness by evaluating the accuracy and diversity of the matches they produce.</p>
          <p>In order to approximate the conditions in real-world public procurement we evaluated the designed matchmaking methods on a large dataset of retrospective data spanning ten years of Czech public procurement, including several related datasets. Preparation of this dataset constituted a fundamental part of our work. Transforming the data into a knowledge base structured as linked open data required an extensive effort that warranted the development of novel and reusable tools for data processing. Both the prepared dataset and the developed tools thus represent a key side contribution of our research. We published the cleaned and enriched Czech public procurement dataset as linked open data for anyone to reuse. Similarly, the implemented tools for working with RDF data were released as open source.</p>
          <p>The evaluation proved it challenging to obtain good results for the matchmaking task. Already during data preparation, we discovered the underlying data to be riddled with errors and ambiguity. Moreover, we problematized the ground truth that the matchmakers use to learn about matching public contracts to bidders. As we explained, the ground truth comprising data on historical awards of public contracts is subject to systemic biases that undermine its relevance for matchmaking. Despite these shortcomings, the evaluation indicated that the SPARQL-based matchmakers can be used to pre-screen relevant bidders or public contracts. Moreover, they can answer matchmaking queries on demand, even on constantly updating data. Apart from having subpar accuracy, the results of the RESCAL-based matchmakers were afflicted with very low diversity. These matchmakers turned out to be inferior in all the evaluated respects when compared with the SPARQL-based ones. We found the assumption that contextual data from linked can improve matchmaking to be justified, although the improvements proceeding from incorporating additional linked data turned out to be relatively minor. Nevertheless, most linked open data must be considered to be raw data that requires significant data preparation effort to realize its effective use.</p>
          <!-- Delta from the state of the art -->
          <p>When we review our progress beyond the state of the art, introduced in the sec. <a href="#sec:related-work">1.7</a>, our key contribution is the adaption of existing generic technologies for a concrete use case concerning matchmaking in the Czech public procurement. Using SPARQL, we developed a novel matchmaking method inspired by case-based reasoning. As a side effect of our investigation in matchmaking methods, we advanced the available means of processing RDF data by developing a set of reusable tools that address some of the recurrent tasks involved in handling RDF data. Ultimately, our work produced a greater value in the developed reusable artefacts for data preparation and matchmaking than as a practical use case in public procurement.</p>
          <p>The presented work was built on open source software as well as data prepared by others. In particular, most of the transformations of the ARES dataset were done by Jakub Klímek. The extracted Business Register data was provided by Ondřej Kokeš. Both these contributions are acknowledged directly in the sec. <a href="#sec:ares">2.4.6.2</a>. zIndex fairness scores were supplied to us by Datlab s.r.o. The software we reused in our work is listed in the appendix <a href="#sec:software">6</a>. The design of the Public Contracts Ontology was a collaborative effort as indicated in the references in the sec. <a href="#sec:pco">2.1.1</a>.</p>
          <!-- Assessment of the applicability of the work
          
          A practical service for Datlab?
          Reusable tools for data processing
          What needs to be done to reuse the matchmakers?
          Practical use?
          -->
          <!-- Assessment of degree of fulfillment of the stated goals -->
          <p>As stated in our goals, we explored the ways of matching of public contracts to bidders when their interactions are described as linked open data. Since the space of possibilities of applying matchmaking in this setting is vast, we managed to explore only a fraction of this space. We used sound heuristics to navigate this space and select the more salient and informative features to explore. <!-- Future work --> Overall, we explored only a few ways of matching public contracts to bidders. Many more ways of relevance engineering for this task are left open to pursue and assess their worth. <!-- Our work suggests that improving the data quality may produce the highest returns. --></p>
          
          </section>
          <section id="references" class="level1 unnumbered">
          <h1>References</h1>
          <div id="refs">
          <div id="ref-Abele2017">
          <p>ABELE, Andrejs, John P. MCCRAE, Paul BUITELAAR, Anja JENTZSCH and Richard CYGANIAK, 2017. <em>Linking open data cloud diagram</em> [online] [accessed 2017-03-02]. Available at: <a href="http://lod-cloud.net" class="uri">http://lod-cloud.net</a></p>
          </div>
          <div id="ref-AccessInfoEurope2011">
          <p>ACCESS INFO EUROPE and OPEN KNOWLEDGE FOUNDATION, 2011. <em>Beyond access: Open government data &amp; the right to (re)use public information</em> [online] [accessed 2017-01-03]. Available at: <a href="http://www.access-info.org/documents/Access_Docs/Advancing/Beyond_Access_7_January_2011_web.pdf" class="uri">http://www.access-info.org/documents/Access_Docs/Advancing/Beyond_Access_7_January_2011_web.pdf</a></p>
          </div>
          <div id="ref-Adomavicius2012">
          <p>ADOMAVICIUS, Gediminas and YoungOk KWON, 2012. Improving aggregate recommendation diversity using ranking-based techniques. <em>IEEE Transactions on Knowledge and Data Engineering</em>. May 2012, <strong>24</strong>(5), 896–911. </p>
          </div>
          <div id="ref-Akoka2007">
          <p>AKOKA, Jacky, Laure BERTI-ÉQUILLE, Omar BOUCELMA, Mokrane BOUZEGHOUB, Isabelle COMYN-WATTIAU, Mireille COSQUER, Virginie GOASDOUÉ-THION, Zoubida KEDAD, Sylvaine NUGIER, Verónika PERALTA and Samira SISAID-CHERFI, 2007. A framework for data quality evaluation in data integration systems. In: <em>9<sup>th</sup> international conference on enterprise information systems</em> [online]. p. 170–175. Available at: <a href="http://l1.lamsade.dauphine.fr/scripts/FILES/publi1091.pdf" class="uri">http://l1.lamsade.dauphine.fr/scripts/FILES/publi1091.pdf</a></p>
          </div>
          <div id="ref-AlvarezRodriguez2013">
          <p>ALVAREZ-RODRÍGUEZ, Jose María, José Emilio LABRA GAYO and Patricia ORDOÑEZ DE PABLOS, 2013. Enabling the matchmaking of organizations and public procurement notices by means of linked open data. In: Patricia ORDOÑEZ DE PABLOS, Miltiadis LYTRAS D., Robert TENNYSON D. and José Emilio LABRA GAYO, eds. <em>Cases on open-linked data and semantic web applications</em> [online]. Hershey (PA): IGI Global, p. 105–131. Available at: doi:<a href="https://doi.org/10.4018/978-1-4666-2827-4.ch006">10.4018/978-1-4666-2827-4.ch006</a></p>
          </div>
          <div id="ref-AlvarezRodriguez2011a">
          <p>ALVAREZ-RODRÍGUEZ, Jose María, José Emilio LABRA GAYO, Ramón CALMEAU, Ángel MARÍN and Jose Luis MARÍN, 2011a. Innovative services to ease the access to the public procurement notices using linking open data and advanced methods based on semantics. In: <em>Proceedings of the 5<sup>th</sup> international conference on methodologies and tools enabling e-government</em> [online]. Available at: <a href="http://www.josemalvarez.es/web/mypapers/metteg2011.pdf" class="uri">http://www.josemalvarez.es/web/mypapers/metteg2011.pdf</a></p>
          </div>
          <div id="ref-AlvarezRodriguez2011c">
          <p>ALVAREZ-RODRÍGUEZ, Jose María, José Emilio LABRA GAYO, Ramón CALMEAU, Ángel MARÍN and Jose Luis MARÍN, 2011b. Query expansion methods and performance: Evaluation for reusing linking open data of the european public procurement notices. In: Jose A. LOZANO, José A. GÁMEZ and José A. MORENO, eds. <em>Advances in artificial intelligence: Proceedings of the 14<sup>th</sup> conference of the spanish association for artificial intelligence</em> [online]. Berlin; Heidelberg: Springer. Lecture notes in computer science. ISBN 978-3-642-25273-0. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-25274-7">10.1007/978-3-642-25274-7</a></p>
          </div>
          <div id="ref-AlvarezRodriguez2012">
          <p>ALVAREZ-RODRÍGUEZ, Jose María, José Emilio LABRA GAYO, Francisco CIFUENTES, Gilor ALOR-HÉRNANDEZ, Cuauhtémoc SÁNCHEZ and Jaime Alberto GUZMÁN LUNA, 2012. Towards a pan-european e-procurement platform to aggregate, publish and search public procurement notices powered by linked open data: The MOLDEAS approach. <em>International Journal of Software Engineering and Knowledge Engineering</em> [online]. 2012, <strong>22</strong>(3). Available at: doi:<a href="https://doi.org/10.1142/S0218194012400086">10.1142/S0218194012400086</a></p>
          </div>
          <div id="ref-AlvarezRodriguez2011b">
          <p>ALVAREZ-RODRÍGUEZ, Jose María, José Emilio LABRA GAYO, Ángel MARÍN and Jose Luis MARÍN, 2011c. Semantic methods for reusing linking open data of the european public procurement notices. In: <em>Extended semantic web conference 2011 PhD symposium</em> [online]. Available at: <a href="http://www.josemalvarez.es/mypapers/eswc2011phdsymposium.pdf" class="uri">http://www.josemalvarez.es/mypapers/eswc2011phdsymposium.pdf</a></p>
          </div>
          <div id="ref-AlvarezRodriguez2014">
          <p>ALVAREZ-RODRÍGUEZ, Jose María, José Emilio LABRA-GAYO and Patricia ORDOÑEZ DE PABLOS, 2014. New trends on e-procurement applying semantic technologies: Current status and future challenges. <em>Computers in Industry</em> [online]. 2014, <strong>65</strong>(5), 800–820. Available at: doi:<a href="https://doi.org/10.1016/j.compind.2014.04.005">10.1016/j.compind.2014.04.005</a></p>
          </div>
          <div id="ref-Ankolekar2002">
          <p>ANKOLEKAR, Anupriya, Mark BURSTEIN, Jerry R. HOBBS, Ora LASSILA, David MARTIN, Drew MCDERMOTT, Sheila A. MCILRAITH, Srini NARAYANAN, Massimo PAOLUCCI, Terry PAYNE and Katia SYCARA, 2002. DAML-S: Web service description for the semantic web. In: Ian HORROCKS and James HENDLER, eds. <em>The semantic web: Proceedings of the first international semantic web conference</em> [online]. Berlin; Heidelberg: Springer, p. 348–363. Lecture notes in computer science. ISBN 978-3-540-43760-4. Available at: doi:<a href="https://doi.org/10.1007/3-540-48005-6_27">10.1007/3-540-48005-6_27</a></p>
          </div>
          <div id="ref-Archer2013">
          <p>ARCHER, Phil, Marios MEIMARIS and Agisilaos PAPANTONIOU, eds., 2013. <em>Registered organization vocabulary</em> [online]. W3C Working Group Note. [accessed 2017-01-01]. Available at: <a href="https://www.w3.org/TR/vocab-regorg" class="uri">https://www.w3.org/TR/vocab-regorg</a></p>
          </div>
          <div id="ref-Ayers2007">
          <p>AYERS, Danny, 2007. Evolving the link. <em>IEEE Internet Computing</em>. 2007, <strong>11</strong>(1), 94–96. ISSN 1089-7801. </p>
          </div>
          <div id="ref-Bandiera2009">
          <p>BANDIERA, Oriana, Andrea PRAT and Tommaso VALLETTI, 2009. Active and passive waste in government spending: Evidence from a policy experiment. <em>American Economic Review</em> [online]. 2009, <strong>99</strong>(4), 1278–1308. Available at: doi:<a href="https://doi.org/10.1257/aer.99.4.1278">10.1257/aer.99.4.1278</a></p>
          </div>
          <div id="ref-Batini2006">
          <p>BATINI, Carlo and Monica SCANNAPIECO, 2006. <em>Data quality: Concepts, methodologies and techniques</em>. Berlin; Heidelberg: Springer. Data-centric systems and applications. ISBN 978-3-540-33173-5. </p>
          </div>
          <div id="ref-Beckett2014">
          <p>BECKETT, David, Tim BERNERS-LEE, Eric PRUD’HOMMEAUX and Gavin CAROTHERS, 2014. <em>RDF 1.1 Turtle: Terse RDF triple language</em> [online]. W3C Recommendation. [accessed 2017-02-17]. Available at: <a href="https://www.w3.org/TR/turtle" class="uri">https://www.w3.org/TR/turtle</a></p>
          </div>
          <div id="ref-Beel2013">
          <p>BEEL, Joeran, Stefan LANGER, Bela GIPP, Marcel GENZMEHR and Andreas NÜRNBERGER, 2013. A comparative analysis of offline and online evaluations and discussion of research paper recommender system evaluation. In: <em>Proceedings of the international workshop on reproducibility and replication in recommender systems evaluation</em> [online]. New York (NY): ACM, p. 7–14. ISBN 978-1-4503-2465-6. Available at: doi:<a href="https://doi.org/10.1145/2532508.2532511">10.1145/2532508.2532511</a></p>
          </div>
          <div id="ref-Beliakov2015">
          <p>BELIAKOV, Gleb, Tommaso CALVO and Simon JAMES, 2015. Aggregation functions for recommender systems. In: Francesco RICCI, Lior ROKACH and Bracha SHAPIRA, eds. <em>Recommender systems handbook</em> [online]. 2nd ed. Berlin; Heidelberg: Springer, p. 777–808. Available at: doi:<a href="https://doi.org/10.1007/978-1-4899-7637-6_23">10.1007/978-1-4899-7637-6_23</a></p>
          </div>
          <div id="ref-BernersLee1996">
          <p>BERNERS-LEE, Tim, 1996. <em>Universal resource identifiers: Axioms of web architecture</em> [online] [accessed 2017-01-01]. Available at: <a href="http://www.w3.org/DesignIssues/Axioms.html" class="uri">http://www.w3.org/DesignIssues/Axioms.html</a></p>
          </div>
          <div id="ref-BernersLee2009">
          <p>BERNERS-LEE, Tim, 2009. <em>Linked data: Design issues</em> [online] [accessed 2017-02-16]. Available at: <a href="http://www.w3.org/DesignIssues/LinkedData.html" class="uri">http://www.w3.org/DesignIssues/LinkedData.html</a></p>
          </div>
          <div id="ref-Bizer2009">
          <p>BIZER, Christian, Tom HEATH and Tim BERNERS-LEE, 2009. Linked data: The story so far. <em>International Journal on Semantic Web and Information Systems</em> [online]. 2009, <strong>5</strong>(3), 1–22. Available at: doi:<a href="https://doi.org/10.4018/jswis.2009081901">10.4018/jswis.2009081901</a></p>
          </div>
          <div id="ref-Bleiholder2006">
          <p>BLEIHOLDER, Jens and Felix NAUMANN, 2006. Conflict handling strategies in an integrated information system. In: <em>Proceedings of the WWW 2006 workshop in information integration on the web (IIWEB)</em> [online]. Available at: <a href="http://edoc.hu-berlin.de/series/informatik-berichte/197/PDF/197.pdf" class="uri">http://edoc.hu-berlin.de/series/informatik-berichte/197/PDF/197.pdf</a></p>
          </div>
          <div id="ref-Bleiholder2008">
          <p>BLEIHOLDER, Jens and Felix NAUMANN, 2008. Data fusion. <em>ACM Computing Surveys</em> [online]. 2008, <strong>41</strong>(1). Available at: doi:<a href="https://doi.org/10.1145/1456650.1456651">10.1145/1456650.1456651</a></p>
          </div>
          <div id="ref-Boncz2014">
          <p>BONCZ, Peter, Orri ERLING and Minh-Duc PHAM, 2014. Advances in large-scale RDF data management. In: Sören AUER, Volha BRYL and Sebastian TRAMP, eds. <em>Linked open data: Creating knowledge out of interlinked data</em> [online]. Berlin; Heidelberg: Springer, Lecture notes in computer science, p. 21–44. ISBN 978-3-319-09845-6. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-09846-3_2">10.1007/978-3-319-09846-3_2</a></p>
          </div>
          <div id="ref-Brickley2002">
          <p>BRICKLEY, Dan, 2002. <em>RDF: Understanding the striped RDF/XML syntax</em> [online] [accessed 2017-01-13]. Available at: <a href="https://www.w3.org/2001/10/stripes" class="uri">https://www.w3.org/2001/10/stripes</a></p>
          </div>
          <div id="ref-Brickley2014">
          <p>BRICKLEY, Dan and Ramanathan V. GUHA, eds., 2014. <em>RDF Schema 1.1</em> [online]. W3C Recommendation. [accessed 2017-02-17]. Available at: <a href="https://www.w3.org/TR/rdf-schema" class="uri">https://www.w3.org/TR/rdf-schema</a></p>
          </div>
          <div id="ref-Bryl2014">
          <p>BRYL, Volha, Christian BIZER, Robert ISELE, Mateja VERLIC, Soon Gill HONG, Sammy JANG, Mun Yong YI and Key-Sun CHOI, 2014. Interlinking and knowledge fusion. In: Sören AUER, Volha BRYL and Sebastian TRAMP, eds. <em>Linked open data: Creating knowledge out of interlinked data</em> [online]. Berlin; Heidelberg: Springer, Lecture notes in computer science, p. 70–89. ISBN 978-3-319-09845-6. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-09846-3_2">10.1007/978-3-319-09846-3_2</a></p>
          </div>
          <div id="ref-Carothers2014">
          <p>CAROTHERS, Gavin, ed., 2014. <em>RDF 1.1 N-Quads: A line-based syntax for RDF datasets</em> [online]. W3C Recommendation. [accessed 2017-02-17]. Available at: <a href="https://www.w3.org/TR/n-quads" class="uri">https://www.w3.org/TR/n-quads</a></p>
          </div>
          <div id="ref-Chang2014">
          <p>CHANG, Kai-Wei, Scott Wen-tau YIH, Bishan YANG and Chris MEEK, 2014. Typed tensor decomposition of knowledge bases for relation extraction. In: <em>Proceedings of the 2014 conference on empirical methods in natural language processing</em> [online]. ACL – Association for Computational Linguistics. Available at: <a href="https://www.microsoft.com/en-us/research/publication/typed-tensor-decomposition-of-knowledge-bases-for-relation-extraction" class="uri">https://www.microsoft.com/en-us/research/publication/typed-tensor-decomposition-of-knowledge-bases-for-relation-extraction</a></p>
          </div>
          <div id="ref-Christen2012">
          <p>CHRISTEN, Peter, 2012. <em>Data matching: Concepts and techniques for record linkage, entity resolution, and duplicate detection</em> [online]. Berlin; Heidelberg: Springer. Data-centric systems and applications. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-31164-2">10.1007/978-3-642-31164-2</a></p>
          </div>
          <div id="ref-Craswell2009">
          <p>CRASWELL, Nick, 2009. Mean reciprocal rank. In: Ling LIU and Tamer ÖZSU, eds. <em>Encyclopedia of database systems</em>. Berlin; Heidelberg: Springer, p. 1703. ISBN 978-0-387-39940-9. </p>
          </div>
          <div id="ref-Cyganiak2014a">
          <p>CYGANIAK, Richard and Dave REYNOLDS, eds., 2014. <em>The RDF Data Cube Vocabulary</em> [online]. W3C Recommendation. [accessed 2017-01-24]. Available at: <a href="https://www.w3.org/TR/vocab-data-cube" class="uri">https://www.w3.org/TR/vocab-data-cube</a></p>
          </div>
          <div id="ref-Cyganiak2014b">
          <p>CYGANIAK, Richard, David WOOD and Markus LANTHALER, eds., 2014. <em>RDF 1.1 concepts and abstract syntax</em> [online]. W3C Recommendation. [accessed 2017-02-17]. Available at: <a href="http://www.w3.org/TR/rdf11-concepts" class="uri">http://www.w3.org/TR/rdf11-concepts</a></p>
          </div>
          <div id="ref-CzechRepublic2016">
          <p>CZECH REPUBLIC, 2016. <em>Zákon č. 134/2016 sb., zákon o zadávání veřejných zakázek</em>. 2016. ISSN 0322‑8037. </p>
          </div>
          <div id="ref-Davies2009">
          <p>DAVIES, John, Alistair DUKE and Atanas KIRIYAKOV, 2009. Semantic search. In: Ayse GÖKER and John DAVIES, eds. <em>Information retrieval: Searching in the 21<sup>st</sup> century</em>. Chichester (UK): John Wiley &amp; Sons, p. 179–213. ISBN 978-0-470-03364-7. </p>
          </div>
          <div id="ref-Delbru2012">
          <p>DELBRU, Renaud, Stephane CAMPINAS and Giovanni TUMMARELLO, 2012. Searching web data: An entity retrieval and high-performance indexing model. <em>Web Semantics</em> [online]. 2012, <strong>10</strong>, 33–58. ISSN 1570-8268. Available at: doi:<a href="https://doi.org/10.1016/j.websem.2011.04.004">10.1016/j.websem.2011.04.004</a></p>
          </div>
          <div id="ref-Deshpande2004">
          <p>DESHPANDE, Mukund and George KARYPIS, 2004. Item-based top-n recommendation algorithms. <em>ACM Transactions on Information Systems</em> [online]. January 2004, <strong>22</strong>(1), 143–177. ISSN 1046-8188. Available at: doi:<a href="https://doi.org/10.1145/963770.963776">10.1145/963770.963776</a></p>
          </div>
          <div id="ref-DiNoia2015">
          <p>DI NOIA, Tommaso and Vito Claudio OSTUNI, 2015. Recommender systems and linked open data. In: Wolfgang FABER and Adrian PASCHKE, eds. <em>Reasoning web. web logic rules. 11<sup>th</sup> international summer school 2015</em> [online]. Berlin; Heidelberg: Springer, p. 88–113. Lecture notes in computer science. Available at: <a href="http://sisinflab.poliba.it/publications/2015/DO15/Recommender%20Systems%20and%20Linked%20Open%20Data%20-%20RW%202015.pdf">http://sisinflab.poliba.it/publications/2015/DO15/Recommender%20Systems%20and%20Linked%20Open%20Data%20-%20RW%202015.pdf</a></p>
          </div>
          <div id="ref-DiNoia2014">
          <p>DI NOIA, Tommaso, Ivan CANTADOR and Vito Claudio OSTUNI, 2014. Linked open data-enabled recommender systems: ESWC 2014 challenge on book recommendation. In: Valentina PRESUTTI, Milan STANKOVIC, Eric CAMBRIA, Iván CANTADOR, Angelo DI IORIO, Tommaso DI NOIA, Christoph LANGE, Diego REFORGIATO RECUPERO and Anna TORDAI, eds. <em>Semantic web evaluation challenge: Revised selected papers</em>. Berlin; Heidelberg: Springer, p. 129–143. Communications in computer and information science. </p>
          </div>
          <div id="ref-DiNoia2007">
          <p>DI NOIA, Tommaso, Eugenio DI SCIASCIO and Francesco M. DONINI, 2007. Semantic matchmaking as non-monotonic reasoning: A description logic approach. <em>Journal of Artificial Intelligence Research</em> [online]. May 2007, <strong>29</strong>(1), 269–307. ISSN 1076-9757. Available at: <a href="https://vvvvw.aaai.org/Papers/JAIR/Vol29/JAIR-2909.pdf" class="uri">https://vvvvw.aaai.org/Papers/JAIR/Vol29/JAIR-2909.pdf</a></p>
          </div>
          <div id="ref-DiNoia2012b">
          <p>DI NOIA, Tommaso, Roberto MIRIZZI, Vito Claudio OSTUNI and Davide ROMITO, 2012a. Exploiting the web of data in model-based recommender systems. In: <em>Proceedings of the 6<sup>th</sup> ACM conference on recommender systems</em> [online]. New York (NY): ACM, p. 253–256. ISBN 978-1-4503-1270-7. Available at: doi:<a href="https://doi.org/10.1145/2365952.2366007">10.1145/2365952.2366007</a></p>
          </div>
          <div id="ref-DiNoia2012a">
          <p>DI NOIA, Tommaso, Roberto MIRIZZI, Vito Claudio OSTUNI, Davide ROMITO and Markus ZANKER, 2012b. Linked open data to support content-based recommender systems. In: <em>Proceedings of the 8<sup>th</sup> international conference on semantic systems</em> [online]. New York (NY): ACM, p. 1–8. ISBN 978-1-4503-1112-0. Available at: doi:<a href="https://doi.org/10.1145/2362499.2362501">10.1145/2362499.2362501</a></p>
          </div>
          <div id="ref-DiNoia2016">
          <p>DI NOIA, Tommaso, Vito Claudio OSTUNI, Paolo TOMEO and Eugenio DI SCIASCIO, 2016. SPrank: Semantic path-based ranking for top-n recommendations using linked open data. <em>ACM Transactions on Intelligent Systems and Technology</em>. 2016, <strong>8</strong>(1). ISSN 2157-6904. </p>
          </div>
          <div id="ref-DiNoia2004">
          <p>DI NOIA, Tommasso, Eugenio DI SCIASCIO, Francesco M. DONINI and Marina MONGIELLO, 2004. A system for principled matchmaking in an electronic marketplace. <em>International Journal of Electronic Commerce</em>. 2004, <strong>8</strong>(4), 9–37. ISSN 1557-9301. </p>
          </div>
          <div id="ref-Distinto2016">
          <p>DISTINTO, Isabella, Mathieu D’AQUIN and Enrico MOTTA, 2016. LOTED2: An ontology of european public procurement notices. <em>Semantic Web Journal</em> [online]. 2016, <strong>7</strong>(3). Available at: <a href="http://oro.open.ac.uk/45732/1/swj678_0.pdf" class="uri">http://oro.open.ac.uk/45732/1/swj678_0.pdf</a></p>
          </div>
          <div id="ref-Erling2012">
          <p>ERLING, Orri, 2012. Virtuoso, a hybrid RDBMS/graph column store. <em>Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</em> [online]. IEEE, March 2012, <strong>35</strong>(1) [accessed 2017-03-04]. Available at: <a href="http://sites.computer.org/debull/A12mar/p3.pdf" class="uri">http://sites.computer.org/debull/A12mar/p3.pdf</a></p>
          </div>
          <div id="ref-EU2004">
          <p>EU, 2004. <em>Directive 2004/18/EC of the European Parliament and of the Council of 31 March 2004 on the coordination of procedures for the award of public works contracts, public supply contracts and public service contracts</em> [online]. 2004. Available at: <a href="http://data.europa.eu/eli/dir/2004/18/oj" class="uri">http://data.europa.eu/eli/dir/2004/18/oj</a></p>
          </div>
          <div id="ref-EU2013">
          <p>EU, 2013. <em>Directive 2013/37/EU of the European Parliament and of the Council of 26 June 2013 amending the Directive 2003/98/EU on the re-use of public sector information</em> [online]. 2013. ISSN 1725-2555. Available at: <a href="http://data.europa.eu/eli/dir/2013/37/oj" class="uri">http://data.europa.eu/eli/dir/2013/37/oj</a></p>
          </div>
          <div id="ref-EU2014a">
          <p>EU, 2014a. <em>Directive 2014/24/EU of the European Parliament and of the Council of 26 February 2014 on public procurement and repealing Directive 2004/18/EC</em> [online]. 2014. Available at: <a href="http://data.europa.eu/eli/dir/2014/24/oj" class="uri">http://data.europa.eu/eli/dir/2014/24/oj</a></p>
          </div>
          <div id="ref-EU2014b">
          <p>EU, 2014b. <em>Directive 2014/25/EU of the European Parliament and of the Council of 26 February 2014 on procurement by entities operating in the water, energy, transport and postal services sectors and repealing Directive 2004/17/EC</em> [online]. 2014. Available at: <a href="http://data.europa.eu/eli/dir/2014/25/oj" class="uri">http://data.europa.eu/eli/dir/2014/25/oj</a></p>
          </div>
          <div id="ref-EuropeanCommission2016">
          <p>EUROPEAN COMMISSION, 2016. <em>Public procurement indicators 2015</em> [online] [accessed 2017-01-09]. Available at: <a href="http://ec.europa.eu/DocsRoom/documents/20679/attachments/1/translations/en/renditions/native" class="uri">http://ec.europa.eu/DocsRoom/documents/20679/attachments/1/translations/en/renditions/native</a></p>
          </div>
          <div id="ref-Euzenat2013">
          <p>EUZENAT, Jérôme and Pavel SHVAIKO, 2013. <em>Ontology matching</em>. 2nd ed. Berlin; Heidelberg: Springer. ISBN 978-3-642-38720-3. </p>
          </div>
          <div id="ref-Feigenbaum2013">
          <p>FEIGENBAUM, Lee, Gregory Todd WILLIAMS, Kendall Grant CLARK and Elias TORRES, eds., 2013. <em>SPARQL 1.1 protocol</em> [online]. W3C Recommendation. [accessed 2017-01-16]. Available at: <a href="https://www.w3.org/TR/sparql11-protocol" class="uri">https://www.w3.org/TR/sparql11-protocol</a></p>
          </div>
          <div id="ref-Franz2009">
          <p>FRANZ, Thomas, Antje SCHULTZ, Sergej SIZOV and Steffen STAAB, 2009. TripleRank: Ranking semantic web data by tensor decomposition. In: Abraham BERNSTEIN, David R. KARGER, Tom HEATH, Lee FEIGENBAUM, Diana MAYNARD, Enrico MOTTA and Krishnaprasad THIRUNARAYAN, eds. <em>The semantic web - ISWC 2009: 8<sup>th</sup> international semantic web conference, ISWC 2009: Proceedings</em> [online]. Berlin; Heidelberg: Springer, p. 213–228. ISBN 978-3-642-04930-9. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-04930-9_14">10.1007/978-3-642-04930-9_14</a></p>
          </div>
          <div id="ref-Friedrich2015">
          <p>FRIEDRICH, Heiko, 2015. <em>Evaluation of tensor-based machine learning algorithms for matching human need descriptions</em> [online]. c01. Research Studios Austria [accessed 2017-03-06]. Available at: <a href="https://sat.researchstudio.at/sites/sat.researchstudio.at/files/matching_technical_report_c012015_2.4_hf_150922.pdf" class="uri">https://sat.researchstudio.at/sites/sat.researchstudio.at/files/matching_technical_report_c012015_2.4_hf_150922.pdf</a></p>
          </div>
          <div id="ref-Friedrich2016">
          <p>FRIEDRICH, Heiko, Florian KLEEDORFER, Soheil HUMAN and Christian HUEMER, 2016. Integrating matching services into the Web of Needs. In: Michael MARTIN, Martí CUQUET and Erwin FOLMER, eds. <em>Joint proceedings of the posters and demos track of the 12<sup>th</sup> international conference on semantic systems - SEMANTiCS2016 and the 1<sup>st</sup> international workshop on semantic change &amp; evolving semantics (SuCCESS’16) co-located with the 12<sup>th</sup> international conference on semantic systems (SEMANTiCS 2016)</em> [online]. Aachen: RWTH Aachen University. CEUR workshop proceedings. Available at: <a href="http://ceur-ws.org/Vol-1695/paper19.pdf" class="uri">http://ceur-ws.org/Vol-1695/paper19.pdf</a></p>
          </div>
          <div id="ref-Futia2017">
          <p>FUTIA, Giuseppe, Alessio MELANDRI, Antonio VETRÒ, Federico MORANDO and Juan Carlos DE MARTIN, 2017. Removing barriers to transparency: A case study on the use of semantic technologies to tackle procurement data inconsistency. In: Eva BLOMQVIST, Diana MAYNARD, Aldo GANGEMI, Rinke HOEKSTRA, Pascal HITZLER and Olaf HARTIG, eds. <em>The semantic web: 14<sup>th</sup> international conference: Proceedings, part i</em> [online]. Cham: Springer, p. 623–637. ISBN 978-3-319-58068-5. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-58068-5_38">10.1007/978-3-319-58068-5_38</a></p>
          </div>
          <div id="ref-Gandon2014">
          <p>GANDON, Fabien and Guus SCHREIBER, eds., 2014. <em>RDF 1.1 XML syntax</em> [online]. W3C Recommendation. [accessed 2017-01-13]. Available at: <a href="https://www.w3.org/TR/rdf-syntax-grammar" class="uri">https://www.w3.org/TR/rdf-syntax-grammar</a></p>
          </div>
          <div id="ref-Garcin2014">
          <p>GARCIN, Florent, Boi FALTINGS, Olivier DONATSCH, Ayar ALAZZAWI, Christophe BRUTTIN and Amr HUBER, 2014. Offline and online evaluation of news recommender systems at swissinfo.ch. In: <em>Proceedings of the 8<sup>th</sup> ACM conference on recommender systems</em> [online]. New York (NY): ACM, p. 169–176. ISBN 978-1-4503-2668-1. Available at: doi:<a href="https://doi.org/10.1145/2645710.2645745">10.1145/2645710.2645745</a></p>
          </div>
          <div id="ref-Ge2010">
          <p>GE, Mouzhi, Carla DELGADO-BATTENFELD and Dietmar JANNACH, 2010. Beyond accuracy: Evaluating recommender systems by coverage and serendipity. In: <em>Proceedings of the 4<sup>th</sup> ACM conference on recommender systems</em> [online]. New York (NY): ACM, p. 257–260. ISBN 978-1-60558-906-0. Available at: doi:<a href="https://doi.org/10.1145/1864708.1864761">10.1145/1864708.1864761</a></p>
          </div>
          <div id="ref-Gearon2013">
          <p>GEARON, Paula, Alexandre PASSANT and Axel POLLERES, eds., 2013. <em>SPARQL 1.1 update</em> [online]. W3C Recommendation. [accessed 2017-02-21]. Available at: <a href="https://www.w3.org/TR/sparql11-update" class="uri">https://www.w3.org/TR/sparql11-update</a></p>
          </div>
          <div id="ref-Goldberg2013">
          <p>GOLDBERG, Daniel W., Morven BALLARD, James H. BOYD, Narelle MULLAN, Carol GARFIELD, Diana ROSMAN and Anna M. FERRANTE, 2013. An evaluation framework for comparing geocoding systems. <em>International Journal of Health Geographics</em> [online]. 2013 [accessed 2017-01-05]. Available at: doi:<a href="https://doi.org/10.1186/1476-072X-12-50">10.1186/1476-072X-12-50</a></p>
          </div>
          <div id="ref-GonzalezCastillo2001">
          <p>GONZÁLEZ-CASTILLO, Javier, David TRASTOUR and Claudio BARTOLINI, 2001. <em>Description logics for matchmaking of services</em> [online]. Hewlett-Packard [accessed 2017-03-03]. Available at: <a href="https://www.hpl.external.hp.com/techreports/2001/HPL-2001-265.pdf" class="uri">https://www.hpl.external.hp.com/techreports/2001/HPL-2001-265.pdf</a></p>
          </div>
          <div id="ref-Gosain2003">
          <p>GOSAIN, Sanjay, 2003. Realizing the vision for web services: Strategies for dealing with imperfect standards. In: John L. KING and Kalle LYYTINEN, eds. <em>Proceedings of the workshop on standard making: A critical research frontier for information systems</em> [online]. p. 10–29. Available at: <a href="http://www.joelwest.org/misq-stds/proceedings/126_10-29.pdf" class="uri">http://www.joelwest.org/misq-stds/proceedings/126_10-29.pdf</a></p>
          </div>
          <div id="ref-Graux2012">
          <p>GRAUX, Hans and Kronenburg TOM, 2012. <em>State of play: Re-use of public procurement data</em> [online]. European Public Sector Information Platform Topic Report 2012/7. [accessed 2017-02-10]. Available at: <a href="https://www.europeandataportal.eu/sites/default/files/2012_re_use_of_public_procurement_data.pdf" class="uri">https://www.europeandataportal.eu/sites/default/files/2012_re_use_of_public_procurement_data.pdf</a></p>
          </div>
          <div id="ref-Gruber1993">
          <p>GRUBER, Thomas R., 1993. A translation approach to portable ontology specifications. <em>Knowledge Acquisition</em> [online]. 1993, <strong>5</strong>(2), 199–220. Available at: <a href="http://tomgruber.org/writing/ontolingua-kaj-1993.htm" class="uri">http://tomgruber.org/writing/ontolingua-kaj-1993.htm</a></p>
          </div>
          <div id="ref-Haarslev2001">
          <p>HAARSLEV, Volker and Ralf MÖLLER, 2001. RACER system description. In: <em>Automated reasoning: Proceedings of the first international joint conference, IJCAR</em> [online]. Berlin; Heidelberg: Springer, p. 701–705. Lecture notes in computer science. ISBN 978-3-540-42254-9. Available at: doi:<a href="https://doi.org/10.1007/3-540-45744-5_59">10.1007/3-540-45744-5_59</a></p>
          </div>
          <div id="ref-Halevy2009">
          <p>HALEVY, Alon, Peter NORVIG and Fernando PEREIRA, 2009. The unreasonable effectiveness of data. <em>IEEE Intelligent Systems</em> [online]. IEEE, 2009, <strong>24</strong>(2), 8–12. ISSN 1541-1672. Available at: doi:<a href="https://doi.org/10.1109/MIS.2009.36">10.1109/MIS.2009.36</a></p>
          </div>
          <div id="ref-Harris2013">
          <p>HARRIS, Steve and Andy SEABORNE, 2013. <em>SPARQL 1.1 query language</em> [online]. W3C Recommendation. [accessed 2017-01-03]. Available at: <a href="http://www.w3.org/TR/sparql11-query" class="uri">http://www.w3.org/TR/sparql11-query</a></p>
          </div>
          <div id="ref-Heath2011">
          <p>HEATH, Tom and Christian BIZER, 2011. <em>Linked data: Evolving the web into a global data space</em> [online]. 1st ed. Morgan &amp; Claypool. Synthesis lectures on the semantic web: Theory and technology. ISBN 978-1-60845-431-0. Available at: doi:<a href="https://doi.org/10.2200/S00334ED1V01Y201102WBE001">10.2200/S00334ED1V01Y201102WBE001</a></p>
          </div>
          <div id="ref-Hebeler2009">
          <p>HEBELER, John, Matthew FISHER, Ryan BLACE and Andrew PEREZ-LOPEZ, 2009. <em>Semantic web programming</em>. Hoboken (NJ): John Wiley &amp; Sons. ISBN 978-0-470-41801-7. </p>
          </div>
          <div id="ref-Heitmann2010">
          <p>HEITMANN, Benjamin and Connor HAYES, 2010. Using linked data to build open, collaborative recommender systems. In: <em>Proceedings of the 2010 AAAI spring symposium: Linked data meets artificial intelligence</em> [online]. Palo Alto (CA): AAAI, p. 76–81. Available at: <a href="http://www.aaai.org/ocs/index.php/SSS/SSS10/paper/view/1067/1452" class="uri">http://www.aaai.org/ocs/index.php/SSS/SSS10/paper/view/1067/1452</a></p>
          </div>
          <div id="ref-Heitmann2014">
          <p>HEITMANN, Benjamin and Conor HAYES, 2014. SemStim at the LOD-RecSys 2014 challenge. In: Valentina PRESUTTI, Milan STANKOVIC, Eric CAMBRIA, Iván CANTADOR, Angelo DI IORIO, Tommaso DI NOIA, Christoph LANGE, Diego REFORGIATO RECUPERO and Anna TORDAI, eds. <em>Semantic web evaluation challenge: Revised selected papers</em> [online]. Berlin; Heidelberg: Springer, p. 170–175. Communications in computer and information science. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-12024-9_22">10.1007/978-3-319-12024-9_22</a></p>
          </div>
          <div id="ref-Heitmann2016">
          <p>HEITMANN, Benjamin and Conor HAYES, 2016. SemStim: Exploiting knowledge graphs for cross-domain recommendation. In: <em>2016 IEEE 16<sup>th</sup> international conference on data mining workshops</em> [online]. New York (NY): IEEE, p. 999–1006. Available at: doi:<a href="https://doi.org/10.1109/ICDMW.2016.0145">10.1109/ICDMW.2016.0145</a></p>
          </div>
          <div id="ref-Hepp2008">
          <p>HEPP, Martin, 2008. GoodRelations: An ontology for describing products and services offers on the web. In: <em>Knowledge engineering: Practice and patterns: Proceedings of the 16<sup>th</sup> international conference, ekaw 2008</em> [online]. Berlin; Heidelberg: Springer, p. 329–346. Available at: doi:<a href="https://doi.org/10.1007/978-3-540-87696-0_29">10.1007/978-3-540-87696-0_29</a></p>
          </div>
          <div id="ref-Herlocker2004">
          <p>HERLOCKER, Jonathan L., Joseph A. KONSTAN, Loren G. TERVEEN and John T. RIEDL, 2004. Evaluating collaborative filtering recommender systems. <em>ACM Transactions on Information Systems</em> [online]. New York (NY): ACM, January 2004, <strong>22</strong>(1), 5–53. Available at: doi:<a href="https://doi.org/10.1145/963770.963772">10.1145/963770.963772</a></p>
          </div>
          <div id="ref-Hevner2004">
          <p>HEVNER, Alan R., Salvatore T. MARCH and Jinsoo PARK, 2004. Design science in information systems research. <em>MIS Quarterly</em>. January 2004, <strong>28</strong>(1), 75–105. </p>
          </div>
          <div id="ref-Hitzler2010">
          <p>HITZLER, Pascal and Frank VAN HARMELEN, 2010. A reasonable semantic web. <em>Semantic Web</em> [online]. 2010, <strong>1</strong>(1, 2), 39–44. Available at: doi:<a href="https://doi.org/10.3233/SW-2010-0010">10.3233/SW-2010-0010</a></p>
          </div>
          <div id="ref-Isele2013">
          <p>ISELE, Robert and Christian BIZER, 2013. Active learning of expressive linkage rules using genetic programming. <em>Web Semantics</em> [online]. December 2013, <strong>23</strong>, 2–15. Available at: <a href="http://www.websemanticsjournal.org/index.php/ps/article/viewFile/340/360" class="uri">http://www.websemanticsjournal.org/index.php/ps/article/viewFile/340/360</a></p>
          </div>
          <div id="ref-Isele2011">
          <p>ISELE, Robert, Anja JENTZSCH and Christian BIZER, 2011. Efficient multidimensional blocking for link discovery without losing recall. In: Amélie MARIAN and Vasilis VASSALOS, eds. <em>Proceedings of the 14<sup>th</sup> international workshop on the web and databases</em> [online]. Available at: <a href="http://www.wiwiss.fu-berlin.de/en/fachbereich/bwl/pwo/bizer/research/publications/IseleJentzschBizer-WebDB2011.pdf" class="uri">http://www.wiwiss.fu-berlin.de/en/fachbereich/bwl/pwo/bizer/research/publications/IseleJentzschBizer-WebDB2011.pdf</a></p>
          </div>
          <div id="ref-Jacobs2004">
          <p>JACOBS, Ian and Norman WALSH, 2004. <em>Architecture of the World Wide Web, volume 1</em> [online]. W3C Recommendation. [accessed 2017-01-19]. Available at: <a href="http://www.w3.org/TR/webarch" class="uri">http://www.w3.org/TR/webarch</a></p>
          </div>
          <div id="ref-Jannach2010">
          <p>JANNACH, Dietmar, Markus ZANKER, Alexander FELFERNIG and Gerhard FRIEDRICH, 2010. <em>Recommender systems: An introduction</em>. 1st ed. New York (NY): Cambridge University Press. ISBN 978-0-521-49336-9. </p>
          </div>
          <div id="ref-Kenny2012">
          <p>KENNY, Charles and Jonathan KARVER, 2012. <em>CDG policy paper</em>: <em>Publish what you buy: The case for routine publication of government contracts</em> [online]. 011. Washington DC: Center for Global Development [accessed 2017-01-03]. Available at: <a href="http://www.cgdev.org/content/publications/detail/1426431" class="uri">http://www.cgdev.org/content/publications/detail/1426431</a></p>
          </div>
          <div id="ref-Kiefer2008">
          <p>KIEFER, Christoph and Abraham BERNSTEIN, 2008. The creation and evaluation of iSPARQL strategies for matchmaking. In: <em>The semantic web: Research and applications: Proceedings of the 5<sup>th</sup> european semantic web conference, ESWC 2008</em> [online]. Berlin; Heidelberg: Springer, p. 463–477. Lecture notes in computer science. ISBN 978-3-540-68233-2. Available at: doi:<a href="https://doi.org/10.1007/978-3-540-68234-9_35">10.1007/978-3-540-68234-9_35</a></p>
          </div>
          <div id="ref-Kiefer2007">
          <p>KIEFER, Christoph, Abraham BERNSTEIN and Markus STOCKER, 2007. The fundamentals of iSPARQL: A virtual triple approach for similarity-based semantic web tasks. In: Karl ABERER, Key-Sun CHOI, Natasha NOY, Dean ALLEMANG, Kyung-Il LEE, Lyndon NIXON, Jennifer GOLBECK, Peter MIKA, Diana MAYNARD, Riichiro MIZOGUCHI, Guus SCHREIBER and Philippe CUDRÉ-MAUROUX, eds. <em>The semantic web</em> [online]. Berlin; Heidelberg: Springer, p. 295–309. Lecture notes in computer science. ISBN 978-3-540-76297-3. Available at: doi:<a href="https://doi.org/10.1007/978-3-540-76298-0_22">10.1007/978-3-540-76298-0_22</a></p>
          </div>
          <div id="ref-Kimball2004">
          <p>KIMBALL, Ralph and Joe CASERTA, 2004. <em>The data warehouse ETL toolkit: Practical techniques for extracting, cleaning, conforming, and delivering data</em>. Hoboken (NJ): Wiley. ISBN 978-0-7645-6757-5. </p>
          </div>
          <div id="ref-Kleedorfer2013">
          <p>KLEEDORFER, Florian and Christina Maria BUSCH, 2013. Beyond data: Building a web of needs. In: Christian BIZER, ed. <em>Linked data on the web: Proceedings of the WWW 2013 workshop on linked data on the web</em> [online]. Aachen: RWTH Aachen University. CEUR workshop proceedings. ISSN 1613-0073. Available at: <a href="http://ceur-ws.org/Vol-996/papers/ldow2013-paper-13.pdf" class="uri">http://ceur-ws.org/Vol-996/papers/ldow2013-paper-13.pdf</a></p>
          </div>
          <div id="ref-Kleedorfer2014">
          <p>KLEEDORFER, Florian, Christina Maria BUSCH, Christian PICHLER and Christian HUEMER, 2014. The case for the Web of Needs. In: <em>Proceedings of the 2014 IEEE 16<sup>th</sup> conference on business informatics</em> [online]. New York (NY): IEEE, p. 94–101. Available at: doi:<a href="https://doi.org/10.1109/CBI.2014.55">10.1109/CBI.2014.55</a></p>
          </div>
          <div id="ref-Kleedorfer2016">
          <p>KLEEDORFER, Florian, Soheil HUMAN, Heiko FRIEDRICH and Christian HUEMER, 2016. Web of Needs: A process overview. In: Michael MARTIN, Martí CUQUET and Erwin FOLMER, eds. <em>Joint proceedings of the posters and demos track of the 12<sup>th</sup> international conference on semantic systems - SEMANTiCS2016 and the 1<sup>st</sup> international workshop on semantic change &amp; evolving semantics (SuCCESS’16) co-located with the 12<sup>th</sup> international conference on semantic systems (SEMANTiCS 2016)</em> [online]. Aachen: RWTH Aachen University. CEUR workshop proceedings. Available at: <a href="http://ceur-ws.org/Vol-1695/paper22.pdf" class="uri">http://ceur-ws.org/Vol-1695/paper22.pdf</a></p>
          </div>
          <div id="ref-Klimek2012">
          <p>KLÍMEK, Jakub, Tomáš KNAP, Jindřich MYNARZ, Martin NEČASKÝ and Vojtěch SVÁTEK, 2012. <em>LOD2 deliverable 9a.1.1: Framework for creating linked data in the domain of public sector contracts</em>. </p>
          </div>
          <div id="ref-Klimek2016">
          <p>KLÍMEK, Jakub, Petr ŠKODA and Martin NEČASKÝ, 2016. LinkedPipes ETL: Evolved linked data preparation. In: Harald SACK, Giuseppe RIZZO, Nadine STEINMETZ, Dunja MLADENIĆ, Sören AUER and Christoph LANGE, eds. <em>The semantic web: ESWC 2016 satellite events. revised selected papers</em> [online]. Berlin; Heidelberg: Springer, p. 95–100. Lecture notes in computer science. ISBN 978-3-319-47602-5. Available at: <a href="http://2016.eswc-conferences.org/sites/default/files/papers/Accepted%20Posters%20and%20Demos/ESWC2016_DEMO_Linked_Pipes_ETL.pdf">http://2016.eswc-conferences.org/sites/default/files/papers/Accepted%20Posters%20and%20Demos/ESWC2016_DEMO_Linked_Pipes_ETL.pdf</a></p>
          </div>
          <div id="ref-Klyne2002">
          <p>KLYNE, Graham and Jeremy CARROLL, eds., 2002. <em>Resource description framework (RDF): Concepts and abstract data model</em> [online]. W3C Working Draft. [accessed 2017-01-23]. Available at: <a href="https://www.w3.org/TR/2002/WD-rdf-concepts-20020829" class="uri">https://www.w3.org/TR/2002/WD-rdf-concepts-20020829</a></p>
          </div>
          <div id="ref-Knap2017">
          <p>KNAP, Tomáš, Peter HANEČÁK, Jakub KLÍMEK, Christian MADER, Martin NEČASKÝ, Bert VAN NUFFELEN and Petr ŠKODA, 2017. UnifiedViews: An ETL tool for RDF data management. <em>Semantic Web: Interoperability, Usability, Applicability</em> [online]. 2017. Available at: <a href="http://www.semantic-web-journal.net/system/files/swj1265.pdf" class="uri">http://www.semantic-web-journal.net/system/files/swj1265.pdf</a></p>
          </div>
          <div id="ref-Kolda2009">
          <p>KOLDA, Tamara G. and Brett W. BADER, 2009. Tensor decompositions and applications. <em>SIAM Review</em> [online]. 2009, <strong>51</strong>(3), 455–500. Available at: doi:<a href="https://doi.org/10.1137/07070111X">10.1137/07070111X</a></p>
          </div>
          <div id="ref-Kolodner1992">
          <p>KOLODNER, Janet L., 1992. An introduction to case-based reasoning. <em>Artificial Intelligence Review</em> [online]. March 1992, <strong>6</strong>(1), 3–34. Available at: doi:<a href="https://doi.org/10.1007/BF00155578">10.1007/BF00155578</a></p>
          </div>
          <div id="ref-Koren2009">
          <p>KOREN, Yehuda, Robert BELL and Chris VOLINSKY, 2009. Matrix factorization techniques for recommender systems. <em>Computer</em> [online]. IEEE, August 2009, <strong>42</strong>(8), 30–37. Available at: doi:<a href="https://doi.org/10.1109/MC.2009.263">10.1109/MC.2009.263</a></p>
          </div>
          <div id="ref-Krompass2015">
          <p>KROMPAß, Denis, Stephan BAIER and Volker TRESP, 2015. Type-constrained representation learning in knowledge graphs. In: Marcelo ARENAS, Oscar CORCHO, Elena SIMPERL, Markus STROHMAIER, Mathieu D’AQUIN, Kavitha SRINIVAS, Paul GROTH, Michel DUMONTIER, Jeff HEFLIN, Krishnaprasad THIRUNARAYAN and Steffen STAAB, eds. <em>The semantic web: ISWC 2015: 14<sup>th</sup> international semantic web conference. proceedings. part i.</em> [online]. Cham: Springer, p. 640–655. Lecture notes in computer science. ISBN 978-3-319-25006-9. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-25007-6_37">10.1007/978-3-319-25007-6_37</a></p>
          </div>
          <div id="ref-Krompass2014">
          <p>KROMPAß, Denis, Maximilian NICKEL and Volker TRESP, 2014. Large-scale factorization of type-constrained multi-relational data. In: <em>2014 international conference on data science and advanced analytics</em> [online]. Shanghai, China: IEEE, p. 18–24. Available at: doi:<a href="https://doi.org/10.1109/DSAA.2014.7058046">10.1109/DSAA.2014.7058046</a></p>
          </div>
          <div id="ref-Kuchar2016">
          <p>KUCHAŘ, Jaroslav, Milan DOJCHINOVSKI and Tomáš VITVAR, 2016. Exploiting temporal dimension in tensor-based link prediction. In: Valérie MONFORT, Karl-Heinz KREMPELS, Tim A. MAJCHRZAK and Žiga TURK, eds. <em>Web information systems and technologies: 11<sup>th</sup> international conference, WEBIST 2015: Revised selected papers</em> [online]. Cham: Springer, p. 211–231. Lecture notes in business information processing. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-30996-5_11">10.1007/978-3-319-30996-5_11</a></p>
          </div>
          <div id="ref-Kuokka1995">
          <p>KUOKKA, Daniel and Larry HARADA, 1995. Matchmaking for information agents. In: <em>Proceedings of the 14<sup>th</sup> international joint conference on artificial intelligence</em> [online]. San Francisco (CA): Morgan Kaufmann, p. 672–678. ISBN 1-55860-363-8. Available at: <a href="https://www.ijcai.org/Proceedings/95-1/Papers/088.pdf" class="uri">https://www.ijcai.org/Proceedings/95-1/Papers/088.pdf</a></p>
          </div>
          <div id="ref-Li2004">
          <p>LI, Lei and Ian HORROCKS, 2004. A software framework for matchmaking based on semantic web technology. <em>International Journal of Electronic Commerce</em> [online]. 2004, <strong>8</strong>(4), 39–60. Available at: doi:<a href="https://doi.org/10.1080/10864415.2004.11044307">10.1080/10864415.2004.11044307</a></p>
          </div>
          <div id="ref-Maali2014">
          <p>MAALI, Fadi, 2014. A data-flow language for big RDF data processing. In: Paul GROTH and Natasha NOY, eds. <em>Proceedings of the doctoral consortium at the 13<sup>th</sup> international semantic web conference (ISWC 2014)</em> [online]. p. 56–63. ISSN 1613-0073. Available at: <a href="http://ceur-ws.org/Vol-1262/paper7.pdf" class="uri">http://ceur-ws.org/Vol-1262/paper7.pdf</a></p>
          </div>
          <div id="ref-Magliacane2012">
          <p>MAGLIACANE, Sara, Alessandro BOZZON and Emanuele DELLA VALLE, 2012. Efficient execution of top-k SPARQL queries. In: Philippe CUDRÉ-MAUROUX, Jeff HEFLIN, Evren SIRIN, Tania TUDORACHE, Jérôme EUZENAT, Manfred HAUSWIRTH, Josiane-Xavier PARREIRA, Jim HENDLER, Guus SCHREIBER, Abraham BERNSTEIN and Eva BLOMQVIST, eds. <em>The semantic web: ISWC 2012</em> [online]. Berlin; Heidelberg: Springer, p. 344–360. Lecture notes in computer science. ISBN 978-3-642-35175-4. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-35176-1_22">10.1007/978-3-642-35176-1_22</a></p>
          </div>
          <div id="ref-Maidel2008">
          <p>MAIDEL, Veronica, Peretz SCHOVAL, Bracha SHAPIRA and Meirav TAIEB-MAIMON, 2008. Evaluation of an ontology-content based filtering method for a personalized newspaper. In: <em>Proceedings of the 2008 ACM conference on recommender systems</em> [online]. New York (NY): ACM, p. 91–98. ISBN 978-1-60558-093-7. Available at: doi:<a href="https://doi.org/10.1145/1454008.1454024">10.1145/1454008.1454024</a></p>
          </div>
          <div id="ref-EightPrinciples2007">
          <p>MALAMUD, Carl, ed., 2007. <em>8 principles of open government data</em> [online] [accessed 2017-02-15]. Available at: <a href="https://public.resource.org/8_principles.html" class="uri">https://public.resource.org/8_principles.html</a></p>
          </div>
          <div id="ref-Marin2013">
          <p>MARÍN, Jose Luis, Mai RODRÍGUEZ, Ángel MARÍN, Ramón CALMEAU, Jose María ALVAREZ-RODRÍGUEZ, Luis POLO-PAREDES, Emilio RUBIERA-AZCONA, Alejandro RODRÍGUEZ-GONZÁLEZ, José Emilio LABRA-GAYO and Patricia ORDOÑEZ DE PABLOS, 2013. Euroalert.net: Aggregating public procurement data to deliver commercial services to SMEs. In: Patricia ORDOÑEZ DE PABLOS, Juan Manuel CUEVA LOVELLE, José Emilio LABRA-GAYO and Robert D. TENNYSON, eds. <em>E-procurement management for successful electronic government systems</em> [online]. Hershey (PA): IGI Global, p. 114–130. Available at: doi:<a href="https://doi.org/10.4018/978-1-4666-2119-0.ch007">10.4018/978-1-4666-2119-0.ch007</a></p>
          </div>
          <div id="ref-McPherson2001">
          <p>MCPHERSON, Miller, Lynn SMITH-LOVIN and James M COOK, 2001. Birds of a feather: Homophily in social networks. <em>Annual review of sociology</em>. 2001, <strong>27</strong>(1), 415–444. </p>
          </div>
          <div id="ref-Mendes2017">
          <p>MENDES, Mara and Mihály FAZEKAS, 2017. <em>Towards more transparent and efficient contracting: Public procurement in the european union</em> [online] [accessed 2017-06-01]. Available at: <a href="https://opentender.eu/blog/2017-03-towards-more-transparency" class="uri">https://opentender.eu/blog/2017-03-towards-more-transparency</a></p>
          </div>
          <div id="ref-Mihindukulasooriya2013">
          <p>MIHINDUKULASOORIYA, Nandana, Raúl GARCÍA-CASTRO and Miguel ESTEBAN-GUTIÉRREZ, 2013. Linked Data Platform as a novel approach for Enterprise Application Integration. In: Olaf HARTIG, Juan SEQUEDA, Aidan HOGAN and Takahide MATSUTSUSUKA, eds. <em>Proceedings of the 4<sup>th</sup> international workshop on consuming linked data co-located with the 12<sup>th</sup> international semantic web conference</em> [online]. Aachen: RWTH Aachen University [accessed 2017-01-09]. CEUR workshop proceedings. Available at: <a href="http://ceur-ws.org/Vol-1034/MihindukulasooriyaEtAl_COLD2013.pdf" class="uri">http://ceur-ws.org/Vol-1034/MihindukulasooriyaEtAl_COLD2013.pdf</a></p>
          </div>
          <div id="ref-Miles2009">
          <p>MILES, Alistair and Sean BECHHOFER, eds., 2009. <em>SKOS Simple Knowledge Organization System reference</em> [online]. W3C Recommendation. W3C [accessed 2017-01-25]. Available at: <a href="https://www.w3.org/TR/skos-reference" class="uri">https://www.w3.org/TR/skos-reference</a></p>
          </div>
          <div id="ref-MunozSoro2015">
          <p>MUÑOZ-SORO, José Félix and Guillermo ESTEBAN, 2015. Using the semantic web for the integration and publication of public procurement data. In: Kö A. and Francesconi E., eds. <em>Electronic government and the information systems perspective: EGOVIS 2015</em> [online]. Cham: Springer. Lecture notes in computer science. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-22389-6_2">10.1007/978-3-319-22389-6_2</a></p>
          </div>
          <div id="ref-MunozSoro2016">
          <p>MUÑOZ-SORO, José Félix, Guillermo ESTEBAN, Oscar CORCHO and Francisco SERÓN, 2016. PPROC, an ontology for transparency in public procurement. <em>Semantic Web</em> [online]. 2016, <strong>7</strong>(3), 295–309. Available at: doi:<a href="https://doi.org/10.3233/SW-150195">10.3233/SW-150195</a></p>
          </div>
          <div id="ref-Mynarz2014c">
          <p>MYNARZ, Jindřich, 2014a. Integration of public procurement data using linked data. <em>Journal of Systems Integration</em> [online]. 2014, <strong>5</strong>(4), 19–31. ISSN 1804-2724. Available at: <a href="http://www.si-journal.org/index.php/JSI/article/viewFile/213/158" class="uri">http://www.si-journal.org/index.php/JSI/article/viewFile/213/158</a></p>
          </div>
          <div id="ref-Mynarz2014a">
          <p>MYNARZ, Jindřich, 2014b. <em>Methods for designing ontologies and vocabularies for data on the web</em> [online] [accessed 2017-01-03]. Available at: <a href="http://www.damepraci.eu/p/designing-ontologies.html" class="uri">http://www.damepraci.eu/p/designing-ontologies.html</a></p>
          </div>
          <div id="ref-Mynarz2015">
          <p>MYNARZ, Jindřich, Vojtěch SVÁTEK and Tommaso DI NOIA, 2015. Matchmaking public procurement linked open data. In: Christophe DEBRUYNE, Hervé PANETTO, Robert MEERSMAN, Tharam DILLON, Georg WEICHART, Yuan AN and Claudio Agostino ARDAGNA, eds. <em>Proceedings of On the Move to Meaningful Internet Systems: OTM 2015 conferences</em> [online]. Berlin; Heidelberg: Springer, p. 405–422. Information systems and applications, incl. internet/Web, and HCI. ISBN 978-3-319-26148-5. Available at: <a href="http://link.springer.com/chapter/10.1007%2F978-3-319-26148-5_27">http://link.springer.com/chapter/10.1007%2F978-3-319-26148-5_27</a></p>
          </div>
          <div id="ref-Mynarz2014b">
          <p>MYNARZ, Jindřich, Václav ZEMAN and Marek DUDÁŠ, 2014. <em>LOD2 deliverable 9a.2.2: Stable implementation of matching functionality into web application for filing public contracts</em> [online]. [accessed 2017-01-03]. Available at: <a href="http://svn.aksw.org/lod2/D9a.2.2/public.pdf" class="uri">http://svn.aksw.org/lod2/D9a.2.2/public.pdf</a></p>
          </div>
          <div id="ref-Morup2011">
          <p>MØRUP, Morten, 2011. Applications of tensor (multiway array) factorizations and decompositions in data mining. <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em> [online]. John Wiley &amp; Sons, 2011, <strong>1</strong>(1), 24–40. ISSN 1942-4795. Available at: doi:<a href="https://doi.org/10.1002/widm.1">10.1002/widm.1</a></p>
          </div>
          <div id="ref-Naumann2006">
          <p>NAUMANN, Felix, Alexander BILKE, Jens BLEIHOLDER and Melanie WEIS, 2006. Data fusion in three steps: Resolving schema, tuple, and value inconsistencies. <em>Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</em> [online]. June 2006, <strong>29</strong>(2), 21–31. Available at: <a href="http://sites.computer.org/debull/A06june/A06JUN-CD.pdf" class="uri">http://sites.computer.org/debull/A06june/A06JUN-CD.pdf</a></p>
          </div>
          <div id="ref-Necasky2014">
          <p>NEČASKÝ, Martin, Jakub KLÍMEK, Jindřich MYNARZ, Tomáš KNAP, Vojtěch SVÁTEK and Jakub STÁRKA, 2014. Linked data support for filing public contracts. <em>Computers in Industry</em> [online]. January 2014. ISSN 0166-3615. Available at: <a href="http://dx.doi.org/10.1016/j.compind.2013.12.006" class="uri">http://dx.doi.org/10.1016/j.compind.2013.12.006</a></p>
          </div>
          <div id="ref-Nedved2017">
          <p>NEDVĚD, Adam, Tomáš DUCHÁČEK and Jiří SKUHROVEC, 2017. <em>Rozhodovací praxe ÚOHS: Mýty a fakta</em> [online]. EconLab. Available at: <a href="http://www.econlab.cz/wp-content/uploads/2014/10/2017-01-26-studie-uohs-final.pdf" class="uri">http://www.econlab.cz/wp-content/uploads/2014/10/2017-01-26-studie-uohs-final.pdf</a></p>
          </div>
          <div id="ref-Nguyen2015">
          <p>NGUYEN, Phuong, Paolo TOMEO, Tommaso DI NOIA and Eugenio DI SCIASCIO, 2015. An evaluation of SimRank and personalized PageRank to build a recommender system for the web of data. In: <em>Proceedings of the 24<sup>th</sup> international conference on world wide web</em> [online]. New York (NY): ACM, p. 1477–1482. ISBN 978-1-4503-3473-0. Available at: doi:<a href="https://doi.org/10.1145/2740908.2742141">10.1145/2740908.2742141</a></p>
          </div>
          <div id="ref-Nickel2013b">
          <p>NICKEL, Maximilian and Volker TRESP, 2013a. An analysis of tensor models for learning on structured data. In: Hendrik BLOCKEEL, Kristian KERSTING, Siegfried NIJSSEN and Filip ŽELEZNÝ, eds. <em>Machine learning and knowledge discovery in databases: Proceedings of ECML PKDD 2013. part II</em> [online]. p. 272–287. Lecture notes in computer science. ISBN 978-3-642-40990-5. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-40991-2_18">10.1007/978-3-642-40991-2_18</a></p>
          </div>
          <div id="ref-Nickel2013c">
          <p>NICKEL, Maximilian and Volker TRESP, 2013b. Tensor factorization for multi-relational learning. In: Hendrik BLOCKEEL, Kristian KERSTING, Siegfried NIJSSEN and Filip ŽELEZNÝ, eds. <em>Machine learning and knowledge discovery in databases: Proceedings of ecml pkdd 2013</em> [online]. p. 617–621. Lecture notes in computer science. ISBN 978-3-642-40993-6. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-40994-3_40">10.1007/978-3-642-40994-3_40</a></p>
          </div>
          <div id="ref-Nickel2014">
          <p>NICKEL, Maximilian, Xueyan JIANG and Volker TRESP, 2014. Reducing the rank in relational factorization models by including observable patterns. In: Z. GHAHRAMANI, M. WELLING, C. CORTES, N. D. LAWRENCE and K. Q. WEINBERGER, eds. <em>Advances in neural information processing systems 27</em> [online]. Curran Associates, p. 1179–1187. Available at: <a href="http://papers.nips.cc/paper/5448-reducing-the-rank-in-relational-factorization-models-by-including-observable-patterns.pdf" class="uri">http://papers.nips.cc/paper/5448-reducing-the-rank-in-relational-factorization-models-by-including-observable-patterns.pdf</a></p>
          </div>
          <div id="ref-Nickel2016">
          <p>NICKEL, Maximilian, Kevin MURPHY, Volker TRESP and Evgeniy GABRILOVICH, 2016. A review of relational machine learning for knowledge graphs. <em>Proceedings of the IEEE</em> [online]. IEEE, January 2016, <strong>104</strong>(1), 11–33. Available at: doi:<a href="https://doi.org/10.1109/JPROC.2015.2483592">10.1109/JPROC.2015.2483592</a></p>
          </div>
          <div id="ref-Nickel2011">
          <p>NICKEL, Maximilian, Volker TRESP and Hans-Peter KRIEGEL, 2011. A three-way model for collective learning on multi-relational data. In: <em>Proceedings of the 28<sup>th</sup> international conference on machine learning (ICML’11)</em> [online]. New York (NY): ACM, p. 809–816. Available at: <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Nickel_438.pdf" class="uri">http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Nickel_438.pdf</a></p>
          </div>
          <div id="ref-Nickel2012">
          <p>NICKEL, Maximilian, Volker TRESP and Hans-Peter KRIEGEL, 2012. Factorizing YAGO: Scalable machine learning for linked data. In: <em>Proceedings of the 21<sup>st</sup> international conference on world wide web (WWW’12)</em>. Lyon, France: ACM, p. 271–280. </p>
          </div>
          <div id="ref-OpenDefinition2015">
          <p>OPEN KNOWLEDGE, 2015. <em>Open definition 2.1</em> [online] [accessed 2017-02-15]. Available at: <a href="http://opendefinition.org/od/2.1/en" class="uri">http://opendefinition.org/od/2.1/en</a></p>
          </div>
          <div id="ref-Virtuoso2017">
          <p>OPENLINK SOFTWARE, 2017. <em>Faceted views over large-scale linked data</em> [online] [accessed 2017-05-18]. Available at: <a href="https://www.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtuosoFacetsViewsLinkedData#Entity%20Ranking">https://www.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtuosoFacetsViewsLinkedData#Entity%20Ranking</a></p>
          </div>
          <div id="ref-Padia2016">
          <p>PADIA, Ankur, Konstantinos KALPAKIS and Tim FININ, 2016. Inferring relations in knowledge graphs with tensor decompositions. In: <em>2016 IEEE international conference on big data</em> [online]. IEEE, p. 4020–4022. Available at: doi:<a href="https://doi.org/10.1109/BigData.2016.7841096">10.1109/BigData.2016.7841096</a></p>
          </div>
          <div id="ref-Parycek2014">
          <p>PARYCEK, Peter, Johann HÖCHTL and Michael GINNER, 2014. Open government data implementation evaluation. <em>Journal of Theoretical and Applied Electronic Commerce Research</em> [online]. May 2014, <strong>9</strong>(2), 80–99. ISSN 0718–1876. Available at: doi:<a href="https://doi.org/10.4067/S0718-18762014000200007">10.4067/S0718-18762014000200007</a></p>
          </div>
          <div id="ref-Paton2016">
          <p>PATON, Norman W., Khalid BELHAJJAME, Suzanne M. EMBURY, Alvaro A. A. FERNANDES and Ruhaila MASKAT, 2016. Pay-as-you-go data integration: Experiences and recurring themes. In: Rūsiņš Mārtiņš FREIVALDS, Gregor ENGELS and Barbara CATANIA, eds. <em>SOFSEM 2016: Theory and practice of computer science: Proceedings of the 42<sup>nd</sup> international conference on current trends in theory and practice of computer science</em> [online]. Berlin; Heidelberg: Springer, p. 81–92. Lecture notes in computer science. Available at: doi:<a href="https://doi.org/10.1007/978-3-662-49192-8_7">10.1007/978-3-662-49192-8_7</a></p>
          </div>
          <div id="ref-Paton2012">
          <p>PATON, Norman W., Klitos CHRISTODOULOU, Alvaro A. A. FERNANDES, Bijan PARSIA and Cornelia HEDELER, 2012. Pay-as-you-go data integration for linked data: Opportunities, challenges and architectures. In: <em>Proceedings of the 4<sup>th</sup> international workshop on semantic web information management</em>. New York (NY): ACM. ISBN 978-1-4503-1446-6. </p>
          </div>
          <div id="ref-Presutti2016">
          <p>PRESUTTI, Valentina, Giorgia LODI, Andrea NUZZOLESE, Aldo GANGEMI, Silvio PERONI and Luigi ASPIRINO, 2016. The role of ontology design patterns in linked data projects. In: Isabelle COMYN-WATTIAU, Katsumi TANAKA, Il-Yeol SONG, Shuichiro YAMAMOTO and Motoshi SAEKI, eds. <em>Conceptual modelling: Proceedings of the 35<sup>th</sup> international conference, ER 2016</em> [online]. Berlin; Heidelberg: Springer, p. 113–121. Lecture notes in computer science. ISSN 0302-9743. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-46397-1_9">10.1007/978-3-319-46397-1_9</a></p>
          </div>
          <div id="ref-Presern2014">
          <p>PREŠERN, Mateja and Gašper ŽEJN, 2014. Supervizor: An indispensable open government application. In: <em>Share-psi 2.0 workshop on uses of open data within government for innovation and efficiency</em> [online]. [accessed 2017-01-03]. Available at: <a href="https://www.w3.org/2013/share-psi/wiki/images/6/6b/Supervizor_Slovenia_description_pdf.pdf" class="uri">https://www.w3.org/2013/share-psi/wiki/images/6/6b/Supervizor_Slovenia_description_pdf.pdf</a></p>
          </div>
          <div id="ref-Prudhommeaux2013">
          <p>PRUD’HOMMEAUX, Eric and Carlos BUIL-ARANDA, 2013. <em>SPARQL 1.1 federated query</em> [online]. W3C Recommendation. W3C [accessed 2017-03-05]. Available at: <a href="https://www.w3.org/TR/sparql11-federated-query" class="uri">https://www.w3.org/TR/sparql11-federated-query</a></p>
          </div>
          <div id="ref-Prudhommeaux2008">
          <p>PRUD’HOMMEAUX, Eric and Andy SEABORNE, 2008. <em>SPARQL Query Language for RDF</em> [online]. W3C Recommendation. W3C [accessed 2017-03-03]. Available at: <a href="https://www.w3.org/TR/rdf-sparql-query" class="uri">https://www.w3.org/TR/rdf-sparql-query</a></p>
          </div>
          <div id="ref-Radinger2013">
          <p>RADINGER, Andreas, Bene RODRIGUEZ-CASTRO, Alex STOLZ and Martin HEPP, 2013. BauDataWeb: The Austrian building and construction materials market as linked data. In: <em>Proceedings of the 9<sup>th</sup> international conference on semantic systems (I-SEMANTICS 2013)</em> [online]. New York (NY): ACM. ISBN 978-1-4503-1972-0/13/09. Available at: <a href="http://semantic.eurobau.com/BauDataWeb-ISEMANTICS2013.pdf" class="uri">http://semantic.eurobau.com/BauDataWeb-ISEMANTICS2013.pdf</a></p>
          </div>
          <div id="ref-Ricci2011">
          <p>RICCI, Francesco, Lior ROKACH, Bracha SHAPIRA and Paul B. KANTOR, eds., 2011. <em>Recommender systems handbook</em>. 1st ed. Springer. ISBN 978-0-387-85820-3. </p>
          </div>
          <div id="ref-Richter2013">
          <p>RICHTER, Michael M. and Rosina O. WEBER, 2013. <em>Case-based reasoning: A textbook</em>. Berlin; Heidelberg: Springer. ISBN 978-3-642-40167-1. </p>
          </div>
          <div id="ref-Ristoski2015">
          <p>RISTOSKI, Petar, Michael SCHUHMACHER and Heiko PAULHEIM, 2015. Using graph metrics for linked open data enabled recommender systems. In: Heiner STUCKENSCHMIDT and Dieter JANNACH, eds. <em>E-commerce and web technologies: 16<sup>th</sup> international conference on electronic commerce and web technologies, EC-Web 2015. revised selected papers</em> [online]. Berlin; Heidelberg: Springer, p. 30–41. Lecture notes in business information processing. Available at: doi:<a href="https://doi.org/10.1007/978-3-319-27729-5_3">10.1007/978-3-319-27729-5_3</a></p>
          </div>
          <div id="ref-Said2013">
          <p>SAID, Alan, Alejandro BELLOGÍN and Arjen P. DE VRIES, 2013. A top-n recommender system evaluation protocol inspired by deployed systems. In: <em>ACM RecSys 2013 workshop on large-scale recommender systems</em> [online]. Available at: <a href="http://graphlab.com/files/lsrs2013/paper_12.pdf" class="uri">http://graphlab.com/files/lsrs2013/paper_12.pdf</a></p>
          </div>
          <div id="ref-Salvadores2008">
          <p>SALVADORES, Manuel, Landong ZUO, S. M. HAZZAZ IMTIAZ, John DARLINGTON, Nicholas GIBBINS, Nigel SHADBOLT and James DOBREE, 2008. Market blended insight: Modeling propensity to buy with the semantic web. In: <em>The semantic web - iswc 2008: Proceedings of the 7<sup>th</sup> international semantic web conference</em> [online]. Berlin; Heidelberg: Springer, p. 777–789. Lecture notes in computer science. ISSN 0302-9743. Available at: doi:<a href="https://doi.org/10.1007/978-3-540-88564-1_50">10.1007/978-3-540-88564-1_50</a></p>
          </div>
          <div id="ref-Schmid1998">
          <p>SCHMID, Beat F. and Markus A. LINDEMANN, 1998. Elements of a reference model for electronic markets. In: <em>Proceedings of the 31<sup>st</sup> hawaii international conference on system sciences</em> [online]. p. 193–201. Available at: doi:<a href="https://doi.org/10.1109/HICSS.1998.655275">10.1109/HICSS.1998.655275</a></p>
          </div>
          <div id="ref-Seaborne2014">
          <p>SEABORNE, Andy, 2014. <em>SPARQL 1.1 property paths</em> [online]. W3C Working Draft. W3C [accessed 2017-05-19]. Available at: <a href="https://www.w3.org/TR/sparql11-property-paths" class="uri">https://www.w3.org/TR/sparql11-property-paths</a></p>
          </div>
          <div id="ref-Sidiropoulos2017">
          <p>SIDIROPOULOS, Nicholas D., Lieven DE LATHAUWER, Xiao FU, Kejun HUANG, Evangelos E. PAPALEXAKIS and Christos FALOUTSOS, 2017. Tensor decomposition for signal processing and machine learning. <em>IEEE Transactions on Signal Processing</em> [online]. July 2017, <strong>65</strong>(13), 3551–3582. ISSN 1053-587X. Available at: doi:<a href="https://doi.org/10.1109/TSP.2017.2690524">10.1109/TSP.2017.2690524</a></p>
          </div>
          <div id="ref-Smyth2007">
          <p>SMYTH, Barry, 2007. Case-based recommendation. In: Peter BRUSILOVSKY, Alfred KOBSA and Wolfgang NEJDL, eds. <em>The adaptive web</em> [online]. Berlin; Heidelberg: Springer, Lecture notes in computer science, p. 342–376. ISBN 978-3-540-72078-2. Available at: doi:<a href="https://doi.org/10.1007/978-3-540-72079-9_11">10.1007/978-3-540-72079-9_11</a></p>
          </div>
          <div id="ref-Snoha2013">
          <p>SNOHA, Matej, Jakub KLÍMEK and Jindřich MYNARZ, 2013. <em>Deliverable 9a.2.1: Prototype of matchmaking web services for linked commerce data in the domain of public sector contracts</em> [online] [accessed 2017-01-18]. Available at: <a href="http://svn.aksw.org/lod2/D9a.2.1/public.pdf" class="uri">http://svn.aksw.org/lod2/D9a.2.1/public.pdf</a></p>
          </div>
          <div id="ref-Soudek2016a">
          <p>SOUDEK, Jan, 2016a. <em>Journal data quality</em> [online] [accessed 2017-01-05]. Available at: <a href="http://wiki.zindex.cz/doku.php?id=en:kvalita_dat_ve_vestniku" class="uri">http://wiki.zindex.cz/doku.php?id=en:kvalita_dat_ve_vestniku</a></p>
          </div>
          <div id="ref-Soudek2016b">
          <p>SOUDEK, Jan, 2016b. <em>zIndex: Public contracting authorities rating</em> [online] [accessed 2017-01-05]. Available at: <a href="http://wiki.zindex.cz/doku.php?id=en:start#metodika_hodnoceni" class="uri">http://wiki.zindex.cz/doku.php?id=en:start#metodika_hodnoceni</a></p>
          </div>
          <div id="ref-Sporny2014">
          <p>SPORNY, Manu, Dave LONGLEY, Gregg KELLOGG, Markus LANTHALER and Niklas LINDSTRÖM, 2014. <em>JSON-LD 1.0: A JSON-based serialization for linked data</em> [online]. W3C Recommendation. W3C [accessed 2017-01-02]. Available at: <a href="http://www.w3.org/TR/json-ld" class="uri">http://www.w3.org/TR/json-ld</a></p>
          </div>
          <div id="ref-Stickler2005">
          <p>STICKLER, Patrick, 2005. <em>CBD: Concise bounded description</em> [online]. W3C Member Submission. W3C [accessed 2017-09-15]. Available at: <a href="https://www.w3.org/Submission/CBD" class="uri">https://www.w3.org/Submission/CBD</a></p>
          </div>
          <div id="ref-Szwabe2015">
          <p>SZWABE, Andrzej, Michal CIESIELCZYK, Pawel MISIOREK and Michal BLINKIEWICZ, 2015. Application of the tensor-based recommendation engine to semantic service matchmaking. In: <em>SEMAPRO 2015: The 9<sup>th</sup> international conference on advances in semantic processing</em>. IARIA, p. 116–125. ISBN 978-1-61208-420-6. </p>
          </div>
          <div id="ref-Thalhammer2012">
          <p>THALHAMMER, Andreas, 2012. Leveraging linked data analysis for semantic recommender systems. In: <em>The semantic web: Research and applications: Proceedings of the 9<sup>th</sup> extended semantic web conference</em> [online]. Berlin; Heidelberg: Springer, p. 823–827. ISBN 978-3-642-30283-1. Available at: doi:<a href="https://doi.org/10.1007/978-3-642-30284-8_64">10.1007/978-3-642-30284-8_64</a></p>
          </div>
          <div id="ref-Trastour2001">
          <p>TRASTOUR, David, Claudio BARTOLINI and Javier GONZALEZ-CASTILLO, 2011. <em>A semantic web approach to service description for matchmaking of services</em> [online]. Hewlett-Packard [accessed 2017-03-03]. Available at: <a href="https://fog.hpl.external.hp.com/techreports/2001/HPL-2001-183.pdf" class="uri">https://fog.hpl.external.hp.com/techreports/2001/HPL-2001-183.pdf</a></p>
          </div>
          <div id="ref-Tresp2014">
          <p>TRESP, Volker and Maximilian NICKEL, 2014. Relational models. In: Reda ALHAJJ and Jon ROKNE, eds. <em>Encyclopedia of social network analysis and mining</em> [online]. New York (NY): Springer, p. 1550–1561. ISBN 978-1-4614-6169-2. Available at: doi:<a href="https://doi.org/10.1007/978-1-4614-6170-8">10.1007/978-1-4614-6170-8</a></p>
          </div>
          <div id="ref-Valle2010">
          <p>VALLE, Francesco, Mathieu D’AQUIN, Tommaso DI NOIA and Enrico MOTTA, 2010. LOTED: Exploiting linked data in analyzing european procurement notices. In: Valentina PRESUTTI, Francois SCHARFFE and Vojtěch SVÁTEK, eds. <em>Proceedings of the 1<sup>st</sup> workshop on knowledge injection into and extraction from linked data</em> [online]. Aachen: RWTH Aachen University, p. 52–63. CEUR workshop proceedings. ISSN 1613-0073. Available at: <a href="http://ceur-ws.org/Vol-631/paper6.pdf" class="uri">http://ceur-ws.org/Vol-631/paper6.pdf</a></p>
          </div>
          <div id="ref-Veit2001">
          <p>VEIT, Daniel, Jörg P. MÜLLER, Martin SCHNEIDER and Björn FIEHN, 2001. Matchmaking for autonomous agents in electronic marketplaces. In: <em>Proceedings of the 5<sup>th</sup> international conference on autonomous agents</em> [online]. New York (NY): ACM, p. 65–66. ISBN 1-58113-326-X. Available at: doi:<a href="https://doi.org/10.1145/375735.375874">10.1145/375735.375874</a></p>
          </div>
          <div id="ref-W3C2012">
          <p>W3C OWL WORKING GROUP, 2012. <em>OWL 2 web ontology language: Document overview</em> [online]. 2nd ed. W3C Recommendation. [accessed 2017-02-17]. Available at: <a href="https://www.w3.org/TR/owl2-overview" class="uri">https://www.w3.org/TR/owl2-overview</a></p>
          </div>
          <div id="ref-Wieringa2014">
          <p>WIERINGA, Roel, 2014. <em>Design science methodology for information systems and software engineering</em> [online]. Berlin; Heidelberg: Springer. ISBN 978-3-662-43838-1. Available at: doi:<a href="https://doi.org/10.1007/978-3-662-43839-8">10.1007/978-3-662-43839-8</a></p>
          </div>
          <div id="ref-Wood2011">
          <p>WOOD, David, ed., 2011. <em>Linking government data</em>. Berlin; Heidelberg: Springer. ISBN 978-1-4614-1767-5. </p>
          </div>
          <div id="ref-Zhiltsov2013">
          <p>ZHILTSOV, Nikita and Eugene AGICHTEIN, 2013. Improving entity search over linked data by modeling latent semantics. In: <em>Proceedings of the 22<sup>nd</sup> ACM international conference on information &amp; knowledge management</em> [online]. New York (NY): ACM, p. 1253–1256. ISBN 978-1-4503-2263-8. Available at: doi:<a href="https://doi.org/10.1145/2505515.2507868">10.1145/2505515.2507868</a></p>
          </div>
          </div>
          
          </section>
          <section id="sec:software" class="level1">
          <h1><span class="header-section-number">6</span> Software</h1>
          <p>The work described in this dissertation involved many software tools. In order to provide a single reference point for these tools, this appendix lists their brief descriptions. Here we describe both the software used for data preparation as well as the software for matchmaking. The descriptions are divided into two categories: software that we reused and software that we developed. Descriptions in each category are sorted in alphabetic order.</p>
          <section id="reused-software" class="level2">
          <h2><span class="header-section-number">6.1</span> Reused software</h2>
          <p>Several tools were reused as is or integrated with other tools. The software listed in this category comprises mostly database systems or data processing tools. <!-- TODO: Should we add Mustache and Graphviz as the reused software? --></p>
          <section id="elasticsearch" class="level3">
          <h3><span class="header-section-number">6.1.1</span> Elasticsearch</h3>
          <p>Elasticsearch (ES)<a href="#fn93" class="footnoteRef" id="fnref93"><sup>93</sup></a> is an open source full-text search engine based on Apache Lucene.<a href="#fn94" class="footnoteRef" id="fnref94"><sup>94</sup></a> ES indexes JSON documents that can be searched via ES query DSL exposed through an HTTP API. The query DSL is an expressive query language based on JSON. The DSL allows to search for terms in full texts and match patterns in the indexed data structures. Simple queries can be combined into complex ones using boolean operators. Besides the basic search operations, ES features high-level query types, such as the More Like This query, which supports similarity-based retrieval. Since ES queries are represented as structured data in JSON, they can be generated easily. <!--
          ES indices can be distributed across several nodes, each hosting a number of index shards.
          The cluster architecture of ES supports horizontal scaling by adding nodes.
          --></p>
          </section>
          <section id="geotools" class="level3">
          <h3><span class="header-section-number">6.1.2</span> GeoTools</h3>
          <p>GeoTools<a href="#fn95" class="footnoteRef" id="fnref95"><sup>95</sup></a> is an open source Java library for working with geospatial data. Its implementation complies with standards of the Open Geospatial Consortium (OGC). For example, it supports reprojection of coordinate data between standard coordinate reference systems.</p>
          </section>
          <section id="linkedpipes-etl" class="level3">
          <h3><span class="header-section-number">6.1.3</span> LinkedPipes-ETL</h3>
          <p>LinkedPipes-ETL (LP-ETL) <span class="citation" data-cites="Klimek2016">(Klímek et al. <a href="#ref-Klimek2016">2016</a>)</span><a href="#fn96" class="footnoteRef" id="fnref96"><sup>96</sup></a> is an open source data processing tool for converting diverse data sources to RDF and performing various transformations of RDF data. LP-ETL follows the workflow of Extract-Transform-Load (ETL). For each ETL phase it offers components to dedicated to specific data processing tasks. For example, an extraction component can download data from given URL, a transformation component can decompress a ZIP archive, and a load component can write data to a file. The components can be composed into pipelines that automate potentially complex data processing workflows. The design of LP-ETL evolved from UnifiedViews and in many respects it can be considered a successor to this project.</p>
          </section>
          <section id="openlink-virtuoso" class="level3">
          <h3><span class="header-section-number">6.1.4</span> OpenLink Virtuoso</h3>
          <p>OpenLink Virtuoso<a href="#fn97" class="footnoteRef" id="fnref97"><sup>97</sup></a> is an RDF store that implements SPARQL and a plethora of additional functionality for working with RDF data. A notable characteristic of Virtuoso is its column-wise storage enabling vectored query execution <span class="citation" data-cites="Erling2012">(Erling <a href="#ref-Erling2012">2012</a>)</span>, which gives Virtuoso a good query performance that scales well to large RDF datasets. Virtuoso offers an open source version that lacks some of the features in the commercial version.</p>
          </section>
          <section id="sec:rescal-software" class="level3">
          <h3><span class="header-section-number">6.1.5</span> RESCAL</h3>
          <p>RESCAL <span class="citation" data-cites="Nickel2011">(Nickel et al. <a href="#ref-Nickel2011">2011</a>)</span> is a tensor factorization technique for relational data modelled as three-way tensors. It has an open source implementation written in Python using the NumPy<a href="#fn98" class="footnoteRef" id="fnref98"><sup>98</sup></a> and SciPy<a href="#fn99" class="footnoteRef" id="fnref99"><sup>99</sup></a> modules for low-level matrix operations. RESCAL achieves superior performance on factorization of large sparse tensors, while having a fundamentally simpler implementation than other tensor factorization techniques.</p>
          </section>
          <section id="saxon-xslt-and-xquery-processor" class="level3">
          <h3><span class="header-section-number">6.1.6</span> Saxon XSLT and XQuery Processor</h3>
          <p>Saxon XSLT and XQuery Processor<a href="#fn100" class="footnoteRef" id="fnref100"><sup>100</sup></a> is an implementation of several W3C standards for processing XML data including XSLT, XQuery, and XPath. It can transform XML data via XSLT stylesheets or query it via XQuery and XPath. The limited Saxon-HE version is available as open source.</p>
          </section>
          <section id="silk-link-discovery-framework" class="level3">
          <h3><span class="header-section-number">6.1.7</span> Silk Link Discovery Framework</h3>
          <p>Silk <span class="citation" data-cites="Bryl2014">(Bryl et al. <a href="#ref-Bryl2014">2014</a>)</span><a href="#fn101" class="footnoteRef" id="fnref101"><sup>101</sup></a> is an open source link discovery framework for instance matching. It offers an extensive arsenal of similarity measures and combination functions for aggregating similarity scores. Silk generates links by executing declarative linkage specifications that describe how to compare resources in the source and target datasets to discover matches. As an alternative to explicit linkage specifications, Silk supports active learning from examples of valid links <span class="citation" data-cites="Isele2013">(Isele and Bizer <a href="#ref-Isele2013">2013</a>)</span>. Resources to interlink can be retrieved from SPARQL endpoints and RDF or CSV files. Silk thus supports integration of heterogeneous data by materializing explicit links across the integrated data sources.</p>
          </section>
          <section id="tarql" class="level3">
          <h3><span class="header-section-number">6.1.8</span> Tarql</h3>
          <p>Tarql<a href="#fn102" class="footnoteRef" id="fnref102"><sup>102</sup></a> is an open source tool for converting CSV to RDF via SPARQL CONSTRUCT queries. It extends the query engine of Apache Jena<a href="#fn103" class="footnoteRef" id="fnref103"><sup>103</sup></a> such that each CSV row is provided as inline data to the SPARQL query provided by the user. Queries can thus refer to tabular data via query variables based on column names in the source CSV. Instead of resorting to custom-coded conversion scripts, such setup enables to harness the expressivity of SPARQL as a native RDF data manipulation language.</p>
          </section>
          <section id="unifiedviews" class="level3">
          <h3><span class="header-section-number">6.1.9</span> UnifiedViews</h3>
          <p>UnifiedViews <span class="citation" data-cites="Knap2017">(Knap et al. <a href="#ref-Knap2017">2017</a>)</span><a href="#fn104" class="footnoteRef" id="fnref104"><sup>104</sup></a> is an open source ETL framework with native support for RDF. It allows to execute data processing tasks, monitor progress of their execution, debug failed executions, and schedule periodic tasks. Concrete data processing workflows can be implemented as pipelines that combine pre-made data processing units. Each unit is responsible for a data processing step, such as applying an XSL transformation or loading metadata into a data catalogue. UnifiedViews has been in development from the year 2013 and it can be considered relatively stable, as it has been deployed to address many use cases since.</p>
          </section>
          </section>
          <section id="developed-software" class="level2">
          <h2><span class="header-section-number">6.2</span> Developed software</h2>
          <p>In order to cover the needs that were not sufficiently addressed by existing software we developed new reusable tools. Most of these tools were implemented in Clojure, with the exception of matchmaker-rescal that was written in Python due to its dependency on RESCAL. All the developed tools expose simple command-line interfaces and are released as open source under the terms of the Eclipse Public License 1.0.</p>
          <section id="sec:discretize-sparql" class="level3">
          <h3><span class="header-section-number">6.2.1</span> discretize-sparql</h3>
          <p>discretize-sparql<a href="#fn105" class="footnoteRef" id="fnref105"><sup>105</sup></a> allows to discretize numeric literals in RDF data exposed via a SPARQL 1.1 Update <span class="citation" data-cites="Gearon2013">(Gearon et al. <a href="#ref-Gearon2013">2013</a>)</span> endpoint. Discretization groups continuous numeric values into discrete intervals. It is typically used for pre-processing continuous data for machine learning tools that support only categorical variables. A discretization task in discretize-sparql is formulated as a SPARQL 1.1 Update operation that uses pre-defined variable names endowed with specific interpretation. The WHERE clause of the operation must contain the variable <code>?value</code> that selects the values to discretize. The variable <code>?interval</code> will be bound to the intervals generated by the tool. Consequently, the update operation can be formulated as if it contained a mapping from the values to discretize to intervals. For instance, <code>?interval</code> can be used in the INSERT clause and <code>?value</code> in the DELETE clause in order to replace the values to discretize with intervals. The provided update operation will be rewritten first to a SELECT query to retrieve the values to discretize and then to an update operation including the actual mapping from numeric literals to intervals.</p>
          </section>
          <section id="elasticsearch-geocoding" class="level3">
          <h3><span class="header-section-number">6.2.2</span> elasticsearch-geocoding</h3>
          <p>elasticsearch-geocoding<a href="#fn106" class="footnoteRef" id="fnref106"><sup>106</sup></a> is a tool for geocoding postal addresses via Elasticsearch (ES). It uses an ES index seeded with reference addresses to which it matches the addresses to geocode. Such an index can be prepared by using sparql-to-jsonld to convert RDF into JSON, followed by jsonld-to-elasticsearch to upload the JSON into ES. The tool loads the addresses to geocode from a SPARQL endpoint using a given SPARQL SELECT query that produces tabular data with column names recognized by this tool for the components of addresses, such as postal codes or house numbers. For each address an ES query is generated to find matching reference addresses. Geo-coordinates of the best ranking result for each query are output as RDF serialized in the N-Triples syntax.</p>
          </section>
          <section id="jsonld-to-elasticsearch" class="level3">
          <h3><span class="header-section-number">6.2.3</span> jsonld-to-elasticsearch</h3>
          <p>jsonld-to-elasticsearch<a href="#fn107" class="footnoteRef" id="fnref107"><sup>107</sup></a> indexes Newline Delimited JSON (NDJSON) in Elasticsearch (ES). Each input line represents a JSON document that is analysed and bulk-indexed in ES using a provided mapping. The mapping specifies a schema that instructs ES how to store and index the individual attributes in the input documents. If the input JSON-LD contains the <code>@context</code> attribute, it is removed due to being redundant. Each JSON-LD document must contain the <code>@id</code> attribute, which used as the document identifier in ES. RDF data can be prepared into this expected format using sparql-to-jsonld.</p>
          </section>
          <section id="sec:matchmaker-sparql" class="level3">
          <h3><span class="header-section-number">6.2.4</span> matchmaker-sparql</h3>
          <p>matchmaker-sparql<a href="#fn108" class="footnoteRef" id="fnref108"><sup>108</sup></a> is a command-line application for evaluation of SPARQL-based matchmaking. The evaluation setup is guided by a configuration file provided to the application. The configuration describes the data to use, connection to a SPARQL endpoint to query and update the data, parameters of the matchmaker, and the evaluation protocol for the n-fold cross-validation.</p>
          <!--
          ### matchmaker-elasticsearch
          -->
          </section>
          <section id="sec:matchmaker-rescal" class="level3">
          <h3><span class="header-section-number">6.2.5</span> matchmaker-rescal</h3>
          <p>matchmaker-rescal<a href="#fn109" class="footnoteRef" id="fnref109"><sup>109</sup></a> is a command-line application that wraps the original implementation of RESCAL in Python. It serves as an exploratory tool for experimentation with RESCAL-based matchmaking. The sole purpose of the tool is to evaluate link prediction for a given relation using cross-validation and the metrics defined in the sec. <a href="#sec:evaluated-metrics">4.3</a>. Its input consists of the ground truth matrix encoding the relation to predict, additional matrices encoding other relations, and configuration with hyper-parameters for RESCAL. The matrices required as input by this tool can be prepared by sparql-to-tensor.</p>
          </section>
          <section id="sparql-to-csv" class="level3">
          <h3><span class="header-section-number">6.2.6</span> sparql-to-csv</h3>
          <p>sparql-to-csv<a href="#fn110" class="footnoteRef" id="fnref110"><sup>110</sup></a> allows to save results of SPARQL queries into CSV. It is primarily intended to support data preparation for analyses that require tabular input. It has two main modes of operation: paged queries and piped queries. In both cases it generates SPARQL queries from Mustache<a href="#fn111" class="footnoteRef" id="fnref111"><sup>111</sup></a> templates, which enables to parameterize the queries. The mode of paged queries splits the provided SELECT query into queries that retrieve partial results delimited by <code>LIMIT</code> and <code>OFFSET</code>, so that demanding queries that produce many results can be executed without running into the load restrictions imposed by the queried RDF stores, such as timeouts or maximum result sizes. The mode of piped queries allows using Unix pipes to chain execution of several dependent SPARQL queries. In this mode, each solution in results of a query is bound as variables for the template that generates the subsequent query. Such approach facilitates decomposition of complex queries into a chain of simpler queries that do not strain the queried RDF store. It also enables to query a SPARQL endpoint using data from another SPARQL endpoint, in a similar manner to federated queries <span class="citation" data-cites="Prudhommeaux2013">(Prud’hommeaux and Buil-Aranda <a href="#ref-Prudhommeaux2013">2013</a>)</span>. Alternatively, the query results can be piped to a template producing SPARQL Update operations, so that complex data transformations can be divided into simpler subtasks.</p>
          </section>
          <section id="sparql-to-graphviz" class="level3">
          <h3><span class="header-section-number">6.2.7</span> sparql-to-graphviz</h3>
          <p>sparql-to-graphviz<a href="#fn112" class="footnoteRef" id="fnref112"><sup>112</sup></a> generates a class diagram representing empirical schema of RDF data exposed via a SPARQL endpoint. The empirical schema reflects the structure of instance data in terms of its vocabularies, so that instead of representing the structures prescribed by vocabularies (e.g., <code>rdfs:domain</code> and <code>rdfs:range</code>), it mirrors the way vocabularies are used in instance data (e.g., actual links between resources). In this way, it supports exploration of unknown data that may not necessarily conform to the expectations set by its vocabularies. In order to separate concerns, in place of producing a visualization, the tool generates a description of the schema in the DOT language.<a href="#fn113" class="footnoteRef" id="fnref113"><sup>113</sup></a> The description can be then turned into an image using Graphviz,<a href="#fn114" class="footnoteRef" id="fnref114"><sup>114</sup></a> an established visualization tool that offers several algorithms for constructing graph layouts. Instead of producing a bitmap image, a vector image in SVG can be generated, which lends itself to further manual post-production to perfect the visualization.</p>
          </section>
          <section id="sparql-to-jsonld" class="level3">
          <h3><span class="header-section-number">6.2.8</span> sparql-to-jsonld</h3>
          <p>sparql-to-jsonld<a href="#fn115" class="footnoteRef" id="fnref115"><sup>115</sup></a> retrieves RDF data from a SPARQL endpoint and serializes it to JSON-LD documents. It starts by fetching a list of IRIs of resources selected by a provided SPARQL SELECT query. The query allows to filter the resources of interest, such as instances of a given class. For each resource a user-defined SPARQL CONSTRUCT or DESCRIBE query is executed. This query selects or constructs features that describe the resource. Both SPARQL queries are provided as Mustache<a href="#fn116" class="footnoteRef" id="fnref116"><sup>116</sup></a> templates to allow parametrization. Each retrieved description in RDF is converted to JSON-LD and transformed via a provided JSON-LD frame that coerces the input RDF graph into a predictable JSON tree. The output is appended to a file that is serialized as Newline Delimited JSON (NDJSON).</p>
          </section>
          <section id="sec:sparql-to-tensor" class="level3">
          <h3><span class="header-section-number">6.2.9</span> sparql-to-tensor</h3>
          <p>sparql-to-tensor<a href="#fn117" class="footnoteRef" id="fnref117"><sup>117</sup></a> exports RDF data from SPARQL endpoints to tensors. The tensors are represented as a collection of frontal slices serialized as sparse matrices in the MatrixMarket coordinate format.<a href="#fn118" class="footnoteRef" id="fnref118"><sup>118</sup></a> IRIs of the tensor entities are written to a <code>headers.txt</code> file. Each IRI is written on a separate line, so that line numbers can be used as indices of the entities in the matrices. The header can thus be used to translate the matrices to IRIs of RDF resources. The output of <em>sparql-to-tensor</em> complies with the format used by the Web of Needs RESCAL matchmaker <span class="citation" data-cites="Friedrich2015">(Friedrich <a href="#ref-Friedrich2015">2015</a>)</span>.</p>
          <p>Tensors are constructed from results of SPARQL SELECT queries provided to the tool by the user. Each query must project several variables with pre-defined interpretation. The <code>?feature</code> variable determines the tensor slice. It typically corresponds to an RDF property, but it can also represent a feature constructed from the source RDF data. The <code>?s</code> variable is an entity that is a subject of the feature, and the <code>?o</code> variable is its object. An optional variable <code>?weight</code> can indicate the weight of the relation between the entities. It is a decimal number from the interval <span class="math inline">\(\left[0, 1\right]\)</span>, with the default value being 1. The SELECT queries must be provided as Mustache<a href="#fn119" class="footnoteRef" id="fnref119"><sup>119</sup></a> templates that allows to retrieve results via pages delimited by <code>LIMIT</code> and <code>OFFSET</code>. Support of multiple queries allows to separate concerns and write simpler queries for the individual features.</p>
          </section>
          <section id="sparql-unlimited" class="level3">
          <h3><span class="header-section-number">6.2.10</span> sparql-unlimited</h3>
          <p>sparql-unlimited<a href="#fn120" class="footnoteRef" id="fnref120"><sup>120</sup></a> can execute SPARQL Update operations that affect many resources by running multiple updates that affect successive subsets of these resources. The input SPARQL Update operation must be provided as a Mustache<a href="#fn121" class="footnoteRef" id="fnref121"><sup>121</sup></a> template that contains a variable for the <code>LIMIT</code> to indicate the size of the subset to process. Operations rendered from this template are executed repeatedly until the requested SPARQL endpoint responds with a message reporting that no data was modified. In order to avoid repeating to process the same subsets of data, either an <code>OFFSET</code> variable can be provided, which is incremented by the limit in each step, or the update operation itself can directly filter out the already processed bindings. The second approach is preferable since it avoids sorting a potentially large list of resources affected by the update operation. Due to its stopping condition, the tool can be used only for update operations that eventually converge to a state, when there is no more data to modify. Since there is no standard way for SPARQL endpoints to respond that no data was modified by a received update operation, the tool relies on the way Virtuoso responds, which makes it usable only with this RDF store.</p>
          </section>
          <section id="vocab-to-graphviz" class="level3">
          <h3><span class="header-section-number">6.2.11</span> vocab-to-graphviz</h3>
          <p>vocab-to-graphviz<a href="#fn122" class="footnoteRef" id="fnref122"><sup>122</sup></a> visualizes RDF vocabularies via Graphviz.<a href="#fn123" class="footnoteRef" id="fnref123"><sup>123</sup></a> It converts an input vocabulary from an RDF file to a description of a class diagram in the DOT language.<a href="#fn124" class="footnoteRef" id="fnref124"><sup>124</sup></a> This description can be subsequently rendered to an image via Graphviz. The generated class diagram captures the relations between the vocabulary’s terms as defined by its schema axioms, such as <code>rdfs:domain</code> or <code>rdfs:range</code>. It thus functions in a way similar to sparql-to-graphviz.</p>
          
          </section>
          </section>
          </section>
          <section id="sec:abbreviations" class="level1 unnumbered">
          <h1>Abbreviations</h1>
          <dl>
          <dt>AAA</dt>
          <dd>Anyone can say anything about anything
          </dd>
          <dt>API</dt>
          <dd>Application Programming Interface
          </dd>
          <dt>ARES</dt>
          <dd>Access to Registers of Economic Subjects/Entities
          </dd>
          <dt>BR</dt>
          <dd>Business Register
          </dd>
          <dt>C4N</dt>
          <dd>Call for Anything
          </dd>
          <dt>CC</dt>
          <dd>Catalog coverage
          </dd>
          <dt>CBR</dt>
          <dd>Case-based reasoning
          </dd>
          <dt>CLI</dt>
          <dd>Command-Line Interface
          </dd>
          <dt>CPC</dt>
          <dd>Central Product Classification
          </dd>
          <dt>CPV</dt>
          <dd>Common Procurement Vocabulary
          </dd>
          <dt>CRS</dt>
          <dd>Coordinate Reference System
          </dd>
          <dt>CSO</dt>
          <dd>Czech Statistical Office
          </dd>
          <dt>CSV</dt>
          <dd>Comma-Separated Values
          </dd>
          <dt>CZK</dt>
          <dd>Czech koruna
          </dd>
          <dt>DAML</dt>
          <dd>DARPA Agent Markup Language
          </dd>
          <dt>DIKE</dt>
          <dd>Department of Information and Knowledge Engineering
          </dd>
          <dt>DL</dt>
          <dd>Description logics
          </dd>
          <dt>DPU</dt>
          <dd>Data Processing Unit
          </dd>
          <dt>DSL</dt>
          <dd>Domain-Specific Language
          </dd>
          <dt>ECB</dt>
          <dd>European Central Bank
          </dd>
          <dt>ELT</dt>
          <dd>Extract Load Transform
          </dd>
          <dt>EPSG</dt>
          <dd>European Petroleum Survey Group
          </dd>
          <dt>ES</dt>
          <dd>Elasticsearch
          </dd>
          <dt>ETL</dt>
          <dd>Extract Transform Load
          </dd>
          <dt>EU</dt>
          <dd>European Union
          </dd>
          <dt>EUR</dt>
          <dd>Euro
          </dd>
          <dt>GDP</dt>
          <dd>Gross Domestic Product
          </dd>
          <dt>GPA</dt>
          <dd>Agreement on Government Procurement
          </dd>
          <dt>HR</dt>
          <dd>Hit rate
          </dd>
          <dt>HTML</dt>
          <dd>Hypertext Markup Language
          </dd>
          <dt>HTTP</dt>
          <dd>Hypertext Transfer Protocol
          </dd>
          <dt>IDF</dt>
          <dd>Inverse document frequency
          </dd>
          <dt>IRI</dt>
          <dd>Internationalized Resource Identifier
          </dd>
          <dt>JSON</dt>
          <dd>JavaScript Object Notation
          </dd>
          <dt>JSON-LD</dt>
          <dd>JSON for Linked Data
          </dd>
          <dt>KQML</dt>
          <dd>Knowledge Query and Manipulation Language
          </dd>
          <dt>LCWA</dt>
          <dd>Local closed world assumption
          </dd>
          <dt>LOD</dt>
          <dd>Linked open data
          </dd>
          <dt>LOTED</dt>
          <dd>Linked Open TED
          </dd>
          <dt>LP-ETL</dt>
          <dd>LinkedPipes-ETL
          </dd>
          <dt>LTP</dt>
          <dd>Long-tail percentage
          </dd>
          <dt>MOLDEAS</dt>
          <dd>Methods on linked data for e-procurement applying semantics
          </dd>
          <dt>MRR</dt>
          <dd>Mean reciprocal rank
          </dd>
          <dt>NACE</dt>
          <dd>Statistical Classification of Economic Activities in the European Community
          </dd>
          <dt>NDJSON</dt>
          <dd>Newline Delimited JSON
          </dd>
          <dt>Non-UNA</dt>
          <dd>Non-unique name assumption
          </dd>
          <dt>NUTS</dt>
          <dd>Nomenclature of Territorial Units for Statistics
          </dd>
          <dt>OGC</dt>
          <dd>Open Geospatial Consortium
          </dd>
          <dt>OIL</dt>
          <dd>Ontology Inference Layer
          </dd>
          <dt>OWA</dt>
          <dd>Open world assumption
          </dd>
          <dt>OWL</dt>
          <dd>Web Ontology Language
          </dd>
          <dt>PC</dt>
          <dd>Prediction coverage
          </dd>
          <dt>PCO</dt>
          <dd>Public Contracts Ontology
          </dd>
          <dt>PPROC</dt>
          <dd>Public Procurement Ontology
          </dd>
          <dt>PR</dt>
          <dd>Public Register
          </dd>
          <dt>PTO</dt>
          <dd>Product Types Ontology
          </dd>
          <dt>RDF</dt>
          <dd>Resource Description Framework
          </dd>
          <dt>RDFS</dt>
          <dd>RDF Schema
          </dd>
          <dt>RDF/XML</dt>
          <dd>RDF 1.1 XML Syntax
          </dd>
          <dt>RN</dt>
          <dd>Registered Identification Number
          </dd>
          <dt>RÚIAN</dt>
          <dd>Registry of territorial identification, addresses, and real estate
          </dd>
          <dt>S-JTSK</dt>
          <dd>Systém Jednotné trigonometrické sítě katastrální
          </dd>
          <dt>SKOS</dt>
          <dd>Simple Knowledge Organization System
          </dd>
          <dt>SPARQL</dt>
          <dd>SPARQL Protocol and RDF Query Language
          </dd>
          <dt>SRL</dt>
          <dd>Statistical relational learning
          </dd>
          <dt>SVG</dt>
          <dd>Scalable Vector Graphics
          </dd>
          <dt>TED</dt>
          <dd>Tenders Electronic Daily
          </dd>
          <dt>TF-IDF</dt>
          <dd>Term frequency-inverse document frequency
          </dd>
          <dt>TLR</dt>
          <dd>Trade Licensing Register
          </dd>
          <dt>URL</dt>
          <dd>Uniform Resource Locator
          </dd>
          <dt>W3C</dt>
          <dd>World Wide Web Consortium
          </dd>
          <dt>WGS84</dt>
          <dd>World Geodetic System
          </dd>
          <dt>XLS</dt>
          <dd>Excel Binary File Format
          </dd>
          <dt>XML</dt>
          <dd>Extensible Markup Language
          </dd>
          <dt>XSL</dt>
          <dd>Extensible Stylesheet Language
          </dd>
          <dt>XSLT</dt>
          <dd>XSL Transformations
          </dd>
          </dl>
          
          </section>
          <section class="footnotes">
          <hr />
          <ol>
          <li id="fn1"><p><a href="http://opendefinition.org" class="uri">http://opendefinition.org</a><a href="#fnref1">↩</a></p></li>
          <li id="fn2"><p><a href="http://opendata.cz" class="uri">http://opendata.cz</a><a href="#fnref2">↩</a></p></li>
          <li id="fn3"><p><a href="https://www.w3.org/standards/semanticweb" class="uri">https://www.w3.org/standards/semanticweb</a><a href="#fnref3">↩</a></p></li>
          <li id="fn4"><p><a href="http://simap.ted.europa.eu/web/simap/standard-forms-for-public-procurement" class="uri">http://simap.ted.europa.eu/web/simap/standard-forms-for-public-procurement</a><a href="#fnref4">↩</a></p></li>
          <li id="fn5"><p><a href="http://ted.europa.eu" class="uri">http://ted.europa.eu</a><a href="#fnref5">↩</a></p></li>
          <li id="fn6"><p><a href="https://www.wto.org/english/tratop_e/gproc_e/gp_gpa_e.htm" class="uri">https://www.wto.org/english/tratop_e/gproc_e/gp_gpa_e.htm</a><a href="#fnref6">↩</a></p></li>
          <li id="fn7"><p>Example queries are available at <a href="http://www.ebusiness-unibw.org/tools/baudataweb-queries" class="uri">http://www.ebusiness-unibw.org/tools/baudataweb-queries</a>.<a href="#fnref7">↩</a></p></li>
          <li id="fn8"><p>FreeClassOWL is an RDF version of <a href="http://freeclass.eu" class="uri">http://freeclass.eu</a>.<a href="#fnref8">↩</a></p></li>
          <li id="fn9"><p><a href="http://rd.10ders.net" class="uri">http://rd.10ders.net</a><a href="#fnref9">↩</a></p></li>
          <li id="fn10"><p><a href="http://www.productontology.org" class="uri">http://www.productontology.org</a><a href="#fnref10">↩</a></p></li>
          <li id="fn11"><p><a href="https://euroalert.net" class="uri">https://euroalert.net</a><a href="#fnref11">↩</a></p></li>
          <li id="fn12"><p><a href="http://ted.europa.eu" class="uri">http://ted.europa.eu</a><a href="#fnref12">↩</a></p></li>
          <li id="fn13"><p><a href="https://jena.apache.org" class="uri">https://jena.apache.org</a><a href="#fnref13">↩</a></p></li>
          <li id="fn14"><p><a href="http://lod2.eu" class="uri">http://lod2.eu</a><a href="#fnref14">↩</a></p></li>
          <li id="fn15"><p><a href="http://lucene.apache.org/solr" class="uri">http://lucene.apache.org/solr</a><a href="#fnref15">↩</a></p></li>
          <li id="fn16"><p><a href="http://vocab.deri.ie/c4n" class="uri">http://vocab.deri.ie/c4n</a><a href="#fnref16">↩</a></p></li>
          <li id="fn17"><p><a href="http://wiki.goodrelations-vocabulary.org/Cookbook/Seeks" class="uri">http://wiki.goodrelations-vocabulary.org/Cookbook/Seeks</a><a href="#fnref17">↩</a></p></li>
          <li id="fn18"><p><a href="https://opencorporates.com" class="uri">https://opencorporates.com</a><a href="#fnref18">↩</a></p></li>
          <li id="fn19"><p><a href="http://lucene.apache.org" class="uri">http://lucene.apache.org</a><a href="#fnref19">↩</a></p></li>
          <li id="fn20"><p><a href="https://github.com/jindrichmynarz/vvz-to-rdf" class="uri">https://github.com/jindrichmynarz/vvz-to-rdf</a><a href="#fnref20">↩</a></p></li>
          <li id="fn21"><p><a href="https://www.vestnikverejnychzakazek.cz" class="uri">https://www.vestnikverejnychzakazek.cz</a><a href="#fnref21">↩</a></p></li>
          <li id="fn22"><p><a href="https://github.com/opendatacz/public-contracts-ontology" class="uri">https://github.com/opendatacz/public-contracts-ontology</a><a href="#fnref22">↩</a></p></li>
          <li id="fn23"><p><a href="http://dublincore.org/documents/dcmi-terms" class="uri">http://dublincore.org/documents/dcmi-terms</a><a href="#fnref23">↩</a></p></li>
          <li id="fn24"><p><a href="http://www.heppnetz.de/ontologies/goodrelations/v1.html" class="uri">http://www.heppnetz.de/ontologies/goodrelations/v1.html</a><a href="#fnref24">↩</a></p></li>
          <li id="fn25"><p><a href="http://aksw.org/Projects/LOD2.html" class="uri">http://aksw.org/Projects/LOD2.html</a><a href="#fnref25">↩</a></p></li>
          <li id="fn26"><p><a href="https://www.w3.org/TR/skos-primer/#secconceptcoordination" class="uri">https://www.w3.org/TR/skos-primer/#secconceptcoordination</a><a href="#fnref26">↩</a></p></li>
          <li id="fn27"><p><a href="http://unstats.un.org/unsd/cr/registry/cpc-2.asp" class="uri">http://unstats.un.org/unsd/cr/registry/cpc-2.asp</a><a href="#fnref27">↩</a></p></li>
          <li id="fn28"><p><a href="http://simap.ted.europa.eu/standard-forms-for-public-procurement" class="uri">http://simap.ted.europa.eu/standard-forms-for-public-procurement</a><a href="#fnref28">↩</a></p></li>
          <li id="fn29"><p><a href="http://www.isvz.cz/ISVZ/Podpora/ISVZ_open_data_vz.aspx" class="uri">http://www.isvz.cz/ISVZ/Podpora/ISVZ_open_data_vz.aspx</a><a href="#fnref29">↩</a></p></li>
          <li id="fn30"><p><a href="https://virtuoso.openlinksw.com" class="uri">https://virtuoso.openlinksw.com</a><a href="#fnref30">↩</a></p></li>
          <li id="fn31"><p><a href="https://jena.apache.org/documentation/io" class="uri">https://jena.apache.org/documentation/io</a><a href="#fnref31">↩</a></p></li>
          <li id="fn32"><p><a href="https://github.com/jindrichmynarz/sparql-to-graphviz" class="uri">https://github.com/jindrichmynarz/sparql-to-graphviz</a><a href="#fnref32">↩</a></p></li>
          <li id="fn33"><p><a href="http://www.graphviz.org" class="uri">http://www.graphviz.org</a><a href="#fnref33">↩</a></p></li>
          <li id="fn34"><p>See the section <em>“Example: Prevent Limits of Sorted LIMIT/OFFSET query”</em> in <a href="http://docs.openlinksw.com/virtuoso/rdfsparqlimplementationextent" class="uri">http://docs.openlinksw.com/virtuoso/rdfsparqlimplementationextent</a> for more details.<a href="#fnref34">↩</a></p></li>
          <li id="fn35"><p><a href="https://github.com/jindrichmynarz/sparql-unlimited" class="uri">https://github.com/jindrichmynarz/sparql-unlimited</a><a href="#fnref35">↩</a></p></li>
          <li id="fn36"><p><a href="https://mustache.github.io" class="uri">https://mustache.github.io</a><a href="#fnref36">↩</a></p></li>
          <li id="fn37"><p>We consider subordinate resources without inbound links as orphaned.<a href="#fnref37">↩</a></p></li>
          <li id="fn38"><p><a href="https://github.com/openlink/virtuoso-opensource/issues/415" class="uri">https://github.com/openlink/virtuoso-opensource/issues/415</a><a href="#fnref38">↩</a></p></li>
          <li id="fn39"><p><a href="https://www.ecb.europa.eu/stats/exchange/eurofxref/html/index.en.html" class="uri">https://www.ecb.europa.eu/stats/exchange/eurofxref/html/index.en.html</a><a href="#fnref39">↩</a></p></li>
          <li id="fn40"><p><a href="https://github.com/openbudgets/datasets/tree/master/ecb-exchange-rates" class="uri">https://github.com/openbudgets/datasets/tree/master/ecb-exchange-rates</a><a href="#fnref40">↩</a></p></li>
          <li id="fn41"><p><a href="http://openbudgets.eu" class="uri">http://openbudgets.eu</a><a href="#fnref41">↩</a></p></li>
          <li id="fn42"><p><a href="https://github.com/jindrichmynarz/sparql-to-csv" class="uri">https://github.com/jindrichmynarz/sparql-to-csv</a><a href="#fnref42">↩</a></p></li>
          <li id="fn43"><p><a href="https://en.wikipedia.org/wiki/Winsorizing" class="uri">https://en.wikipedia.org/wiki/Winsorizing</a><a href="#fnref43">↩</a></p></li>
          <li id="fn44"><p><a href="https://www.elastic.co/products/elasticsearch" class="uri">https://www.elastic.co/products/elasticsearch</a><a href="#fnref44">↩</a></p></li>
          <li id="fn45"><p><a href="http://ruian.linked.opendata.cz:8890/sparql" class="uri">http://ruian.linked.opendata.cz:8890/sparql</a><a href="#fnref45">↩</a></p></li>
          <li id="fn46"><p><a href="https://developers.google.com/maps/documentation/geocoding/policies#map" class="uri">https://developers.google.com/maps/documentation/geocoding/policies#map</a><a href="#fnref46">↩</a></p></li>
          <li id="fn47"><p><a href="http://wiki.openstreetmap.org/wiki/Nominatim" class="uri">http://wiki.openstreetmap.org/wiki/Nominatim</a><a href="#fnref47">↩</a></p></li>
          <li id="fn48"><p><a href="https://github.com/jindrichmynarz/sparql-to-jsonld" class="uri">https://github.com/jindrichmynarz/sparql-to-jsonld</a><a href="#fnref48">↩</a></p></li>
          <li id="fn49"><p><a href="https://github.com/jindrichmynarz/jsonld-to-elasticsearch" class="uri">https://github.com/jindrichmynarz/jsonld-to-elasticsearch</a><a href="#fnref49">↩</a></p></li>
          <li id="fn50"><p><a href="https://github.com/jindrichmynarz/elasticsearch-geocoding" class="uri">https://github.com/jindrichmynarz/elasticsearch-geocoding</a><a href="#fnref50">↩</a></p></li>
          <li id="fn51"><p><a href="https://developers.google.com/maps/documentation/geocoding" class="uri">https://developers.google.com/maps/documentation/geocoding</a><a href="#fnref51">↩</a></p></li>
          <li id="fn52"><p><a href="https://developer.mapquest.com/products/geocoding" class="uri">https://developer.mapquest.com/products/geocoding</a><a href="#fnref52">↩</a></p></li>
          <li id="fn53"><p><a href="https://developer.here.com/rest-apis/documentation/geocoder" class="uri">https://developer.here.com/rest-apis/documentation/geocoder</a><a href="#fnref53">↩</a></p></li>
          <li id="fn54"><p><a href="http://simap.ted.europa.eu/web/simap/cpv" class="uri">http://simap.ted.europa.eu/web/simap/cpv</a><a href="#fnref54">↩</a></p></li>
          <li id="fn55"><p>E.g., <em>“Broccoli”</em> has broader concept <em>“Vegetables”</em>.<a href="#fnref55">↩</a></p></li>
          <li id="fn56"><p>E.g., <em>“Vegetables”</em> has broader concept <em>“Vegetables, fruits and nuts”</em>.<a href="#fnref56">↩</a></p></li>
          <li id="fn57"><p><a href="https://github.com/opendatacz/cpv2rdf" class="uri">https://github.com/opendatacz/cpv2rdf</a><a href="#fnref57">↩</a></p></li>
          <li id="fn58"><p><a href="http://dublincore.org/documents/dcmi-terms" class="uri">http://dublincore.org/documents/dcmi-terms</a><a href="#fnref58">↩</a></p></li>
          <li id="fn59"><p><a href="https://unifiedviews.eu" class="uri">https://unifiedviews.eu</a><a href="#fnref59">↩</a></p></li>
          <li id="fn60"><p><a href="http://simap.ted.europa.eu/web/simap/cpv" class="uri">http://simap.ted.europa.eu/web/simap/cpv</a><a href="#fnref60">↩</a></p></li>
          <li id="fn61"><p><a href="https://jena.apache.org/documentation/query/cmds.html" class="uri">https://jena.apache.org/documentation/query/cmds.html</a><a href="#fnref61">↩</a></p></li>
          <li id="fn62"><p><a href="http://wwwinfo.mfcr.cz/ares/ares.html.en" class="uri">http://wwwinfo.mfcr.cz/ares/ares.html.en</a><a href="#fnref62">↩</a></p></li>
          <li id="fn63"><p><a href="https://or.justice.cz/ias/ui/rejstrik" class="uri">https://or.justice.cz/ias/ui/rejstrik</a><a href="#fnref63">↩</a></p></li>
          <li id="fn64"><p><a href="http://www.rzp.cz/eng/index.html" class="uri">http://www.rzp.cz/eng/index.html</a><a href="#fnref64">↩</a></p></li>
          <li id="fn65"><p><a href="https://www.czso.cz/csu/res/business_register" class="uri">https://www.czso.cz/csu/res/business_register</a><a href="#fnref65">↩</a></p></li>
          <li id="fn66"><p><a href="http://wwwinfo.mfcr.cz/ares/ares_xml.html.cz" class="uri">http://wwwinfo.mfcr.cz/ares/ares_xml.html.cz</a><a href="#fnref66">↩</a></p></li>
          <li id="fn67"><p>See the periodical report of the Czech Statistical Office: <a href="https://www.czso.cz/documents/10180/33134052/14007016q301.pdf/db871117-2431-4bba-b8d9-2288cd10862e" class="uri">https://www.czso.cz/documents/10180/33134052/14007016q301.pdf/db871117-2431-4bba-b8d9-2288cd10862e</a><a href="#fnref67">↩</a></p></li>
          <li id="fn68"><p><a href="https://github.com/mff-uk/DPUs/tree/master/dpu-domain-specific/ares" class="uri">https://github.com/mff-uk/DPUs/tree/master/dpu-domain-specific/ares</a><a href="#fnref68">↩</a></p></li>
          <li id="fn69"><p><a href="https://github.com/opendatacz/ARES2RDF" class="uri">https://github.com/opendatacz/ARES2RDF</a><a href="#fnref69">↩</a></p></li>
          <li id="fn70"><p><a href="http://tarql.github.io" class="uri">http://tarql.github.io</a><a href="#fnref70">↩</a></p></li>
          <li id="fn71"><p><a href="https://www.czso.cz/csu/czso/klasifikace_ekonomickych_cinnosti_cz_nace" class="uri">https://www.czso.cz/csu/czso/klasifikace_ekonomickych_cinnosti_cz_nace</a><a href="#fnref71">↩</a></p></li>
          <li id="fn72"><p><a href="http://www.cuzk.cz/Uvod/Produkty-a-sluzby/RUIAN/RUIAN.aspx" class="uri">http://www.cuzk.cz/Uvod/Produkty-a-sluzby/RUIAN/RUIAN.aspx</a><a href="#fnref72">↩</a></p></li>
          <li id="fn73"><p>Valid as of September 2016.<a href="#fnref73">↩</a></p></li>
          <li id="fn74"><p>See the definition at <a href="https://www.czso.cz/csu/rso/adresni_misto" class="uri">https://www.czso.cz/csu/rso/adresni_misto</a>.<a href="#fnref74">↩</a></p></li>
          <li id="fn75"><p><a href="https://nahlizenidokn.cuzk.cz/StahniAdresniMistaRUIAN.aspx" class="uri">https://nahlizenidokn.cuzk.cz/StahniAdresniMistaRUIAN.aspx</a><a href="#fnref75">↩</a></p></li>
          <li id="fn76"><p>See the documentation (<a href="http://vdp.cuzk.cz/vymenny_format/csv/ad-csv-struktura.pdf" class="uri">http://vdp.cuzk.cz/vymenny_format/csv/ad-csv-struktura.pdf</a>) of the Czech addresses data.<a href="#fnref76">↩</a></p></li>
          <li id="fn77"><p><a href="http://freegis.fsv.cvut.cz/gwiki/S-JTSK_/_Chyba_p%C5%99i_transformaci_z_WGS84_do_S-JTSK">http://freegis.fsv.cvut.cz/gwiki/S-JTSK_/_Chyba_p%C5%99i_transformaci_z_WGS84_do_S-JTSK</a><a href="#fnref77">↩</a></p></li>
          <li id="fn78"><p><a href="https://github.com/linkedpipes/etl/tree/master/plugins/t-geoTools" class="uri">https://github.com/linkedpipes/etl/tree/master/plugins/t-geoTools</a><a href="#fnref78">↩</a></p></li>
          <li id="fn79"><p><a href="http://www.geotools.org" class="uri">http://www.geotools.org</a><a href="#fnref79">↩</a></p></li>
          <li id="fn80"><p><a href="http://epsg.io/5514" class="uri">http://epsg.io/5514</a><a href="#fnref80">↩</a></p></li>
          <li id="fn81"><p><a href="http://epsg.io/2065" class="uri">http://epsg.io/2065</a><a href="#fnref81">↩</a></p></li>
          <li id="fn82"><p><a href="http://geoportal.cuzk.cz/(S(dz3yiewehucysxhe2piompn3))/Default.aspx?mode=TextMeta&amp;text=wcts&amp;menu=19" class="uri">http://geoportal.cuzk.cz/(S(dz3yiewehucysxhe2piompn3))/Default.aspx?mode=TextMeta&amp;text=wcts&amp;menu=19</a><a href="#fnref82">↩</a></p></li>
          <li id="fn83"><p><a href="http://zindex.cz/en" class="uri">http://zindex.cz/en</a><a href="#fnref83">↩</a></p></li>
          <li id="fn84"><p><a href="http://www.econlab.cz/en" class="uri">http://www.econlab.cz/en</a><a href="#fnref84">↩</a></p></li>
          <li id="fn85"><p><a href="http://datlab.cz" class="uri">http://datlab.cz</a><a href="#fnref85">↩</a></p></li>
          <li id="fn86"><p><a href="https://virtuoso.openlinksw.com" class="uri">https://virtuoso.openlinksw.com</a><a href="#fnref86">↩</a></p></li>
          <li id="fn87"><p><a href="https://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtBulkRDFLoader" class="uri">https://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtBulkRDFLoader</a><a href="#fnref87">↩</a></p></li>
          <li id="fn88"><p><a href="http://docs.openlinksw.com/virtuoso/fn_exp" class="uri">http://docs.openlinksw.com/virtuoso/fn_exp</a><a href="#fnref88">↩</a></p></li>
          <li id="fn89"><p><a href="http://www.numpy.org" class="uri">http://www.numpy.org</a><a href="#fnref89">↩</a></p></li>
          <li id="fn90"><p><a href="https://www.scipy.org" class="uri">https://www.scipy.org</a><a href="#fnref90">↩</a></p></li>
          <li id="fn91"><p><a href="http://www.openblas.net" class="uri">http://www.openblas.net</a><a href="#fnref91">↩</a></p></li>
          <li id="fn92"><p>91 % of search engine users consider only the top 10 results, according to a study (<a href="http://www.seo-takeover.com/case-study-click-through-rate-google" class="uri">http://www.seo-takeover.com/case-study-click-through-rate-google</a>).<a href="#fnref92">↩</a></p></li>
          <li id="fn93"><p><a href="https://www.elastic.co/products/elasticsearch" class="uri">https://www.elastic.co/products/elasticsearch</a><a href="#fnref93">↩</a></p></li>
          <li id="fn94"><p><a href="http://lucene.apache.org" class="uri">http://lucene.apache.org</a><a href="#fnref94">↩</a></p></li>
          <li id="fn95"><p><a href="http://www.geotools.org" class="uri">http://www.geotools.org</a><a href="#fnref95">↩</a></p></li>
          <li id="fn96"><p><a href="http://etl.linkedpipes.com" class="uri">http://etl.linkedpipes.com</a><a href="#fnref96">↩</a></p></li>
          <li id="fn97"><p><a href="https://virtuoso.openlinksw.com" class="uri">https://virtuoso.openlinksw.com</a><a href="#fnref97">↩</a></p></li>
          <li id="fn98"><p><a href="http://www.numpy.org" class="uri">http://www.numpy.org</a><a href="#fnref98">↩</a></p></li>
          <li id="fn99"><p><a href="https://www.scipy.org" class="uri">https://www.scipy.org</a><a href="#fnref99">↩</a></p></li>
          <li id="fn100"><p><a href="http://www.saxonica.com/products/products.xml" class="uri">http://www.saxonica.com/products/products.xml</a><a href="#fnref100">↩</a></p></li>
          <li id="fn101"><p><a href="http://silkframework.org" class="uri">http://silkframework.org</a><a href="#fnref101">↩</a></p></li>
          <li id="fn102"><p><a href="http://tarql.github.io" class="uri">http://tarql.github.io</a><a href="#fnref102">↩</a></p></li>
          <li id="fn103"><p><a href="https://jena.apache.org" class="uri">https://jena.apache.org</a><a href="#fnref103">↩</a></p></li>
          <li id="fn104"><p><a href="https://unifiedviews.eu" class="uri">https://unifiedviews.eu</a><a href="#fnref104">↩</a></p></li>
          <li id="fn105"><p><a href="https://github.com/jindrichmynarz/discretize-sparql" class="uri">https://github.com/jindrichmynarz/discretize-sparql</a><a href="#fnref105">↩</a></p></li>
          <li id="fn106"><p><a href="https://github.com/jindrichmynarz/elasticsearch-geocoding" class="uri">https://github.com/jindrichmynarz/elasticsearch-geocoding</a><a href="#fnref106">↩</a></p></li>
          <li id="fn107"><p><a href="https://github.com/jindrichmynarz/jsonld-to-elasticsearch" class="uri">https://github.com/jindrichmynarz/jsonld-to-elasticsearch</a><a href="#fnref107">↩</a></p></li>
          <li id="fn108"><p><a href="https://github.com/jindrichmynarz/matchmaker-sparql" class="uri">https://github.com/jindrichmynarz/matchmaker-sparql</a><a href="#fnref108">↩</a></p></li>
          <li id="fn109"><p><a href="https://github.com/jindrichmynarz/matchmaker-rescal" class="uri">https://github.com/jindrichmynarz/matchmaker-rescal</a><a href="#fnref109">↩</a></p></li>
          <li id="fn110"><p><a href="https://github.com/jindrichmynarz/sparql-to-csv" class="uri">https://github.com/jindrichmynarz/sparql-to-csv</a><a href="#fnref110">↩</a></p></li>
          <li id="fn111"><p><a href="https://mustache.github.io" class="uri">https://mustache.github.io</a><a href="#fnref111">↩</a></p></li>
          <li id="fn112"><p><a href="https://github.com/jindrichmynarz/sparql-to-graphviz" class="uri">https://github.com/jindrichmynarz/sparql-to-graphviz</a><a href="#fnref112">↩</a></p></li>
          <li id="fn113"><p><a href="http://www.graphviz.org/doc/info/lang.html" class="uri">http://www.graphviz.org/doc/info/lang.html</a><a href="#fnref113">↩</a></p></li>
          <li id="fn114"><p><a href="http://www.graphviz.org" class="uri">http://www.graphviz.org</a><a href="#fnref114">↩</a></p></li>
          <li id="fn115"><p><a href="https://github.com/jindrichmynarz/sparql-to-jsonld" class="uri">https://github.com/jindrichmynarz/sparql-to-jsonld</a><a href="#fnref115">↩</a></p></li>
          <li id="fn116"><p><a href="https://mustache.github.io" class="uri">https://mustache.github.io</a><a href="#fnref116">↩</a></p></li>
          <li id="fn117"><p><a href="https://github.com/jindrichmynarz/sparql-to-tensor" class="uri">https://github.com/jindrichmynarz/sparql-to-tensor</a><a href="#fnref117">↩</a></p></li>
          <li id="fn118"><p><a href="http://math.nist.gov/MatrixMarket/formats.html#MMformat" class="uri">http://math.nist.gov/MatrixMarket/formats.html#MMformat</a><a href="#fnref118">↩</a></p></li>
          <li id="fn119"><p><a href="https://mustache.github.io" class="uri">https://mustache.github.io</a><a href="#fnref119">↩</a></p></li>
          <li id="fn120"><p><a href="https://github.com/jindrichmynarz/sparql-unlimited" class="uri">https://github.com/jindrichmynarz/sparql-unlimited</a><a href="#fnref120">↩</a></p></li>
          <li id="fn121"><p><a href="https://mustache.github.io" class="uri">https://mustache.github.io</a><a href="#fnref121">↩</a></p></li>
          <li id="fn122"><p><a href="https://github.com/jindrichmynarz/vocab-to-graphviz" class="uri">https://github.com/jindrichmynarz/vocab-to-graphviz</a><a href="#fnref122">↩</a></p></li>
          <li id="fn123"><p><a href="http://www.graphviz.org" class="uri">http://www.graphviz.org</a><a href="#fnref123">↩</a></p></li>
          <li id="fn124"><p><a href="http://www.graphviz.org/doc/info/lang.html" class="uri">http://www.graphviz.org/doc/info/lang.html</a><a href="#fnref124">↩</a></p></li>
          </ol>
          </section>
        </div>
      </article>
          </main>
    <script src="https://dokie.li/scripts/simplerdf.js"></script>
    <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
    <script src="https://dokie.li/scripts/medium-editor-tables.min.js"></script>
    <script src="https://dokie.li/scripts/do.js"></script>
    <script src="resources/js/script.js"></script>
  </body>
</html>
